{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxsTPZphD2ol"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.wkb as wkblib\n",
    "import numpy as np\n",
    "import keplergl\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import osmium\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pyproj import Proj\n",
    "from h3 import h3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from rtree import index\n",
    "\n",
    "from utils.OSRMFramework import OSRMFramework\n",
    "from utils.RouteAnnotator import RouteAnnotator\n",
    "from utils.plot_geometry import plot_geometry\n",
    "from utils.conversor import latlon2linestring\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 0\n",
    "\n",
    "\n",
    "###### constants\n",
    "TARGET = 'fare_amount'\n",
    "DATASET_PATH = 'data/taxi_fare_sample_100000.csv'\n",
    "H3_RES_ANALYSIS = 7\n",
    "\n",
    "IS_DOCKER = os.environ.get('IS_DOCKER', False)\n",
    "if IS_DOCKER:\n",
    "    OSRM_PATH = 'osrm-router:5000'  # OSRM path for when it's instantiated through docker-compose (service name)\n",
    "else:\n",
    "    OSRM_PATH = 'localhost:5000'    # OSRM path for when it's instantiated locally\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "Notebook for AMLD 2020 Workshop: [Feature Engineering for Spatial Data Analysis](https://appliedmldays.org/workshops/feature-engineering-for-spatial-data-analysis).  \n",
    "  \n",
    "Authors:  \n",
    "* Caio Miyashiro: caiohenrique37@gmail.com  \n",
    "* Eva Jaumann: eva.jaumann@mytaxi.com  \n",
    "* Selim Onat: selim.onat@mytaxi.com  \n",
    "\n",
    "Github Repository: https://github.com/caiomiyashiro/geospatial_data_analysis/tree/master/AMLD-2020  \n",
    "\n",
    "# Installation\n",
    "\n",
    "If you did not already: Please follow the README file for setup instructions.\n",
    "\n",
    "Make sure that you have a kaggle account.\n",
    "\n",
    "---------------\n",
    "  \n",
    "# Introduction\n",
    "  \n",
    "Welcome to the workshop! Today you're going to learn several techniques related to the processing of geospatial data. We're going to use as background motivation the Kaggle dataset on [Prediction of Taxis Fares](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction)\n",
    " \n",
    "<img src=\"images/wordcloud_.png\" width=\"500\"> \n",
    "\n",
    "The main sections we're going to cover are:\n",
    "\n",
    "<img src=\"images/toc_0.png\" width=\"500\"> \n",
    "\n",
    "## Presentation and full notebook\n",
    "\n",
    "We prepared a short version to follow during the workshop (this notebook Presentation_AMLD_2020.ipynb). To get more information (also after the workshop) have a look at Complete_AMLD_2020.ipynb. You can always check out this notebook if you get stucked and want to have a look at the solutions for the exercises.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "<img src=\"images/kaggle_page.png\" width=\"700\"> \n",
    "<center>Kaggle Taxi Prediction Homepage Picture</center>  \n",
    "\n",
    "The challenge of this dataset is to predict the final fare paid by user by just having a small set of variables to work with:\n",
    "* **pickup_datetime**: Time that a user was picked-up by a taxi\n",
    "* **pickup_latitude**:   pickup location\n",
    "* **pickup_longitude**:  pickup location\n",
    "* **dropoff_latitude**:  drop-off location\n",
    "* **dropoff_longitude**: drop-off location   \n",
    "* **passenger_count**: amount of passengers in this trip\n",
    "  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    1. The main focus of this workshop will be <b>not</b> about minimizing your model error score, but rather to explain to you the main concepts and techniques when working with spatial data :) <br/><br/>\n",
    "    2. The original dataset is big (> 5Gb of data) so, for this workshop, we've randomly sampled it and made it available for you as a csv file. Lets take a look at it:\n",
    "</div>\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "1SLycpVN9BM-",
    "outputId": "1855cfed-4c48-4817-9b74-6e8b04d7ce40"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET_PATH)\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "display(df.head())\n",
    "display(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude/Longitude\n",
    "\n",
    "Latitude and longitude are imaginary horizontal and vertical lines respectively that run around the earth.  \n",
    "\n",
    "* Latitude run across the globe, and the latitude that runs through the middle of the Earth is given the number zero degrees (0°) and is called the Equator.  \n",
    "\n",
    "* Longitude are vertical lines around Earth. They meet at the poles and are wide apart at Equator. By convention, the line with 0° degrees longitude passes the Royal Observatory in Greenwich, England.\n",
    "\n",
    "Together, latitude and longitude identify a point on a spherical system, in our case on our planet The Earth.  \n",
    "\n",
    "<img src=\"images/latitude_longitude.png\" width=\"600\"> \n",
    "<center>Image source - <a href=\"https://en.wikipedia.org/wiki/Geographic_coordinate_system\">https://en.wikipedia.org/wiki/Geographic_coordinate_system</a></center> \n",
    "\n",
    "For example the statue of liberty has a latitude of `40.691332` and a longitude of `-74.0446291`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fe6TjpszDVbG"
   },
   "source": [
    "# First Analysis  \n",
    "\n",
    "Before we start understanding our data, we should check its integrity. We apply some sanity checks in the data to remove anything that'd be impossible to happen, such as negative fares.\n",
    "\n",
    "## Check amount of NaN\n",
    "\n",
    "The number of nan elements are irrelevant. So we just remove them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check amount of NaN - just remove them\n",
    "display(df.isna().sum())\n",
    "\n",
    "df.dropna(subset=['dropoff_longitude', 'dropoff_latitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check impossible fares\n",
    "\n",
    "Note that we're removing only **impossible** fares for now. Any filter based on value distribution should be done only using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['fare_amount'].describe())\n",
    "print('')\n",
    "\n",
    "fare_under_0 = np.sum(df['fare_amount']<=0)\n",
    "print(f'{fare_under_0} rows with fare under or equal 0')\n",
    "\n",
    "df = df.loc[df['fare_amount'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Data Visual Analysis\n",
    "  \n",
    "From the first `describe` function in the section above, we see that there're a few latitudes/longitudes for both pick up and drop-off that are away from the main mass of data. In order to confirm we can use a visual tool.   \n",
    "\n",
    "### Kepler\n",
    "  \n",
    "Kepler can visualize the following polygons:   \n",
    "<img src=\"images/kepler_polygons_.jpg\" width=\"200\"> \n",
    "  \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    In the complete version of this notebook, we go through each of the layers in H3. Check it out.\n",
    "</div>  \n",
    "\n",
    "  \n",
    "You can check a few visuals in the [examples section](https://github.com/keplergl/kepler.gl/blob/master/docs/user-guides/c-types-of-layers.md) of kepler Github repository.  \n",
    "\n",
    "### Check pickup and dropoff points points  \n",
    "\n",
    "For checking pick-up and drop-off locations, we're going to check these `Points` in kepler by sending the latitudes and longitudes from both fields and build a Points layer in Kepler\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w1 = keplergl.KeplerGl(height=500)\n",
    "w1.add_data(data=df[['pickup_latitude', 'pickup_longitude',\n",
    "                     'dropoff_latitude', 'dropoff_longitude']], name='points')\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some points are quite wrong, being not just outside New York, but rather in other countries or even in the middle of the ocean. Lets do a simple approach to remove those points and build a [bounding box](https://wiki.openstreetmap.org/wiki/Bounding_Box) (bbox), *i.e.*, a square around New York.  \n",
    "  \n",
    "In order to build a squared bbox, we need two pairs of [latitude, longitude] points, the bottom left and top right corners. In order to get them, we can click on specific places on [Google Maps](https://www.google.com/maps) and get the returned latitude/longitudes points. Alternatively, [boundingbox](https://boundingbox.klokantech.com/) can be used. Finally, everything that's outside the bbox definition, we will filter out from our dataset.\n",
    "\n",
    "<img src=\"images/ny_bbox.png\" width=\"600\"> \n",
    "<center>Image source - <a href=\"https://gis.stackexchange.com/questions/255158/get-minimum-and-maximum-latitude-and-longitude-of-new-york\">https://gis.stackexchange.com/questions/255158/get-minimum-and-maximum-latitude-and-longitude-of-new-york</a></center> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outside_bbox(df, bbox, lat_col, lon_col):\n",
    "    df_ = df.copy()\n",
    "    mask_lat = (df_[lat_col] > bbox[0][0]) & (df_[lat_col] < bbox[1][0])\n",
    "    mask_lon = (df_[lon_col] > bbox[0][1]) & (df_[lon_col] < bbox[1][1])\n",
    "    return df_.loc[(mask_lat) & (mask_lon)]\n",
    "\n",
    "bottom_left_lat_lon = [40.492016, -74.279034]\n",
    "upper_right_lat_lon = [40.913473, -73.689152]\n",
    "bbox = [bottom_left_lat_lon, upper_right_lat_lon]\n",
    "\n",
    "df = remove_outside_bbox(df, bbox, 'pickup_latitude', 'pickup_longitude')\n",
    "df = remove_outside_bbox(df, bbox, 'dropoff_latitude', 'dropoff_longitude')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = keplergl.KeplerGl(height=500)\n",
    "w1.add_data(data=df[['pickup_latitude', 'pickup_longitude',\n",
    "                     'dropoff_latitude', 'dropoff_longitude']], name='points')\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3bypvW8YD-Ih"
   },
   "source": [
    "# Split training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "MRKsnouKEAvN",
    "outputId": "07716d2f-065f-4330-89df-513f69eb0fe9"
   },
   "outputs": [],
   "source": [
    "######\n",
    "###### Helper functions\n",
    "def split_training_test(df, target=TARGET, test_size=0.2):\n",
    "    X = df.drop(TARGET, axis=1)\n",
    "    y = df[TARGET]\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=42) \n",
    "\n",
    "def join_Xy(df, target, target_str=TARGET):\n",
    "    df_ = df.copy()\n",
    "    df_[target_str] = target\n",
    "    return df_\n",
    "\n",
    "######\n",
    "###### Main function\n",
    "def get_initial_training_testing_set(df):\n",
    "    X_train, X_test, y_train, y_test = split_training_test(df)\n",
    "    print(f'Shape X_train {X_train.shape}')\n",
    "    print(f'Shape X_test {X_test.shape}')\n",
    "\n",
    "    # join target for easier exploratory analysis\n",
    "    df_train = join_Xy(X_train, y_train)\n",
    "    df_test = join_Xy(X_test, y_test)\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = get_initial_training_testing_set(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "In the machine learning context, we want to have the best quality data in our training procedure, as outliers can impact your model training process and therefore, create bad predictions for when your model is in production. With this second step on data analysis, we check the data distribution to check for extreme variations in our dataset. Besides serving for the purpose explained above, this also indicate the type of rules that we should consider when the model is in `production`.  \n",
    "  \n",
    "- For example, what should we do if we have a model for predicting Taxi fares made for NY in production and we receive a fare prediction request with a pick-up latitude/longitude referencing Brazil? \n",
    "\n",
    "\n",
    "## Remove fares < minimum fare and outliers > 99th percentile\n",
    "\n",
    "Lets use a simple percentil rule in order to remove training data whose fare is minimum that the city regulation defines and more than 99% of the other fares presenting in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# minimum fare = $2.5: https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page\n",
    "\n",
    "display(df_train['fare_amount'].describe(percentiles=[.01, .05, .1, .90,.95,.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove fares smaller than 2.5 dollars and higher than 52 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[(df_train['fare_amount'] >= 2.5) & (df_train['fare_amount'] <= 52)]\n",
    "\n",
    "plt.hist(df_train['fare_amount'], bins=51)\n",
    "plt.title('Histogram of Fare')\n",
    "plt.xlabel('Dollars ($)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average demand per region over 1 weekday - H3\n",
    "\n",
    "On the maps we saw the pickup and dropoff locations. How can we define areas with high demand? To do this we need to divide the area of the city in smaller neighborhoods.\n",
    "\n",
    "Zipcodes?\n",
    "<img src=\"images/bronxzip.png\" width=\"300\"> \n",
    "<center>Image source - <a href=\"http://map-world.us/bronx-zip-code-map.html\">worldmap</a></center>\n",
    "\n",
    "Grid!\n",
    "\n",
    "<img src=\"images/catan.png\" width=\"300\"> \n",
    "<center>Catan (game) Image source - <a href=\"https://en.wikipedia.org/wiki/Catan#/media/File:Catan_Universe_fixed_setup.svg\">wikipedia</a></center>\n",
    "\n",
    "Different resolutions possible.\n",
    "\n",
    "On a sphere: Fullerene like structure with pentagons (could be placed into water)\n",
    "\n",
    "\n",
    "<img src=\"images/sphereh3.png\" width=\"500\"> \n",
    "<center>Image source - <a href=\"https://eng.uber.com/h3/\">https://eng.uber.com/h3/</a></center> \n",
    "\n",
    "\n",
    "We will use [H3](https://uber.github.io/h3), Uber's open source geospational indexing system and use it for visualisations. The size of the hexagons is defined by the [resolution](https://uber.github.io/h3/#/documentation/core-library/resolution-table), e.g. a resolution of 7 results in a hexagon area of 5 square km. \n",
    "\n",
    "In the following we will calculate the demand per hexagon on a given weekday and visualize it with kepler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## helper function\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def add_day(day, reference_day='2019-01-07'):\n",
    "    return pd.to_datetime(reference_day) + timedelta(days=day)\n",
    "\n",
    "def average_demand_weekday(df,\n",
    "                           pickup_latitude='pickup_latitude', pickup_longitude='pickup_longitude',\n",
    "                           dropoff_latitude='dropoff_latitude', dropoff_longitude='dropoff_longitude',\n",
    "                           pickup_datetime='pickup_datetime', fare='fare_amount', resolution=H3_RES_ANALYSIS):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # hexagon indices\n",
    "    df['pickup_h3'] = [h3.geo_to_h3(lat, long, res=resolution) for lat, long in zip(df[pickup_latitude], \n",
    "                                                                                    df[pickup_longitude])]\n",
    "    df['dropoff_h3'] = [h3.geo_to_h3(lat, long, res=resolution) for lat, long in zip(df[dropoff_latitude], \n",
    "                                                                                    df[dropoff_longitude])]\n",
    "    # weekday\n",
    "    df['weekday'] = df[pickup_datetime].dt.dayofweek\n",
    "    \n",
    "    # aggregate per hexagon and weekday \n",
    "    df_aggregated = df.groupby(['pickup_h3','weekday']).size().reset_index(name='nr_tours')\n",
    "    \n",
    "    df_demand = df.merge(df_aggregated, how = 'inner', on = ['pickup_h3', 'weekday'])\n",
    "    \n",
    "    # introduce a reference day as a reference week to have a playback option in kepler\n",
    "    df_demand['ref_week_date'] = df_demand.weekday.apply(add_day)\n",
    "\n",
    "\n",
    "    return df_demand[['ref_week_date', 'pickup_h3', 'dropoff_h3', 'nr_tours', fare]]\n",
    "\n",
    "df_hex = average_demand_weekday(df_train, resolution=H3_RES_ANALYSIS)\n",
    "\n",
    "df_hex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = keplergl.KeplerGl(height=500)\n",
    "w1.add_data(data=df_hex, name='hexagons')\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps we used to show the hexagon map (open the menu first with the arrow symbol **>**):\n",
    "\n",
    "<img src=\"images/kepler_howto.gif\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Origin-Destination map over 1 Week - H3\n",
    "\n",
    "To see which pickup-dropoff combinations are the most demanded, we connect the pickup hexagon centers with the dropoff hexagon centers.  \n",
    "\n",
    "Use the **arc** layer to show the distances between pickup and dropoff.\n",
    "\n",
    "**Can you think on the most popular drop-off?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_arc_weekday(df,\n",
    "                        pickup_hex='pickup_h3', dropoff_hex='dropoff_h3',\n",
    "                        nr_tours='nr_tours', ref_week_date='ref_week_date'):\n",
    "    \n",
    "    center_pickup_hex = 'center_pickup_hex'\n",
    "    center_dropoff_hex = 'center_dropoff_hex'\n",
    "    df[center_pickup_hex] = df[pickup_hex].apply(lambda x: h3.h3_to_geo(x))\n",
    "    df[center_dropoff_hex] = df[dropoff_hex].apply(lambda x: h3.h3_to_geo(x))\n",
    "    \n",
    "    df['lng_pickup'] = df[center_pickup_hex].apply(lambda x: x[1])\n",
    "    df['lat_pickup'] = df[center_pickup_hex].apply(lambda x: x[0])\n",
    "    df['lng_dropoff'] = df[center_dropoff_hex].apply(lambda x: x[1])\n",
    "    df['lat_dropoff'] = df[center_dropoff_hex].apply(lambda x: x[0])\n",
    "    return df[['lat_pickup', 'lng_pickup', 'lat_dropoff', 'lng_dropoff', nr_tours, ref_week_date]].drop_duplicates()\n",
    "\n",
    "df_arc = average_arc_weekday(df_hex)\n",
    "df_arc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = keplergl.KeplerGl(height=500)\n",
    "w1.add_data(data=df_arc.drop_duplicates(), name='hexagons')\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "\n",
    "<img src=\"images/toc_1.png\" width=\"500\"> \n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Feature Engineering - Model Iterations\n",
    "\n",
    "Now that we have a cleaner training set, we can use it to create our predictions! For teaching purposes, we're going to create multiple models, with incremental set of features/complexity. For each iteration, we're going to show a demo with the feature's concept, create a pipeline, train our models and then show and store the errors by their's [Root Mean Squared Error - RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation).\n",
    "\n",
    "## Start experiment tracking\n",
    "\n",
    "We're going to store the RMSEs for each iteration in this dictionary and look at it at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShZLGIYzk-cI"
   },
   "outputs": [],
   "source": [
    "iteration_results = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0 - Lazy Estimator - Median Fare\n",
    "\n",
    "In order to have a baseline, we're going to start with a lazy model, one that, for every row in our training (and testing) dataset, will predict only the median fare of the training set. The only purpose of this is to have a upper error limit and we hope to see it decreasing after every iteration on the model's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_f = df_train['fare_amount'].median()\n",
    "print(f'Median Fare: {median_f}')\n",
    "\n",
    "y_test_pred_lazy = [median_f] * df_test.shape[0]                      # array with same length as y_test\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(df_test[TARGET], y_test_pred_lazy)) # RMSE\n",
    "iteration_results['exp_0_lazy_rmse'] = rmse                           # store in experiments results\n",
    "print(f'RMSE: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSmcxpLMF_xJ"
   },
   "source": [
    "## Model 1 - Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "sO-69Fi2FAYt",
    "outputId": "b4d0270e-167d-4a8d-b251-6d1cd365b238"
   },
   "outputs": [],
   "source": [
    "## Small Example\n",
    "##\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return (((x2-x1)**2 + (y2-y1)**2)**(1/2))\n",
    "\n",
    "df_train_temp = df_train.head(3).copy()\n",
    "df_train_temp['euclidean_dist'] = euclidean_distance(df_train_temp['pickup_latitude'], df_train_temp['pickup_longitude'], \n",
    "                                                   df_train_temp['dropoff_latitude'], df_train_temp['dropoff_longitude'])\n",
    "\n",
    "df_train_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCdZLbWRG15v"
   },
   "outputs": [],
   "source": [
    "def standardize_features(df):\n",
    "    df_copy = df.copy()\n",
    "    col_names = df_copy.columns\n",
    "    ixs = df_copy.index\n",
    "    return pd.DataFrame(StandardScaler().fit_transform(df_copy), columns=col_names, index=ixs)\n",
    "\n",
    "def feature_pipeline_1(df, target_col=TARGET,\n",
    "                              pickup_latitude='pickup_latitude', pickup_longitude='pickup_longitude',\n",
    "                              dropoff_latitude='dropoff_latitude', dropoff_longitude='dropoff_longitude'):\n",
    "  \n",
    "    EUCLIDEAN_FEAT = 'euclidean_dist'\n",
    "    FEATURES = [EUCLIDEAN_FEAT]\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[EUCLIDEAN_FEAT] = euclidean_distance(x1=df_copy[pickup_latitude], y1=df_copy[pickup_longitude], \n",
    "                                                 x2=df_copy[dropoff_latitude], y2=df_copy[dropoff_longitude])\n",
    "    # in the EDA, we probably treat the nulls, so for now, just drop them\n",
    "    df_copy = df_copy.dropna()\n",
    "        \n",
    "    return df_copy[FEATURES + [target_col]]\n",
    "\n",
    "df_train_1 = feature_pipeline_1(df_train)\n",
    "df_test_1 = feature_pipeline_1(df_test)\n",
    "\n",
    "df_train_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-eXRxwMFHVjE"
   },
   "outputs": [],
   "source": [
    "model_1 = LinearRegression()\n",
    "model_1.fit(df_train_1.drop(TARGET, axis=1), df_train_1[TARGET])\n",
    "\n",
    "y_test_pred_1 = model_1.predict(df_test_1.drop(TARGET, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "N37YPJLAdTzy",
    "outputId": "b581fb72-42f9-4f2f-c8bb-ef9d506297f0"
   },
   "outputs": [],
   "source": [
    "def print_evaluation(y_obs, y_pred, max_lim_y=100, return_errors=True):\n",
    "    rmse_hist = np.sqrt((y_pred - y_obs)**2)\n",
    "    plt.hist(rmse_hist[y_obs < max_lim_y], bins=100)\n",
    "    plt.title('RMSE Distribution');\n",
    "\n",
    "    if(return_errors == True):\n",
    "        return mae_hist\n",
    "\n",
    "#####\n",
    "rmse = np.sqrt(mean_squared_error(df_test_1[TARGET], y_test_pred_1))\n",
    "iteration_results['exp_1_rmse'] = rmse\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "print_evaluation(df_test_1[TARGET], y_test_pred_1, return_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hed4d4mGlVEZ"
   },
   "source": [
    "## Why the feature value above is not correct? \n",
    "\n",
    "Although it works for our model, if you look at pipeline1's output, the euclidean distance values don't make any sense. Why is that?\n",
    "\n",
    "### Projections\n",
    "\n",
    "<img src=\"images/mercator_vs_real.jpg\" width=\"500\"> \n",
    "<center><a href=\"https://newsini.com/news/this-map-reveals-the-actual-size-and-shape-of-every-country-in-the-world?uid=18225\">Image source</a></center> \n",
    "\n",
    "There are many types of projections, popular ones include: **Universal transverse Mercator (UTM)** and **Lambert Conformal Conic**. UTM is discussed in detail below, the latter is used mainly in mid-latitude areas.This uses two Standard Parallel (lines of latitudes which are unevenly spaced concentric circles).  \n",
    "\n",
    "### Universal Transverse Mercator (UTM) \n",
    "\n",
    "<img src=\"images/utm_grid.jpg\" width=\"500\"> \n",
    "<center>UTM Projection grid - <a href=\"https://www.youtube.com/watch?v=LcVlx4Gur7I\">https://www.youtube.com/watch?v=LcVlx4Gur7I</a></center> \n",
    "  \n",
    "UTM projects the latitude and longitude to a [three numbers representation](https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system#Locating_a_position_using_UTM_coordinates):\n",
    "* **Zone** - Zone is a number attributed to every grid as in the picture above. We can use online tools to figure out the UTM zone of a region\n",
    "* **Easting** - For every grid, their central meridian starts with a value of 500.000 and the *easting* value represents how many `meters` east is the point to this grid's meridian\n",
    "* **Northing** - For every grid, this value represents how far away the point is from the south pole `in meters`\n",
    "\n",
    "### Adapted Pipeline 1 code for UTM\n",
    "\n",
    "To project our lat/lon points to UTM, we use the [pyproj](https://pypi.org/project/pyproj/#description) package. **Notice** how, in the function, we have to send the coordinates in the correct order - **longitude**/**latitude**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Small Example\n",
    "##\n",
    "def latlon2UTM(latitudes, longitudes, utm_proj_zone=18):\n",
    "    # project points to UTM\n",
    "    # New York city zone = 18 - https://www.latlong.net/lat-long-utm.html\n",
    "    lonlat2UTM = Proj(proj='utm', zone=utm_proj_zone, ellps='WGS84') \n",
    "    point_easting, point_northing = lonlat2UTM(longitudes, latitudes)\n",
    "    # lonlat2UTM(point_easting,point_northing,inverse=True) # to convert UTM back to lon/lat\n",
    "    return point_easting, point_northing\n",
    "\n",
    "df_train_temp = df_train.head(3).copy()\n",
    "\n",
    "pickup_dropoff_cols = ['pickup', 'dropoff']\n",
    "for point in pickup_dropoff_cols:\n",
    "    point_easting, point_northing = latlon2UTM(latitudes=df_train_temp[f'{point}_latitude'].values,\n",
    "                                               longitudes=df_train_temp[f'{point}_longitude'].values)\n",
    "    df_train_temp[f'{point}_easting'] = point_easting\n",
    "    df_train_temp[f'{point}_northing'] = point_northing\n",
    "    \n",
    "df_train_temp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pipeline_1b(df, target_col=TARGET,\n",
    "                       pickup_latitude='pickup_latitude', pickup_longitude='pickup_longitude',\n",
    "                       dropoff_latitude='dropoff_latitude', dropoff_longitude='dropoff_longitude',\n",
    "                       utm_proj_zone=18):\n",
    "  \n",
    "    EUCLIDEAN_FEAT = 'euclidean_dist_km'\n",
    "    FEATURES = [EUCLIDEAN_FEAT]\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    pickup_dropoff_cols = ['pickup', 'dropoff']\n",
    "    for point in pickup_dropoff_cols:\n",
    "        point_easting, point_northing = latlon2UTM(latitudes=df_copy[f'{point}_latitude'].values,\n",
    "                                                   longitudes=df_copy[f'{point}_longitude'].values)\n",
    "        df_copy[f'{point}_easting'] = point_easting\n",
    "        df_copy[f'{point}_northing'] = point_northing\n",
    "    \n",
    "    df_copy[EUCLIDEAN_FEAT] = euclidean_distance(x1=df_copy['pickup_northing'], y1=df_copy['pickup_easting'], \n",
    "                                                 x2=df_copy['dropoff_northing'], y2=df_copy['dropoff_easting'])/1000\n",
    "    # in the EDA, we probably treat the nulls, so for now, just drop them\n",
    "    df_copy = df_copy.dropna()\n",
    "        \n",
    "    return df_copy[FEATURES + [target_col]]\n",
    "\n",
    "df_train_1b = feature_pipeline_1b(df_train)\n",
    "df_test_1b = feature_pipeline_1b(df_test)\n",
    "\n",
    "df_train_1b.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1b = LinearRegression()\n",
    "model_1b.fit(df_train_1b.drop(TARGET, axis=1), df_train_1b[TARGET])\n",
    "\n",
    "y_test_pred_1b = model_1b.predict(df_test_1b.drop(TARGET, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(df_test_1b[TARGET], y_test_pred_1b))\n",
    "iteration_results['exp_1b_rmse'] = rmse\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "print_evaluation(df_test_1b[TARGET], y_test_pred_1b, return_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the score didn't change much, for small distances there's a big chance that latitude/longitude are highly correlated to UTM projections, except for the unit, which in UTM refers to meters, while lat/lon distances doesn't have any real unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1[['euclidean_dist']].merge(df_train_1b[['euclidean_dist_km']], left_index=True, right_index=True).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan\n",
    "\n",
    "The manhattan distance, also coincidentally called the [taxi cab distance](https://en.wikipedia.org/wiki/Taxicab_geometry), is a different distance definition, in which the distance between two points $A$ and $B$ is the sum of the absolute differences of their Cartesian coordinates, that is:\n",
    "\n",
    "$$d_{1}(p,q) = \\left \\| p-q \\right \\|_{1} = \\sum_{i=1}^{n}\\left | p_{i} - q_{i} \\right |$$  \n",
    "  \n",
    "The name refers to the minimum distance a taxi would have to travel in a place with regular square blocks instead of a direct straight line.  \n",
    "\n",
    "<img src=\"images/manhattan_distance.png\" width=\"250\"> \n",
    "<center>Manhattan distance from <a href=\"https://de.wikipedia.org/wiki/Datei:Manhattan_distance_bgiu.png\">Wikipedia</a></center> \n",
    "\n",
    "### Haversine Distance:\n",
    "\n",
    "Both euclidean and manhattan are calculated considering that point P and Q are in a plane like a square. However, depending on the distance between the two points, these distances don't take the curvature of the earth into consideration, and their values would measure a distance that would cross through the earth!\n",
    "  \n",
    "<img src=\"images/haversine_example.png\" width=\"250\"> \n",
    "<center>Blue line representing euclidean distance between two further points P and Q and Haversine distance in red. Adapted from <a href=\"https://en.wikipedia.org/wiki/Great-circle_distance\">Wikipedia</a></center>    \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    As in this workshop we're dealing with small enough distances considering the whole earth as parameter, we don't expect big differences between the euclidean and Haversine. However, if you're dealing with bigger distances, Haversine should be preferred.\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "HjB400AfdUsm",
    "outputId": "89229ef5-4b3e-4b09-8662-60ac4bfb77b0"
   },
   "outputs": [],
   "source": [
    "## Small Example\n",
    "##\n",
    "def manhattan_distance(x1, y1, x2, y2):        \n",
    "    dy = np.abs(y2 - y1)\n",
    "    dx = np.abs(x2 - x1)\n",
    "    return dy + dx\n",
    "\n",
    "def lat_longs2radians(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    return lat1, lon1, lat2, lon2\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.\n",
    "\n",
    "    \"\"\"\n",
    "    lat1, lon1, lat2, lon2 = lat_longs2radians(lat1, lon1, lat2, lon2)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "\n",
    "#####\n",
    "#####\n",
    "df_train_temp = df_train.head(3).copy()\n",
    "df_train_temp['haversine_dist'] = haversine_distance(df_train_temp['pickup_latitude'], df_train_temp['pickup_longitude'], \n",
    "                                                     df_train_temp['dropoff_latitude'], df_train_temp['dropoff_longitude'])\n",
    "\n",
    "pickup_dropoff_cols = ['pickup', 'dropoff']\n",
    "for point in pickup_dropoff_cols:\n",
    "    point_easting, point_northing = latlon2UTM(latitudes=df_train_temp[f'{point}_latitude'].values,\n",
    "                                               longitudes=df_train_temp[f'{point}_longitude'].values)\n",
    "    df_train_temp[f'{point}_easting'] = point_easting\n",
    "    df_train_temp[f'{point}_northing'] = point_northing\n",
    "df_train_temp['manhattan_dist'] = manhattan_distance(df_train_temp['pickup_northing'], df_train_temp['pickup_easting'], \n",
    "                                                     df_train_temp['dropoff_northing'], df_train_temp['dropoff_easting'])\n",
    "\n",
    "df_train_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_qHuzQIvMoY"
   },
   "outputs": [],
   "source": [
    "def feature_pipeline_2(df, target_col=TARGET, \n",
    "                              pickup_latitude='pickup_latitude', dropoff_latitude='dropoff_latitude', \n",
    "                              pickup_longitude='pickup_longitude', dropoff_longitude='dropoff_longitude'):\n",
    "    EUCLIDEAN_FEAT = 'euclidean_dist_km'\n",
    "    HAVERSINE_FEAT = 'haversine_dist_km'\n",
    "    MANHATTAN_FEAT = 'manhattan_dist_km'\n",
    "    FEATURES = [EUCLIDEAN_FEAT, MANHATTAN_FEAT, HAVERSINE_FEAT]\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Execute previous features' pipeline\n",
    "    df_euclidean = feature_pipeline_1b(df_copy)\n",
    "    df_copy[EUCLIDEAN_FEAT] = df_euclidean[EUCLIDEAN_FEAT]\n",
    "    \n",
    "    pickup_dropoff_cols = ['pickup', 'dropoff']\n",
    "    for point in pickup_dropoff_cols:\n",
    "        point_easting, point_northing = latlon2UTM(latitudes=df_copy[f'{point}_latitude'].values,\n",
    "                                                   longitudes=df_copy[f'{point}_longitude'].values)\n",
    "        df_copy[f'{point}_easting'] = point_easting\n",
    "        df_copy[f'{point}_northing'] = point_northing\n",
    "\n",
    "    df_copy[MANHATTAN_FEAT] = manhattan_distance(x1=df_copy['pickup_northing'], y1=df_copy['pickup_easting'], \n",
    "                                                 x2=df_copy['dropoff_northing'], y2=df_copy['dropoff_easting'])/1000\n",
    "    df_copy[HAVERSINE_FEAT] = haversine_distance(lat1=df_copy[pickup_latitude], lon1=df_copy[pickup_longitude], \n",
    "                                                 lat2=df_copy[dropoff_latitude], lon2=df_copy[dropoff_longitude])\n",
    "\n",
    "    # in the EDA, we probably treat the nulls, so for now, just drop them\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    return df_copy[FEATURES + [target_col]]\n",
    "\n",
    "df_train_2 = feature_pipeline_2(df_train)\n",
    "df_test_2 = feature_pipeline_2(df_test)\n",
    "\n",
    "df_train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7N9psNc8_XW"
   },
   "outputs": [],
   "source": [
    "model_2 = LinearRegression()\n",
    "model_2.fit(df_train_2.drop(TARGET, axis=1), df_train_2[TARGET])\n",
    "\n",
    "y_test_pred_2 = model_2.predict(df_test_2.drop(TARGET, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "5Dt9GkRI7lzA",
    "outputId": "de61466f-04b0-47d4-882d-93233a158650"
   },
   "outputs": [],
   "source": [
    "rmse_2 = np.sqrt(mean_squared_error(df_test_2[TARGET], y_test_pred_2))\n",
    "iteration_results['exp_2_rmse'] = rmse_2\n",
    "print(f'RMSE Error: {rmse_2}')\n",
    "\n",
    "print_evaluation(df_test_2[TARGET], y_test_pred_2, return_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "f9xm6EBNyK4x",
    "outputId": "fef691c8-dd5c-490e-fc5c-d0e186c21159"
   },
   "outputs": [],
   "source": [
    "iteration_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NszUYghK-Gjs"
   },
   "source": [
    "## Using fastest route distance and duration - OSRM\n",
    "\n",
    "<img src=\"images/route_distance_.png\" width=\"350\"> \n",
    "<center>Blue line representing straight line distance between points A and B while real route distance is displayed by Google Maps. Source: <a href=\"https://shorturl.at/cqz37\">Google Maps</a></center>  \n",
    "\n",
    "Routers such as google maps have the city street layout with them and so they can calculate routes by different means of transportation. A open source tool works over [Open Street Maps](https://www.openstreetmap.org/about) (OSM) and can calculate these routes for us. For testing their API, they make a [demo](https://map.project-osrm.org/) and a [public server](https://github.com/Project-OSRM/osrm-backend/wiki/Demo-server) available, but in both you're limited to a limited amount of requests, in order not to overload them.   \n",
    "\n",
    "### Setup  \n",
    "\n",
    "We can setup OSRM by two ways:\n",
    "* Use a standard pre-created dockerfile - [Source](https://hub.docker.com/r/osrm/osrm-backend/)\n",
    "* Compile from source - [Source](https://github.com/Project-OSRM/osrm-backend#building-from-source)\n",
    "\n",
    "OSRM works with many configuration files, such as transportation profiles, which configure streets max speeds and allowed paths depending on which transportation mean you're using.\n",
    "\n",
    "While setting up this workshop project (with `make`) we've already set up a functional OSRM for you and you can go to next cells to test it out :)  \n",
    "  \n",
    "However, for the learning process, we can do it locally with the instructions below, which, by the way, are the same commands that the `makefile` is doing in `make setup`.  \n",
    "\n",
    "Instructions:   \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">     \n",
    "    \n",
    "* Open a terminal and go to your project folder, create a <i>router</i> folder for cleanliness\n",
    "\n",
    "* wget http://download.geofabrik.de/north-america/us/new-york-latest.osm.pbf (file is ~200 Mb) or (**preferably**) the custom smaller version of just New York city - https://amldspatial.s3.eu-central-1.amazonaws.com/new_york_city.osm.pbf\n",
    "\n",
    "* In the terminal, move to the folder where the file above was put and execute the following commands, considering that **the file name must match with the downloaded file**:\n",
    "\n",
    "* docker run -t -v \"${PWD}:/data\" osrm/osrm-backend osrm-extract -p /opt/car.lua /data/new_york_city.osm.pbf\n",
    "\n",
    "* docker run -t -v \"${PWD}:/data\" osrm/osrm-backend osrm-partition /data/new_york_city.osm.pbf\n",
    "\n",
    "* docker run -t -v \"${PWD}:/data\" osrm/osrm-backend osrm-customize /data/new_york_city.osm.pbf\n",
    "\n",
    "* docker run -t -i -p 5000:5000 -v \"${PWD}:/data\" osrm/osrm-backend osrm-routed --algorithm mld /data/new_york_city.osm.pbf  \n",
    "     \n",
    "</div>    \n",
    "\n",
    "Now you have the router running in a dedicated process. In order to test it, open another tab in the terminal and type the following:  \n",
    "\n",
    "Test: `curl \"http://localhost:5000/route/v1/driving/-73.996070,40.732605;-73.980675,40.761864?steps=false&geometries=geojson&annotations=true&overview=full\"`  \n",
    " \n",
    "This is a command to make a route between the two provided lon/lats. You can check more options at the [API documentation](http://project-osrm.org/docs/v5.5.1/api/#route-service)  \n",
    "\n",
    "\n",
    "### Calculate street distances and durations  \n",
    "\n",
    "For our pipelines, we're going to use the custom class `utils.OSRMFramework.OSRMFramework` to request and retrieve route data from a OSRM instance. When building the class, you have to send the instance's URL. When it's set up locally with docker, the address is `localhost:5000`. After it, you can use `OSRMFramework.route` sending the pick-up and dropoff's latitudes and longitudes and obtain back 5 types of data:  \n",
    "1 - and 2. - Latitude and longitudes that compose the route   \n",
    "3 - Estimated route distance  \n",
    "4 - Estimated route duration  \n",
    "5 - OSM Node ids. These will be explained in the [Traffic Prototype](#Traffic-Prototype) section.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24SDQOnJy52y"
   },
   "outputs": [],
   "source": [
    "## Small Example\n",
    "##\n",
    "lat1, lon1 = 40.732605,-73.996070\n",
    "lat2, lon2 = 40.761864,-73.980675\n",
    "\n",
    "osm = OSRMFramework(OSRM_PATH)\n",
    "lat, lon, distance, duration, node_ids = osm.route(lat1, lon1, lat2, lon2)\n",
    "\n",
    "t = latlon2linestring(lat, lon)\n",
    "\n",
    "plot_geometry(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Small Example\n",
    "##\n",
    "def get_route(osm, lat1, lon1, lat2, lon2):\n",
    "    lat, lon, distance, duration, node_ids = osm.route(lat1, lon1, lat2, lon2)\n",
    "    return_col_names = ['route', 'distance_m', 'duration_sec', 'node_ids']\n",
    "    if(type(lat) == float):\n",
    "        return pd.Series([np.nan] * len(return_col_names), index=return_col_names)\n",
    "    else:\n",
    "        return pd.Series([latlon2linestring(lat, lon), distance, duration, node_ids], index=return_col_names)\n",
    "\n",
    "routes = df_train.head().apply(lambda row: get_route(osm, \n",
    "                                            row['pickup_latitude'],\n",
    "                                            row['pickup_longitude'],\n",
    "                                            row['dropoff_latitude'],\n",
    "                                            row['dropoff_longitude']), axis=1)\n",
    "routes['route'] = GeoSeries([elem[0] if type(elem) == GeoSeries else np.nan for elem in routes['route']], index=routes.index)\n",
    "\n",
    "routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP: because router take too much time, I've saved the routes from this sample here. If you want to\n",
    "# use the router anyway, comment lines where pickle is read and uncomment commented lines\n",
    "\n",
    "def feature_pipeline_3(df, target_col=TARGET, \n",
    "                              pickup_latitude='pickup_latitude', dropoff_latitude='dropoff_latitude', \n",
    "                              pickup_longitude='pickup_longitude', dropoff_longitude='dropoff_longitude',\n",
    "                              osm_router=OSRM_PATH,\n",
    "                              test_mode=None):\n",
    "    EUCLIDEAN_FEAT = 'euclidean_dist_km'\n",
    "    HAVERSINE_FEAT = 'haversine_dist_km'\n",
    "    MANHATTAN_FEAT = 'manhattan_dist_km'\n",
    "    ROUTE_DISTANCE = 'route_distance_km'\n",
    "    ROUTE_DURATION = 'route_duration_min'\n",
    "    \n",
    "    FEATURES = [EUCLIDEAN_FEAT, \n",
    "                MANHATTAN_FEAT, \n",
    "                HAVERSINE_FEAT,\n",
    "                ROUTE_DISTANCE,\n",
    "                ROUTE_DURATION] \n",
    "\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Execute previous features' pipeline\n",
    "    df_pipeline2 = feature_pipeline_2(df_copy)\n",
    "    df_copy[EUCLIDEAN_FEAT] = df_pipeline2[EUCLIDEAN_FEAT]\n",
    "    df_copy[MANHATTAN_FEAT] = df_pipeline2[MANHATTAN_FEAT]\n",
    "    df_copy[HAVERSINE_FEAT] = df_pipeline2[HAVERSINE_FEAT]\n",
    "    \n",
    "    if(test_mode == 'train'):                                                          # TEST MODE\n",
    "        with open('data/temp_routes_train.pickle', 'rb') as f:                         # TEST MODE\n",
    "            routes = pickle.load(f)                                                    # TEST MODE\n",
    "    elif(test_mode == 'test'):                                                         # TEST MODE\n",
    "        with open('data/temp_routes_test.pickle', 'rb') as f:                          # TEST MODE\n",
    "            routes = pickle.load(f)                                                    # TEST MODE   \n",
    "        \n",
    "#     routes = df_copy.apply(lambda row: get_route(osm, \n",
    "#                                             row['pickup_latitude'],\n",
    "#                                             row['pickup_longitude'],\n",
    "#                                             row['dropoff_latitude'],\n",
    "#                                             row['dropoff_longitude']), axis=1)\n",
    "#     routes['route'] = GeoSeries([elem[0] if type(elem) == GeoSeries else np.nan for elem in routes['route']], index=routes.index)\n",
    "    df_copy[ROUTE_DISTANCE] = routes['distance_m']/1000\n",
    "    df_copy[ROUTE_DURATION] = routes['duration_sec']/60\n",
    "    \n",
    "    # in the EDA, we probably treat the nulls, so for now, just drop them\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    return df_copy[FEATURES + [target_col]], routes\n",
    "\n",
    "df_train_3, routes_train = feature_pipeline_3(df_train, test_mode='train')\n",
    "df_test_3, routes_test = feature_pipeline_3(df_test, test_mode='test')\n",
    "\n",
    "df_train_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = LinearRegression()\n",
    "model_3.fit(df_train_3.drop(TARGET, axis=1), df_train_3[TARGET])\n",
    "y_test_pred_3 = model_3.predict(df_test_3.drop(TARGET, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####\n",
    "rmse_3 = np.sqrt(mean_squared_error(df_test_3[TARGET], y_test_pred_3))\n",
    "iteration_results['exp_3_osrm'] = rmse_3\n",
    "print(f'RMSE Error: {rmse_3}')\n",
    "\n",
    "print_evaluation(df_test_3[TARGET], y_test_pred_3, return_errors=False)\n",
    "plt.xlim([0, 50]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise OSRM\n",
    "  \n",
    "1. Set up router in your local machine and call the route service using the example above\n",
    "2. Call the `route` service by using a pick-up point from inside New York city and one outside the city, in another state, for example. What happened? What if both pick-up and drop off are outside New York city?  If you want to confirm, try to use the \"Small Example\" code from below to check your assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One point inside, one outside NY\n",
    "\n",
    "inside_ny_lat, inside_ny_lon = [40.870450, -73.879747]\n",
    "outside_ny_lat, outside_ny_lon = [42.297001, -71.113800]\n",
    "\n",
    "osm = OSRMFramework(OSRM_PATH)\n",
    "lat, lon, distance, duration, node_ids = osm.route(inside_ny_lat, inside_ny_lon, outside_ny_lat, outside_ny_lon)\n",
    "\n",
    "t = latlon2linestring(lat, lon)\n",
    "\n",
    "print(f'Distance (KM): {distance/1000}')\n",
    "print(f'Duration (minutes): {duration/60}')\n",
    "plot_geometry(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both points outside, 1 points gets snapped, the other not \n",
    "\n",
    "outside1_ny_lat, outside1_ny_lon = [42.329999, -71.072593]\n",
    "outside2_ny_lat, outside2_ny_lon = [42.297001, -71.113800]\n",
    "\n",
    "osm = OSRMFramework(OSRM_PATH)\n",
    "lat, lon, distance, duration, node_ids = osm.route(outside1_ny_lat, outside1_ny_lon, outside2_ny_lat, outside2_ny_lon)\n",
    "\n",
    "t = latlon2linestring(lat, lon)\n",
    "\n",
    "print(f'Distance (KM): {distance/1000}')\n",
    "print(f'Duration (minutes): {duration/60}')\n",
    "plot_geometry(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Exploratory Analysis\n",
    "\n",
    "### Check why we have some route distance == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_3_analysis = df_train_3.merge(df_train, how='left', left_index=True, right_index=True)\n",
    "\n",
    "df_train_3_analysis['route_distance_km'] = routes_train['distance_m']/1000\n",
    "display(df_train_3_analysis.loc[df_train_3_analysis['route_distance_km'] == 0].head()) # PROBLEM pickup == dropoff\n",
    "\n",
    "##########################################################\n",
    "### Filter where pick up and drop off are the same\n",
    "##########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some leftover routes still have route_distance == 0, investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_3_analysis = df_train_3_analysis.loc[df_train.index].copy()\n",
    "\n",
    "df_train_3_analysis['distances'] = haversine_distance(df_train_3_analysis['pickup_latitude'], df_train_3_analysis['dropoff_latitude'],\n",
    "                               df_train_3_analysis['pickup_longitude'], df_train_3_analysis['dropoff_longitude'])\n",
    "\n",
    "df_train_3_analysis.loc[df_train_3_analysis['route_distance_km'] == 0].head() ### ?? - ~65 examples\n",
    "\n",
    "##########################################################\n",
    "### Filter where route_distance == 0 for next iteration###\n",
    "##########################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check price per kilometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_train_3_analysis.loc[df_train_3_analysis['route_distance_km'] != 0]\n",
    "\n",
    "price_km = temp['fare_amount_y']/temp['route_distance_km']\n",
    "display(price_km.describe(percentiles=[.9, .95, .99]))\n",
    "\n",
    "##########################################################\n",
    "### Filter where price per km > 5 - 95th percentile    ###\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed Nearest neighbors fares\n",
    "\n",
    "What do have trips in common?\n",
    "\n",
    "Last approach, we're going to find the most similar trips that were performed in the past. Based on the [fare prices specification](https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page), we can see that the day of the week, hour and distance make an impact in the final overall price. Therefore, we'll try to cover this features based on:\n",
    "* Trips that started in the same point in the week (full weekly seasonality) \n",
    "* Trips that started and finished roughly at the same locations  \n",
    "  \n",
    "Given the most similar trips for a given specific taxi trip, we can then average out their fares to produce a new feature for our model, `nn_avg_fare`. The only hyperparameter we have to deal with is the number of nearest neighbors that we are going to average our final fare from.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Small Example\n",
    "##\n",
    "def nn_avg_fare(df, target_col=TARGET, \n",
    "                pickup_latitude='pickup_latitude', pickup_longitude='pickup_longitude',\n",
    "                dropoff_latitude='dropoff_latitude', dropoff_longitude='dropoff_longitude',\n",
    "                pickup_datetime_col='pickup_datetime', n_neighbors=7, test_mode='test',\n",
    "                nn_avg_fare_model=None):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    minutes_since_monday_midnight = 'minutes_since_monday_midnight' # monday 00:00 = 0, tuesday 00:00 = 24, so on..\n",
    "    df_copy[minutes_since_monday_midnight] = df_copy[pickup_datetime_col].dt.dayofweek * (24*60) + \\\n",
    "                                               df_copy[pickup_datetime_col].dt.hour * 60 + \\\n",
    "                                               df_copy[pickup_datetime_col].dt.minute\n",
    "\n",
    "\n",
    "    nn_features = [pickup_latitude, pickup_longitude, \n",
    "                       dropoff_latitude, dropoff_longitude, \n",
    "                       minutes_since_monday_midnight]\n",
    "    nn_data = standardize_features(df_copy[nn_features])             # for k-means it's important to standardize feat.\n",
    "    nn_data[target_col] = df_copy[target_col]\n",
    "    if(test_mode == 'train'):                                        # if it's training, used dataset to create model \n",
    "        nn_avg_fare_model = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "        nn_avg_fare_model.fit(nn_data.drop(target_col, axis=1), nn_data[target_col])\n",
    "    predictions = nn_avg_fare_model.predict(nn_data.drop(target_col, axis=1))\n",
    "    return predictions, nn_avg_fare_model\n",
    "\n",
    "#####\n",
    "#####\n",
    "df_train_temp = df_train.head(50).copy()\n",
    "df_test_temp = df_train.iloc[50:53].copy()\n",
    "df_train_temp['nn_avg_fare'], nn_avg_fare_model = nn_avg_fare(df_train_temp, test_mode='train')\n",
    "df_test_temp['nn_avg_fare'] , nn_avg_fare_model = nn_avg_fare(df_test_temp, \n",
    "                                                              test_mode='test', \n",
    "                                                              nn_avg_fare_model=nn_avg_fare_model)\n",
    "\n",
    "df_test_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pipeline_4(df, target_col=TARGET, \n",
    "                              pickup_latitude='pickup_latitude', dropoff_latitude='dropoff_latitude', \n",
    "                              pickup_longitude='pickup_longitude', dropoff_longitude='dropoff_longitude',\n",
    "                              pickup_datetime_col='pickup_datetime',\n",
    "                              osm_router=OSRM_PATH, \n",
    "                              nn_avg_fare_model=None, n_neighbors=7,\n",
    "                              test_mode=None):\n",
    "    EUCLIDEAN_FEAT = 'euclidean_dist_km'\n",
    "    HAVERSINE_FEAT = 'haversine_dist_km'\n",
    "    MANHATTAN_FEAT = 'manhattan_dist_km'\n",
    "    ROUTE_DISTANCE = 'route_distance_km'\n",
    "    ROUTE_DURATION = 'route_duration_min'\n",
    "    NN_AVG_FARE = 'nn_avg_fare'\n",
    "    \n",
    "    FEATURES = [EUCLIDEAN_FEAT, \n",
    "                MANHATTAN_FEAT,\n",
    "                HAVERSINE_FEAT,\n",
    "                ROUTE_DISTANCE,\n",
    "                ROUTE_DURATION,\n",
    "                NN_AVG_FARE] \n",
    "\n",
    "    df_copy = df.copy()    \n",
    "    \n",
    "    # Execute previous features' pipeline\n",
    "    df_pipeline3, routes3 = feature_pipeline_3(df_copy, test_mode=test_mode)\n",
    "    df_copy[EUCLIDEAN_FEAT] = df_pipeline3[EUCLIDEAN_FEAT]\n",
    "    df_copy[MANHATTAN_FEAT] = df_pipeline3[MANHATTAN_FEAT]\n",
    "    df_copy[HAVERSINE_FEAT] = df_pipeline3[HAVERSINE_FEAT]\n",
    "    df_copy[ROUTE_DISTANCE] = df_pipeline3[ROUTE_DISTANCE]\n",
    "    df_copy[ROUTE_DURATION] = df_pipeline3[ROUTE_DURATION]\n",
    "\n",
    "    \n",
    "    ###### Outliers Filtering\n",
    "    # in the EDA, we probably treat the nulls, so for now, just drop them\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    df_copy = df_copy.loc[df_copy[ROUTE_DISTANCE] != 0].copy()\n",
    "    \n",
    "    price_per_km = df_copy[target_col]/(df_copy[ROUTE_DISTANCE])\n",
    "    df_copy = df_copy.loc[price_per_km < 5].copy()\n",
    "    ######\n",
    "    \n",
    "    ###### NN avg fare model\n",
    "    if(test_mode == 'train'):\n",
    "        df_copy[NN_AVG_FARE], nn_avg_fare_model = nn_avg_fare(df_copy, test_mode='train', n_neighbors=n_neighbors)\n",
    "    else:\n",
    "        df_copy[NN_AVG_FARE], nn_avg_fare_model = nn_avg_fare(df_copy, test_mode='test', \n",
    "                                                              nn_avg_fare_model=nn_avg_fare_model)\n",
    "    ######\n",
    "    \n",
    "    return df_copy[FEATURES + [target_col]], routes3, nn_avg_fare_model\n",
    "\n",
    "df_train_4, routes_train, nn_avg_fare_model = feature_pipeline_4(df_train, test_mode='train')\n",
    "df_test_4, routes_test, nn_avg_fare_model = feature_pipeline_4(df_test, test_mode='test', nn_avg_fare_model=nn_avg_fare_model)\n",
    "\n",
    "df_train_4.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = LinearRegression()\n",
    "model_4.fit(df_train_4.drop(TARGET, axis=1), df_train_4[TARGET])\n",
    "y_test_pred_4 = model_4.predict(df_test_4.drop(TARGET, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####\n",
    "rmse_4 = np.sqrt(mean_squared_error(df_test_4[TARGET], y_test_pred_4))\n",
    "iteration_results['exp_4_knn_rmse'] = rmse_4\n",
    "print(f'RMSE Error: {rmse_4}')\n",
    "\n",
    "print_evaluation(df_test_4[TARGET], y_test_pred_4, return_errors=False)\n",
    "plt.xlim([0, 50]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">     \n",
    "<b>Personal Note</b>: Although we've seen improvements in this iteration, it's in the modelling's objetive to determine the usefulness of this feature. The previous iterations' results didn't improve much because, besides that the features were correlated, they also didn't capture all the factors that compose the final fare price given the dataset that we were given to.  \n",
    "     \n",
    "The KNNRegression approach that we execute, capture close by trips and \"mimics\" their final price as feature, without putting much thought on what are the <b>main driving factors that compose the final fare</b>. If performance is the objective, this is always a good feature. However, if we were modelling/explaining this process, I wouldn't personally go for this feature\n",
    "</div>  \n",
    "\n",
    "**Can you think of more features?**\n",
    "\n",
    "### Exercise KNNRegressor  \n",
    "\n",
    "Using the codes above, define what is the best value of `n_neighbors` that minimizes the test RMSE?\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "## Enter values to check different values for neighbors\n",
    "###########################################################################################################\n",
    "# replace 1 by the minimum number of neighbors you want to test\n",
    "START_N = 1\n",
    "\n",
    "# replace 2 by the maximum number of neighbors you want to test (if you chose a too high number and the execution\n",
    "# takes too long: interrupt the kernel with the square symbol and choose a smaller number)\n",
    "END_N = 2\n",
    "###########################################################################################################\n",
    "\n",
    "n_knn = range(START_N, END_N+1)\n",
    "for n in n_knn:\n",
    "    df_train_ex_knn, _, nn_avg_fare_model = feature_pipeline_4(df_train, test_mode='train', n_neighbors=n)\n",
    "    df_test_ex_knn, _, _ = feature_pipeline_4(df_test, test_mode='test', nn_avg_fare_model=nn_avg_fare_model)\n",
    "    \n",
    "    model_ex_knn = LinearRegression()\n",
    "    model_ex_knn.fit(df_train_ex_knn.drop(TARGET, axis=1), df_train_ex_knn[TARGET])\n",
    "    y_test_pred_ex_knn = model_ex_knn.predict(df_test_ex_knn.drop(TARGET, axis=1))\n",
    "    \n",
    "    rmse_ex_knn = np.sqrt(mean_squared_error(df_test_ex_knn[TARGET], y_test_pred_ex_knn))\n",
    "    print(f'RMSE Error - N = {n}: {rmse_ex_knn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "\n",
    "<img src=\"images/toc_2.png\" width=\"500\"> \n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "### How to speed up spatial queries:\n",
    "\n",
    "<img src=\"images/spatial_indexes.png\" width=\"500\"> \n",
    "<center>Example of spatial indexes. Source: <a href=\"https://www.youtube.com/watch?v=_95bSEqMzUA\">Youtube: Alexander Müller - Spatial Range Queries Using Python In-Memory Indices</a></center>  \n",
    "\n",
    "**Spatial Indexes**:   \n",
    "  \n",
    "<img src=\"images/r_tree.png\" width=\"600\"> \n",
    "<center>Insert process of a R-Tree</center>  \n",
    "  \n",
    "<div class=\"alert alert-block alert-info\">     \n",
    "The R-Tree helps with filtering out the majority of the dataset that lives outside of the point's bounding box. After the big filtering is done, all the other calculations follow in the same way.  \n",
    "</div>   \n",
    "      \n",
    "[Here](https://geoffboeing.com/2016/10/r-tree-spatial-index-python/) is an example that filter streets based on a city polygon using R-Trees.\n",
    "  \n",
    "Lets say we wanted to check which routes intersect with a route of interest that we manually selected. [Geopandas](http://geopandas.org/) can perform [spatial joins](https://medium.com/@bobhaffner/spatial-joins-in-geopandas-c5e916a763f3) in order to join tables [based on the relationship types between two geometry columns](https://shapely.readthedocs.io/en/latest/manual.html#binary-predicates). Here, we use the `intersects` operation in GeoPandas to check whether two linestrings intersect each other. However, the operation is unfortunately still [done element per element](https://matthewrocklin.com/blog/work/2017/09/21/accelerating-geopandas-1). With the R-Tree, we still do element by element search, but we can firstly subselect close-by elements and then perform `intersects` with a small subset of the total dataset. With bigger datasets, this difference in approaches can lead to big differences in performance, as we can see in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_r_tree(gdf, geometry_col):\n",
    "    idx = index.Index()\n",
    "    #Populate R-tree index with bounds of grid cells\n",
    "    for ix, cell in gdf.iterrows():\n",
    "        # in GeoPandas, there's always a geometry col and it's always a shapely \n",
    "        idx.insert(ix, cell[geometry_col].bounds)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_geoix = routes_train.loc[(routes_train['distance_m'] > 100) & (routes_train['distance_m'] < 500)]\n",
    "df_geoix = GeoDataFrame(df_geoix, geometry='route') # convert df to GeoDf, allow methods such as 'intersects'\n",
    "n_samples = np.arange(7500, 75001, 7500)            # different resample sizes (10)\n",
    "times_no_geoix = []                                 # store processing time for searches with no spatial index\n",
    "times_geoix = []                                    # store processing time for searches with R-Tree\n",
    "intersect_line = df_geoix.loc[1584]['route']        # linstring geometry\n",
    "\n",
    "for n_sample in n_samples:\n",
    "    print(f'N Samples: {n_sample}')\n",
    "    df_geoix_sample = df_geoix.sample(n_sample, replace=True, random_state=42).reset_index(drop=True)\n",
    "    idx = create_r_tree(df_geoix_sample, 'route')\n",
    "    \n",
    "    #### without index\n",
    "    time_no_geoix = time.time()\n",
    "    filter_1 = df_geoix_sample.intersects(intersect_line)\n",
    "    routes_ix = filter_1[filter_1 == True].index\n",
    "    times_no_geoix.append(time.time() - time_no_geoix)\n",
    "    print(f'* intersecting routes found without index: {len(routes_ix)}')\n",
    "    \n",
    "    #### with index\n",
    "    # Filter possible candidates by bouding boxes\n",
    "    time_with_geoix = time.time()\n",
    "    idxs = list(idx.intersection(intersect_line.bounds))\n",
    "    if(len(idxs) > 0):\n",
    "        # Now do actual intersection\n",
    "        filter_2 = df_geoix_sample.loc[idxs].intersects(intersect_line)\n",
    "        routes_ix = df_geoix_sample.loc[filter_2[filter_2 == True].index]\n",
    "        times_geoix.append(time.time() - time_with_geoix)\n",
    "        print(f'* intersecting routes found with index: {len(routes_ix)} \\n')\n",
    "    \n",
    "plt.plot(n_samples, times_no_geoix)\n",
    "plt.plot(n_samples, times_geoix)\n",
    "plt.title('Comparison between search times without index and R-Tree')\n",
    "plt.xlabel('Number of Geometries')\n",
    "plt.ylabel('Search Time (seconds)')\n",
    "plt.legend(['Without Index', 'R-Tree']);        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">     \n",
    "You might have notices the first line of code in the code block above, where I filtered for short routes. The reason I've done this is that the spatial indexes are efficient when the object you're trying to join with results in a small number of intersection with other elements. If we had selected a long street that intersected with most routes from our dataset, the index would filter almost no elements and the following search operation would be similar to the one performed by GeoPandas\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "\n",
    "<img src=\"images/toc_3.png\" width=\"500\"> \n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "# Traffic Prototype\n",
    "\n",
    "<img src=\"images/traffic.png\" width=\"500\"> \n",
    "<center>Traffic information from Google Maps</center> \n",
    "\n",
    "\n",
    "## GPS Traces\n",
    "\n",
    "<img src=\"images/bad_gps.png\" width=\"500\"> \n",
    "<center>Bad GPS Signal. Source: <a href=\"https://www.gps.gov/systems/gps/performance/accuracy/\">https://www.gps.gov/systems/gps/performance/accuracy/</a></center>  \n",
    "\n",
    "\n",
    "## OSM Data Representation\n",
    "\n",
    "### Nodes, Ways and Relations  \n",
    "\n",
    "<img src=\"images/osm_data_types_.jpg\" width=\"500\"> \n",
    "<center>Data types in OSM</center>  \n",
    "\n",
    "Everything that is inside OSM is represent by one of three possible geometries ([Reference](https://labs.mapbox.com/mapping/osm-data-model/)):\n",
    "* **Node** Example: https://www.openstreetmap.org/node/6343276469  \n",
    "* **Way** Example: https://www.openstreetmap.org/way/4402228\n",
    "* **Relations** Example: https://www.openstreetmap.org/relation/1685018  \n",
    "\n",
    "### Tags\n",
    "  \n",
    "Tags are the metadata of each data type defined above. All data types contains a set of possible (but not mandatory) tags related to the tags' semantics. For instance, if you look at the left side of the screen in any of the data types examples above, you'll see that:  \n",
    "* **Nodes** Example: `latitude/longitude`\n",
    "* Street **Ways** Example: `bicycle path`, if it has a `sidewalk` and mainly, what is it's `maximum driving speed`. **It doesn't always have maximum speed**. \n",
    "* An important feature about **Relations** is their [administrative level](https://wiki.openstreetmap.org/wiki/Key:admin%20level?uselang=en-GB).  \n",
    "  \n",
    "## Traffic Proof of Concept (POC)\n",
    "\n",
    "OSRM allows you to define what is the street speed by allowing you to provide an external CSV file that sets it. For that, you need to define the speeds segment by segment, *i.e.*, by each pair of sequential node ids, you can set what is the street speed at that point ([Reference](https://github.com/Project-OSRM/osrm-backend/wiki/Traffic)).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo route command with previous router\n",
    "\n",
    "lat1, lon1 = 40.732605,-73.996070\n",
    "lat2, lon2 = 40.761864,-73.980675\n",
    "\n",
    "lat, lon, distance, duration, node_ids = osm.route(lat1, lon1, lat2, lon2)\n",
    "\n",
    "t = latlon2linestring(lat, lon)\n",
    "\n",
    "plot_geometry(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each segment between 2 nodes, set speed to lowest possible = 1 km/h. `NOT 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_from = []\n",
    "node_to = []\n",
    "node_speed = []\n",
    "i = 1\n",
    "while i < len(node_ids):\n",
    "    node_from.append(node_ids[i-1])\n",
    "    node_to.append(node_ids[i])    \n",
    "    node_speed.append(1)\n",
    "    i += 1\n",
    "blocked = pd.DataFrame({'node_from': node_from, 'node_to': node_to, 'node_speed': node_speed})\n",
    "\n",
    "# Save file with segment speeds with no columns headers nor row indexes\n",
    "# blocked.to_csv('router/test_traffic.csv', header=False, index=False)\n",
    "\n",
    "blocked.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset router updating street speeds informations\n",
    "\n",
    "The procedure here is the same as the previous router set up. The `only difference is the new parameter in osrm-customize` \n",
    "  \n",
    "--> `--segment-speed-file /data/traffic_file.csv`.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "\n",
    "* <b>Running in you local machine:</b>  \n",
    "\n",
    "* docker run -t -v \"${PWD}:/data\" osrm/osrm-backend osrm-extract -p /opt/car.lua /data/router/new_york_city.osm.pbf\n",
    "\n",
    "* docker run -t -v \"${PWD}:/data\" osrm/osrm-backend osrm-partition /data/router/new_york_city.osm.pbf\n",
    "\n",
    "* docker run -t -v \"${PWD}:/data\" osrm/osrm-backend osrm-customize /data/router/new_york_city.osm.pbf `--segment-speed-file /data/data/test_traffic.csv`\n",
    "\n",
    "* docker run -t -i -p 5000:5000 -v \"${PWD}:/data\" osrm/osrm-backend osrm-routed --algorithm mld /data/router/new_york_city.osm.pbf  \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "\n",
    "* <b>Manual execution for AMLD Workshop, in the workshop's root directory:</b>  \n",
    "\n",
    "* docker-compose stop osrm-router\n",
    "\n",
    "* docker-compose rm osrm-router\n",
    "\n",
    "* docker-compose create osrm-router\n",
    "\n",
    "* docker run -t -v \"${PWD}:/data\" osrm/osrm-backend osrm-customize /data/router/new_york_city.osm.pbf --segment-speed-file /data/data/test_traffic.csv  \n",
    "\n",
    "* docker-compose start osrm-router\n",
    "</div>\n",
    "  \n",
    "The commands above are already coded in the file `restart_osrm_traffic.sh`. **If you can't execute the shell script, like Windows users, you can execute the above commands by copying/pasting them in your terminal.** By executing it, we're able to stop the current router in the docker-compose containers, update the router with traffic information and restart it without having the stop the workshop's docker-compose structure, *i.e.*, without stoping this jupyter notebook.  \n",
    "  \n",
    "* In your terminal, in the workshop's root folder, execute the shell script.\n",
    "\n",
    "### Call router again and check new route\n",
    "\n",
    "With the new router set up, we can calculate the route for the same pick-up/drop-off points as before and see how it's changed. We can see that the router avoids as best as possible to stay away from the segments we marked as heavy traffic, *i.e.*, segment speed = 1 km/h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lon, distance, duration, node_ids = osm.route(lat1, lon1, lat2, lon2)\n",
    "\n",
    "t = latlon2linestring(lat, lon)\n",
    "\n",
    "plot_geometry(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match noisy GPS to Node IDs\n",
    "\n",
    "The POC works fine if you have the node id's for all your route points, which unfortunately, isn't the case.  \n",
    "Besides:  \n",
    "\n",
    "- GPS sampling might be not stable, *e.g.*, some times we have sampling of 5 secs, other times 10 secs.\n",
    "- Points might not fall exactly on the street due to GPS inaccuracy\n",
    "\n",
    "So, lets recreate a possible real world GPS trace by taking a trace returned by the router in section [Embed Nearest neighboors fares](#Embed-Nearest-neighboors-fares) and disturb it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = df_train.merge(routes_train, how='left', left_index=True, right_index=True)\n",
    "trace = lines['route'].iloc[30]\n",
    "\n",
    "lon, lat = trace.xy[0], trace.xy[1]\n",
    "\n",
    "lat, lon = np.delete(lat, -3),  np.delete(lon, -3)       # cut corner at the bottom part\n",
    "np.random.seed(42)                                       # choose 10 random points to disturb\n",
    "choices = np.random.choice(range(len(lat)), size=10, replace=False)\n",
    "lat[choices] = lat[choices] + np.random.normal(0, 0.001) # add noise \n",
    "lon[choices] = lon[choices] + np.random.normal(0, 0.001) # add noise\n",
    "\n",
    "t = latlon2linestring(lat, lon)\n",
    "\n",
    "plot_geometry(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of the points don't even fall on a street segment and, like in the route close to the bottom-right side, a node indicating a turn is missing in a way that, when we connect the nodes with a line, the line goes through the building blocks.  \n",
    "  \n",
    "As we know that every node in OSM has a lat/lon associated with it, we could associate each trace point to the closest node. As Newson and Krumm showed ([Reference](https://www.ismll.uni-hildesheim.de/lehre/semSpatial-10s/script/6.pdf)):\n",
    "* This is error prone, as this kind of approach in street condensed networks can match GPS points to unrelated street segments. \n",
    "* The approach doesn't take the previous and future GPS points in order to match a GPS point to the street, *i.e.*, if previous and next point are over a bridge, it's highly improbable that the current matched GPS point is outside the bridge, even if it's closer to a node outside the bridge.  \n",
    "  \n",
    "We can see this kind of examples in the picture below:  \n",
    "\n",
    "<img src=\"images/map_matching_example.png\" width=\"300\"> \n",
    "<center>GPS matching potential errors - as in <a href=\"https://www.ismll.uni-hildesheim.de/lehre/semSpatial-10s/script/6.pdf\">https://www.ismll.uni-hildesheim.de/lehre/semSpatial-10s/script/6.pdf</a></center> \n",
    "\n",
    "### Snap noisy GPS points to street using map matching \n",
    "\n",
    "Newson and Krumm created an approach that takes into account the whole sequence of nodes and then try to match them in a probabilistic way. \n",
    "  \n",
    "OSRM already provide it for us out of the box and we can use the OSRMFramework class to extract the main information using the router we have already set up.  \n",
    "* API - http://project-osrm.org/docs/v5.5.1/api/#match-service  \n",
    "  \n",
    "Main parameters to be understood here are:  \n",
    "* **geometries**\n",
    "* **timestamps**\n",
    "* **radiuses**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "\n",
    "In the complete version of this notebook, we fully describe the parameter above. Check it out. \n",
    "</div>\n",
    "\n",
    "Lets now take a look of how the disturbed street is after map matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lat, lon, nodes_id = osm.match(lat, lon, timestamps=None, radiuses=None)\n",
    "\n",
    "t = latlon2linestring(lat, lon)\n",
    "\n",
    "plot_geometry(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Data Processing\n",
    "\n",
    "<img src=\"images/traffic_sequential.jpg\" width=\"400\"> \n",
    "<center>Trace accumulation per time window</center> \n",
    "  \n",
    "So until now, we know how we can have multiple GPS points and their associated node ids and timestamps. However, `how do I turn them into traffic information?`  \n",
    "\n",
    "Two sources of data: \n",
    "* **Real time**\n",
    "* **Historical aggregates**\n",
    "\n",
    "You can check that these are the two options used by google here: https://www.google.com/maps/place/Nova+Iorque,+NY,+EUA/@40.6971494,-74.2598655,10z/data=!3m1!4b1!4m5!3m4!1s0x89c24fa5d33f083b:0xc80b8f06e177fe62!8m2!3d40.7127753!4d-74.0059728!5m1!1e1\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "The accessibility to public car's GPS traces is still rare. When companies have access to this kind of information, they usually don't make publicly available. In this traffic section we're working with a mock of GPS signals, made by OSRM itself to build routes from A to B. The objective of this part then is not to analyze data or summaries statistics, but to understand possible techniques on how to embed traffic information into your OSRM.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_k = lines.loc[lines['pickup_datetime'].dt.year == 2015][['pickup_datetime', 'node_ids']].copy()\n",
    "lines_k['pickup_datetime'] = lines_k['pickup_datetime'].dt.strftime('%H:00:00')\n",
    "\n",
    "display(lines_k.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **objective** of the following code blocs is to convert the dataframe above into a a list line segments (2 node ids), the hour bin and how many times this segment appeared in this time bin, like the example below:\n",
    "\n",
    "| datetime | count | segment linestring|\n",
    "|------|------|------|\n",
    "|   13:00  | 5 | LINESTRING (-73.9600437 40.7980478, -73.959547...|  \n",
    "\n",
    "For that we need to:\n",
    "1. Process every node id pair from every route, *i.e.*, the route's segments\n",
    "2. Count how many times the segment occurred grouped by hour bin\n",
    "3. Extract node's position (lat/lon) and convert segments to geometric formats  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Process every node id pair from every route, i.e., the route's segments \n",
    "\n",
    "from_node = []\n",
    "to_node = []\n",
    "seg_date = []\n",
    "for ix, row in lines_k.dropna().iterrows():\n",
    "    row_df = pd.DataFrame()\n",
    "    lat_lon_pairs = []\n",
    "    i = 1\n",
    "    while i < len(row['node_ids']):\n",
    "        from_node.append(row['node_ids'][i-1])\n",
    "        to_node.append(row['node_ids'][i]) \n",
    "        seg_date.append(row['pickup_datetime'])\n",
    "        i += 1\n",
    "\n",
    "seg_df = pd.DataFrame({'seg_date': seg_date, 'from_node': from_node, 'to_node': to_node})\n",
    "seg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Count how many times the segment occurred grouped by hour bin\n",
    "\n",
    "def count_duplicates(group):\n",
    "    group_res = group.groupby(['from_node', 'to_node']).size().reset_index().rename(columns={0:'records'})\n",
    "    group_res['seg_date'] = group.name\n",
    "    return group_res\n",
    "\n",
    "group_res = seg_df.groupby('seg_date').apply(count_duplicates).reset_index(drop=True)\n",
    "group_res.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks 1 and 2 are straightforward, needing basically some processing. However, for task 3, we need to map our node ids to lat/lons and, unfortunately, **OSRM doesn't provide us with that information**, for optimization reasons. You check some issues in Github [here](https://github.com/Project-OSRM/osrm-backend/issues/5310) and [here](https://github.com/Project-OSRM/osrm-backend/issues/5490) and some more at their Github repository. Enter...[OSMNx](https://geoffboeing.com/2016/11/osmnx-python-street-networks/).  \n",
    "\n",
    "### Open Street Maps + NetworkX (OSMnx) and RouteAnnotator   \n",
    "\n",
    "<img src=\"images/lausanne_.png\" width=\"350\"> \n",
    "<center>Lausanne's driving network</center>  \n",
    "\n",
    "OSMnx is a framework that works over OSM data and has multiple ways to extract data about street networks, including, nodes, ways, relations and **all the tags associated with them**.  \n",
    "  \n",
    "The `RouteAnnotator` class access OSMnx, retrieve desired street network and extract all metadata related to nodes, ways and node segments (node pairs and nodes belonging to same way). For now, it can only extract a network based on [osmnx.graph_from_place](https://osmnx.readthedocs.io/en/stable/osmnx.html#osmnx.core.graph_from_place) function but you can read more about other ways to retrieve data in OSMnx's documentation.  \n",
    "\n",
    "RouteAnnotator creates three main functions similar to [Mapbox's route-annotator](https://github.com/mapbox/route-annotator). Its main functions are:\n",
    "* **segment_lookup** - provided a list of node ids with size $N$, it returns $N-1$ segments containing data such as the way_id that this segment belongs to\n",
    "* **way_lookup** - provided a list of ways_id, it returns the metadata associated with all ways, including an ordered list of node_ids that compose that way\n",
    "* **node_lookup** - provided a list of node_ids, it returns the metadata associated with all nodes, including latitude and longitude.  \n",
    "  \n",
    "We're going to use RouteAnnotator in order to retrieve the nodes' lat/lon and keep processing our traffic dataset:  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">  \n",
    "<b>Observation</b>: OSMnx download a uncompressed geoJSON file from OSM and this file tends to be quite big. While executing the code block below inside docker, we had memory problems because of that, so we decided to execute locally, save the lookups and importing them locally for this workshop. The lookups are exactly the same as if you executed the commented lines, but mind the memory consumption, as it can break your container.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and process new york city street graph\n",
    "#\n",
    "# ra = RouteAnnotator('new york, USA', 'drive_service')\n",
    "# ra.build_lookups()\n",
    "\n",
    "# AMLD version, load already saved lookups\n",
    "ra = RouteAnnotator.AMLD_local_lookups('new york, USA', 'drive_service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract node's position (lat/lon) and convert segments to geometric formats\n",
    "\n",
    "line_seg = []\n",
    "for ix, row in group_res.iterrows():\n",
    "    try:\n",
    "        metadata1 = ra.node_lookup(row['from_node'])  # use RouteAnnotator to retrieve node's metadata\n",
    "        m1_lat_lon = [metadata1['y'], metadata1['x']] # such as lat/lon\n",
    "        metadata2 = ra.node_lookup(row['to_node'])\n",
    "        m2_lat_lon = [metadata2['y'], metadata2['x']]\n",
    "        line_segment = latlon2linestring(lat = [m1_lat_lon[0], m2_lat_lon[0]], \n",
    "                                         lon = [m1_lat_lon[1], m2_lat_lon[1]])[0]\n",
    "        line_seg.append(line_segment)\n",
    "    except Exception:\n",
    "        line_seg.append(np.nan)\n",
    "        continue\n",
    "        \n",
    "\n",
    "group_res = GeoDataFrame(group_res, geometry=line_seg)\n",
    "group_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have the street segment and their respective \"popularity\", *i.e.*, count, aggregated by hour. In a real GPS database, the `records` columns would be replaced by the average speed of gps traces that passed through it. Lets visualize it!  \n",
    "\n",
    "After finalizing the dataset above, we could save them as the first CSV file that we've saved containing our desired metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = keplergl.KeplerGl(height=500)\n",
    "w1.add_data(data=group_res[['seg_date', 'geometry', 'records']].dropna(), name='traces')\n",
    "w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "\n",
    "<img src=\"images/toc_4.png\" width=\"500\"> \n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "# Retrieving Extra Map Features\n",
    "\n",
    "Objective: Extract extra information, we're open to new kind of possibilities when doing our analysis/features:\n",
    "* Do you want to check what are the most illuminated streets in your city? Maybe you can build a router only for safe illuminated streets, or [find the streets with most lamps for your happy dog](http://sk53-osm.blogspot.com/2013/04/maps-for-dogs-or-lamp-posts-in-chains.html)\n",
    "* You can investigate which neighbors have [better living features](https://github.com/Z3tt/30DayMapChallenge/blob/master/Day15_Names/Names_BerlinRoads.pdf), such as parks, benches, business.  \n",
    "  \n",
    "We can locally search for nearby elements in OSM by clicking with the right click over a map region an choosing `query features`, like in the image below:  \n",
    "\n",
    "<img src=\"images/osm_query_features_.png\" width=\"600\"> \n",
    "<center>Local features query in OSM</center>  \n",
    " \n",
    "\n",
    "## pyosmium  \n",
    "\n",
    "As the name implies, pyosmium is a framework to work with the [Osmium Library](https://osmcode.org/libosmium/) from OSM. It provides a sequential way to access each of OSM's data types and callbacks for data processing.  \n",
    "\n",
    "To start working with it:\n",
    "* We first create a class that inherits from `osmium.SimpleHandler`\n",
    "* Pyosmium will provide standards callback methods for every element present inside the osm.pbf file:\n",
    "    * `way`\n",
    "    * `node`\n",
    "    * `relation`\n",
    "* And a standard method to read the osm.pbf file, called `apply_file`  \n",
    "  \n",
    "Lets say you want to check where and how distributed are New York's schools. The *tag* that defines the node as a school, or other types of objects is usually defined by a tag `amenity`, in this case, == `school`. There's a really good [list of amenities at OSM wikipedia page](https://wiki.openstreetmap.org/wiki/Key:amenity). Lets take a look on an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPyosmium(osmium.SimpleHandler):          # class must inherit from osmium.SimpleHandler\n",
    "    def __init__(self, pbf_path, amenity_name):\n",
    "        osmium.SimpleHandler.__init__(self)        # initialize pyosmium\n",
    "        self.amenity_name = amenity_name           # parameter, search for this specific amenity\n",
    "        self.wkb_fab = osmium.geom.WKBFactory()    # builds geometry over OSM objects\n",
    "        self.points = []                           # store points geometries\n",
    "        self.names = []                            # store points names\n",
    "        \n",
    "        self.apply_file(pbf_path, locations=True)  # initialize osm.pbf file process\n",
    "        \n",
    "        self.points = GeoSeries(self.points)       # AFTER process is done, convert collected geometry into GeoSeries\n",
    "        self.df = GeoDataFrame({'name': self.names}, geometry=self.points) # convert all data to DataFrame\n",
    "        \n",
    "        \n",
    "    def node(self, node):\n",
    "        # TagList can't be converted to dict automatically, see:\n",
    "        # https://github.com/osmcode/pyosmium/issues/106\n",
    "        tags_dict = {tag.k: tag.v for tag in node.tags}\n",
    "        if('amenity' in tags_dict.keys() and tags_dict['amenity'] == self.amenity_name):\n",
    "            wkb = self.wkb_fab.create_point(node)   # extract Point's hex location data\n",
    "            points = wkblib.loads(wkb, hex=True)    # convert hex data to WKB geometry format\n",
    "            self.points.append(points)              # store geometry in list\n",
    "            self.names.append(tags_dict['name'] if 'name' in tags_dict.keys() else '') # store name IF name exist\n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "PBF_PATH = 'data/new_york_city.osm.pbf'\n",
    "test_pyosmium = TestPyosmium(PBF_PATH, amenity_name='school')\n",
    "print(f'Execution time: {(time.time() - init_time)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geometry(test_pyosmium.df['geometry'], marker_cluster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise pyosmium \n",
    "\n",
    "Provide a helper map for tourists in NY showing them where are the concentration of drinkable water or public toilets in the city.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_time = time.time()\n",
    "PBF_PATH = 'data/new_york_city.osm.pbf'\n",
    "test_pyosmium = TestPyosmium(PBF_PATH, amenity_name='toilets')\n",
    "print(f'Execution time: {(time.time() - init_time)/60} minutes')\n",
    "\n",
    "plot_geometry(test_pyosmium.df['geometry'], marker_cluster=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things I'd like to cover if we had infinite workshop time\n",
    "\n",
    "- **Spatial Statistics and pysal**  \n",
    "- **GPS Traces database, cleaning and processing**  \n",
    "- **Demand Modelling and Demand Prediction**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AMLD_2020.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
