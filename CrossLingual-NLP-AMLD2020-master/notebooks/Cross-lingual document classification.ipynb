{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem description\n",
    "\n",
    "Cross-lingual document classification (CLDC) is the text mining problem where we are given:\n",
    "- labeled documents for training in a source language $\\ell_1$, and \n",
    "- test documents written in a target language $\\ell_2$. \n",
    "\n",
    "For example, the training documents are written in English, and the test documents are written in French. \n",
    "\n",
    "\n",
    "CLDC is an interesting problem. The hope is that we can use resource-rich languages to train models that can be applied to resource-deprived languages. This would result in transferring knowledge from one language to another. \n",
    "There are several methods that can be used in this context. In this workshop we start from naive approaches and progressively introduce more complex solutions. \n",
    "\n",
    "The most naive solution is to ignore the fact the training and test documents are written in different languages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from collections import Counter\n",
    "from src.models import *\n",
    "from src.utils import *\n",
    "from src.dataset import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/images/classes.png\">\n",
    "\n",
    "1. Dataset: holds the data of sources and target language\n",
    "2. System: This is a set of steps: Does fit, predict. Can be in the form of a pipeline also\n",
    "3. Experiment: Given a Dataset and a System it fits, predicts and reports evaluation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop we will use a dataset from the [SemEval](http://alt.qcri.org/semeval2015/) workshop for the Sentiment Analysis task. While the tasks have three classes, that is **Positive, Negative, Neutral**, we will use only two classes in order to simplify it. So, let's load the data for a pair of languages and check a few statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\"../data/datasets/\",\"en\", \"es\")\n",
    "\n",
    "dataset.load_data()\n",
    "#To check the arguments of the function\n",
    "#print(dataset.load_cl_embeddings.__doc__)\n",
    "dataset.load_cl_embeddings(\"../\",300,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the counts on the classes for the source language\n",
    "sns.countplot(dataset.y_train,order=[\"negative\",\"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And for the Spanish dataset\n",
    "sns.countplot(dataset.y_test,order=[\"negative\",\"positive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the datasets are unbalanced as we have much more positive comments that negative ones. We will start by establishing a few baselines and see how we can improve over them by leveraging cross-lingual word embeddings. We will start with a dummy classifier that will respect the distribution of the classes to generate some random predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep the scores of all the expriments in a table\n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Model\", \"f-score\"]\n",
    "\n",
    "# Majority Class\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer()), \n",
    "                     ('classifier', DummyClassifier(\"stratified\"))])\n",
    "runner = Runner(pipeline, dataset)\n",
    "score = runner.eval_system()\n",
    "x.add_row([\"Dummy\", format_score(score)])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a model that just uses term frequencies in order to represent the documents. We expect that in cases where the source and target languages share a part of the vocabulary, for example in latin languages, this approach can potentially give descent results. We will just use unigrams for this exercice but of course you can alter this baseline in order to leverage character n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression on words\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer(lowercase=True)), \n",
    "                     ('classifier', LogisticRegression(solver=\"lbfgs\"))])\n",
    "runner = Runner(pipeline, dataset)\n",
    "score = runner.eval_system()\n",
    "x.add_row([\"LR unigrams\",format_score(score)])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see now how we can leverage the cross-lingual word embeddings in order to perform zero-shot learning. A simple but effective baseline consists of averaging the word embeddings in each document in order to come up with a document (or sentence) representation. We will do that by using a look-up table in order to pull the appropriate cross-linual word embeddings for each document as it is shown in the diagram:\n",
    "\n",
    "![](../data/images/vec_average.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw during the introduction we use a binary representation for the document terms which we use to perform a look-up in the embeddings matrix of size $V\\times d$, where $V$ is the size of the vocabulary and $d$ the dimension of the latent space, and pull the vectors. In the example we will pull three vectors. Finally, we will just calculate our document vector by just averaging the vectors. We will repeat this operation for each document in both the target and the source languages. Then we will follow the zero-shot learning framework and we will train a classifier on the source language and predict on the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, myclf in zip(['Knn-nBow', 'LR-nBow'],[KNeighborsClassifier(n_neighbors=2), LogisticRegression(C=10, solver=\"lbfgs\")]):\n",
    "\n",
    "    avg_baseline = nBowClassifier(myclf,dataset.source_embeddings,dataset.target_embeddings)\n",
    "\n",
    "    pipeline = Pipeline([('vectorizer', CountVectorizer(lowercase=True,vocabulary=dataset.vocab_)), \n",
    "                         ('classifier', avg_baseline)])\n",
    "\n",
    "    runner = Runner(pipeline, dataset)\n",
    "    x.add_row([name, format_score(runner.eval_system())])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following model we will use the LASER representations in order to train the classifiers within the same framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, myclf in zip(['Knn-laser', 'LR-laser'],[KNeighborsClassifier(n_neighbors=2), LogisticRegression(C=10, solver=\"lbfgs\")]):\n",
    "    laser_clf = LASERClassifier(myclf, dataset.source_lang, dataset.target_lang)\n",
    "    pipeline = Pipeline([(\"doc2laser\",Doc2Laser()),('classifier', laser_clf)])\n",
    "    pipeline.set_params(doc2laser__lang=dataset.source_lang)\n",
    "    pipeline.fit(dataset.train,dataset.y_train)\n",
    "    runner = Runner(pipeline, dataset)\n",
    "\n",
    "    pipeline.set_params(doc2laser__lang=dataset.target_lang)\n",
    "    x.add_row([name, format_score(runner.eval_system(prefit=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the zero-shot learning using LASER representations can achieve state-of-the-art results in this pair of languages. \n",
    "\n",
    "***Exercises:*** \n",
    "\n",
    "* Use other pairs of languages and see the performance. For example, you can try to transfer from more distant languages like Russian.\n",
    "* Write a function in order to calculate all the pairs of (source,target) languages and compare the results.\n",
    "* Tune the classifier or use other type of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
