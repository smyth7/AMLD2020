{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Coding3_ToThePoint_RLworkshop.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fAA1s_fFJZAN","colab_type":"text"},"source":["# Finding the optimal policy"]},{"cell_type":"code","metadata":{"id":"F2Z8BNMEMtUA","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9RpY31zrNrbx","colab_type":"text"},"source":["### Define the graph as an MDP with states, actions & transitions\n","\n","![graph](https://drive.google.com/uc?id=11XU7Qm4jlOVhCf5q5126jmS6dKFcXBtP)"]},{"cell_type":"markdown","metadata":{"id":"3OAAII7jOUCZ","colab_type":"text"},"source":["Definition of states and their respective types: \n"," - `0`: absorbing state\n"," - `1`: regular state"]},{"cell_type":"code","metadata":{"id":"H9GksexMM_wQ","colab_type":"code","colab":{}},"source":["states = { 0: 1,\n","            1: 1,\n","            2: 1,\n","            3: 1,\n","            4: 1,\n","            5: 1,\n","            6: 1,\n","            7: 1,\n","            8: 1,\n","            9: 0\n","}\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkBHP1GjOqbr","colab_type":"text"},"source":["Mapping of states to the list of actions that can be taken. Actions are considered in a clockwise order given a state in the graph. "]},{"cell_type":"code","metadata":{"id":"4NmAEoO1OL2p","colab_type":"code","colab":{}},"source":["actions = { 0: (0,1,2),\n","            1: (0,1),\n","            2: (0,1),\n","            3: (0,1),\n","            4: (0,1),\n","            5: (0,1, 2, 3),\n","            6: (0,1),\n","            7: (0,1),\n","            8: (0,1),\n","            9: (0,)\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2d_ZxeIgPObO","colab_type":"text"},"source":["Mapping of a state and an action to the resulting state and reward: `(s1, a) -> (s2, r)` where `s1` is the current state and `a` is the action taken, resulting in state `s2` and yielding reward `r`."]},{"cell_type":"code","metadata":{"id":"zYF2Lef7NIto","colab_type":"code","colab":{}},"source":["transitions = {\n","    # state 0\n","    (0,0) : (1,-5),\n","    (0,1) : (2, -2),\n","    (0,2) : (3, 1),\n","    # state 1\n","    (1,0) : (4, -2),\n","    (1,1) : (2, 1),\n","    # state 2\n","    (2,0) : (5, 3),\n","    (2,1) : (3, 2),\n","    # state 3\n","    (3,0) : (5, -1),\n","    (3,1) : (6, -5),\n","    # state 4\n","    (4,0) : (7, -2),\n","    (4,1) : (5, 5),\n","    # state 5\n","    (5,0) : (7, -2),\n","    (5,1):  (8 , -4),\n","    (5, 2): (9, -7),\n","    (5, 3): (6, -2),\n","    # state 6\n","    (6,0): (5, -3),\n","    (6,1): (9, 1),\n","    # state 7\n","    (7,0) : (8, -4),\n","    (7,1): (5, -2),\n","    # state 8\n","    (8,0) :(9, 10),\n","    (8,1): (5, -4),\n","    # state 9 (absorbing state)\n","    (9,0) :(9, 0)\n","}\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4aITGXSXQUAQ","colab_type":"text"},"source":["`MDP` defines a Markov decision process, based on the graph above."]},{"cell_type":"code","metadata":{"id":"44HdEHdZQTZR","colab_type":"code","colab":{}},"source":["MDP = {\n","    \"states\" : states,\n","    \"actions\": actions,\n","    \"transitions\" : transitions\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvydm56SOS0e","colab_type":"text"},"source":["Some policies..  \n","\n","A policy maps a state `s` to a list of probabilities `p`, in which the `i`*th* probability `p_i` is the probability that the `i`*th* action from `actions[s]` is taken in state `s`."]},{"cell_type":"code","metadata":{"id":"g6tNE1QgSrAI","colab_type":"code","colab":{}},"source":["policy1 = {\n","    0: [0,1,0],\n","    1: [0,1],\n","    2: [0,1],\n","    3: [1, 0],\n","    4: [1, 0],\n","    5: [0, 1, 0, 0],\n","    6: [0, 1],\n","    7: [1, 0],\n","    8: [1, 0]\n","}\n","\n","policy2 = {\n","    0: [0.3,0.5,0.2],\n","    1: [0, 1],\n","    2: [0, 1],\n","    3: [1, 0],\n","    4: [1, 0],\n","    5: [0, 1, 0, 0],\n","    6: [0, 1],\n","    7: [1, 0],\n","    8: [1, 0]\n","}\n","\n","\n","policy3 = {\n","    0: [0.4,0.2,0.4],\n","    1: [0.7,0.3],\n","    2: [0.4,0.6],\n","    3: [0.7,0.3],\n","    4: [0.2,0.8],\n","    5: [0.25,0.25,0.25,0.25],\n","    6: [1, 0],\n","    7: [0.5,0.5],\n","    8: [0.6,0.4]\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_90NoqQ1YQ3N","colab_type":"text"},"source":["### Determine the optimal value function."]},{"cell_type":"markdown","metadata":{"id":"ta80s1gbby4Q","colab_type":"text"},"source":["$$\n","v^{\\star}(s) = \\max_{a}q^{\\star}(s,a)\n","\\\\\n","\\\\\n","$$\n","\n","$$\n","q^{\\star}(s,a) = \\sum_{s'}p(s'|s,a)\\left[r(s,a,s') + \\gamma v^{\\star}(s') \\right]\n","$$\n","\n","with\n","$$\n","\\gamma = 1 \n","$$\n","and\n","$$\n","p(s'|s,a) = 1\n","$$\n","it results in \n","$$\n","q^{\\star}(s,a) = r(s,a,s') + \\gamma v^{\\star}(s')\n","$$\n"]},{"cell_type":"code","metadata":{"id":"POH_7PSEYSCL","colab_type":"code","outputId":"d4d32849-1ca2-4d5b-ae65-6e3f9decad9d","executionInfo":{"status":"ok","timestamp":1579902440829,"user_tz":-60,"elapsed":714,"user":{"displayName":"katrien van meulder","photoUrl":"","userId":"07051660409659315107"}},"colab":{"base_uri":"https://localhost:8080/","height":646}},"source":["v_star = np.zeros(10)\n","number_states = 10\n","\n","for iteration in np.arange(15):\n","    q_star = np.zeros((10, 4))\n","    for s in np.arange(number_states):\n","        for a in MDP['actions'][s]:\n","            r= MDP['transitions'][(s, a)][1]\n","            nextState = MDP['transitions'][(s, a)][0]\n","            q_star[s, a] =  r + v_star[nextState]\n","    v_star = np.max(q_star, axis=1)\n","    print(v_star)\n","\n","\n","for i in range(number_states):\n","  print(f\"State {i}: {v_star[i]}\")\n","print(\"============== Q table ==============\" )\n","print(\"   a_0 a_1 a_2 a_3\")\n","print(q_star)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[ 1.  1.  3.  0.  5. -2.  1.  0. 10.  0.]\n","[ 1.  4.  2.  0.  3.  6.  1.  6. 10.  0.]\n","[ 1.  3.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","[ 7. 10.  9.  5. 11.  6.  3.  6. 10.  0.]\n","State 0: 7.0\n","State 1: 10.0\n","State 2: 9.0\n","State 3: 5.0\n","State 4: 11.0\n","State 5: 6.0\n","State 6: 3.0\n","State 7: 6.0\n","State 8: 10.0\n","State 9: 0.0\n","============== Q table ==============\n","   a_0 a_1 a_2 a_3\n","[[ 5.  7.  6.  0.]\n"," [ 9. 10.  0.  0.]\n"," [ 9.  7.  0.  0.]\n"," [ 5. -2.  0.  0.]\n"," [ 4. 11.  0.  0.]\n"," [ 4.  6. -7.  1.]\n"," [ 3.  1.  0.  0.]\n"," [ 6.  4.  0.  0.]\n"," [10.  2.  0.  0.]\n"," [ 0.  0.  0.  0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AQ8UZiQZqdbF","colab_type":"text"},"source":["![optValue](https://drive.google.com/uc?id=1F6lv_-faTNFPuVpn6a3ijtGm3SnE_Deh)"]},{"cell_type":"code","metadata":{"id":"OMPB96NuFXHf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}