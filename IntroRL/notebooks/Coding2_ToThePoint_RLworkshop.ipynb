{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Coding2_ToThePoint_RLworkshop.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dPFaiPILKILy","colab_type":"text"},"source":["# Determine the value function for a given policy using the Bellman equation"]},{"cell_type":"code","metadata":{"id":"F2Z8BNMEMtUA","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9RpY31zrNrbx","colab_type":"text"},"source":["### Define the graph as an MDP with states, actions & transitions\n","\n","![graph](https://drive.google.com/uc?id=11XU7Qm4jlOVhCf5q5126jmS6dKFcXBtP)"]},{"cell_type":"markdown","metadata":{"id":"3OAAII7jOUCZ","colab_type":"text"},"source":["Definition of states and their respective types: \n"," - `0`: absorbing state\n"," - `1`: regular state"]},{"cell_type":"code","metadata":{"id":"H9GksexMM_wQ","colab_type":"code","colab":{}},"source":["states = { 0: 1,\n","            1: 1,\n","            2: 1,\n","            3: 1,\n","            4: 1,\n","            5: 1,\n","            6: 1,\n","            7: 1,\n","            8: 1,\n","            9: 0\n","}\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkBHP1GjOqbr","colab_type":"text"},"source":["Mapping of states to the list of actions that can be taken. Actions are considered in a clockwise order given a state in the graph. "]},{"cell_type":"code","metadata":{"id":"4NmAEoO1OL2p","colab_type":"code","colab":{}},"source":["actions = { 0: (0,1,2),\n","            1: (0,1),\n","            2: (0,1),\n","            3: (0,1),\n","            4: (0,1),\n","            5: (0,1, 2, 3),\n","            6: (0,1),\n","            7: (0,1),\n","            8: (0,1),\n","            9: (0,)\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2d_ZxeIgPObO","colab_type":"text"},"source":["Mapping of a state and an action to the resulting state and reward: `(s1, a) -> (s2, r)` where `s1` is the current state and `a` is the action taken, resulting in state `s2` and yielding reward `r`."]},{"cell_type":"code","metadata":{"id":"zYF2Lef7NIto","colab_type":"code","colab":{}},"source":["transitions = {\n","    # state 0\n","    (0,0) : (1,-5),\n","    (0,1) : (2, -2),\n","    (0,2) : (3, 1),\n","    # state 1\n","    (1,0) : (4, -2),\n","    (1,1) : (2, 1),\n","    # state 2\n","    (2,0) : (5, 3),\n","    (2,1) : (3, 2),\n","    # state 3\n","    (3,0) : (5, -1),\n","    (3,1) : (6, -5),\n","    # state 4\n","    (4,0) : (7, -2),\n","    (4,1) : (5, 5),\n","    # state 5\n","    (5,0) : (7, -2),\n","    (5,1):  (8 , -4),\n","    (5, 2): (9, -7),\n","    (5, 3): (6, -2),\n","    # state 6\n","    (6,0): (5, -3),\n","    (6,1): (9, 1),\n","    # state 7\n","    (7,0) : (8, -4),\n","    (7,1): (5, -2),\n","    # state 8\n","    (8,0) :(9, 10),\n","    (8,1): (5, -4),\n","    # state 9 (absorbing state)\n","    (9,0) :(9, 0)\n","}\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4aITGXSXQUAQ","colab_type":"text"},"source":["`MDP` defines a Markov decision process, based on the graph above."]},{"cell_type":"code","metadata":{"id":"44HdEHdZQTZR","colab_type":"code","colab":{}},"source":["MDP = {\n","    \"states\" : states,\n","    \"actions\": actions,\n","    \"transitions\" : transitions\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvydm56SOS0e","colab_type":"text"},"source":["Some policies..  \n","\n","A policy maps a state `s` to a list of probabilities `p`, in which the `i`*th* probability `p_i` is the probability that the `i`*th* action from `actions[s]` is taken in state `s`."]},{"cell_type":"code","metadata":{"id":"g6tNE1QgSrAI","colab_type":"code","colab":{}},"source":["policy1 = {\n","    0: [0,1,0],\n","    1: [0,1],\n","    2: [0,1],\n","    3: [1, 0],\n","    4: [1, 0],\n","    5: [0, 1, 0, 0],\n","    6: [0, 1],\n","    7: [1, 0],\n","    8: [1, 0]\n","}\n","\n","policy2 = {\n","    0: [0.3,0.5,0.2],\n","    1: [0, 1],\n","    2: [0, 1],\n","    3: [1, 0],\n","    4: [1, 0],\n","    5: [0, 1, 0, 0],\n","    6: [0, 1],\n","    7: [1, 0],\n","    8: [1, 0]\n","}\n","\n","\n","policy3 = {\n","    0: [0.4,0.2,0.4],\n","    1: [0.7,0.3],\n","    2: [0.4,0.6],\n","    3: [0.7,0.3],\n","    4: [0.2,0.8],\n","    5: [0.25,0.25,0.25,0.25],\n","    6: [1, 0],\n","    7: [0.5,0.5],\n","    8: [0.6,0.4]\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EhzMD-ufJ9Tf","colab_type":"text"},"source":["### Determine the value functions using the Bellman equation for policy1 and policy2"]},{"cell_type":"markdown","metadata":{"id":"ow9eqj0bM2cM","colab_type":"text"},"source":["$$\n","v_{\\pi}(s) = \\sum_{a}\\pi(a|s)\\sum_{s'}p(s'|s,a)\\left[ r(s,a,s') + \\gamma v_\\pi(s')\\right]\n","$$\n","$$ \\gamma = 1 $$ and $$ p(s', s, a) = 1$$\n"]},{"cell_type":"markdown","metadata":{"id":"ZwRJV1XiNwxw","colab_type":"text"},"source":["$$\n","v_{\\pi}(s) = \\sum_{a}\\pi(a|s)\\left[ r(s,a,s') + v_\\pi(s')\\right]\n","$$\n"]},{"cell_type":"code","metadata":{"id":"W5Mc71E6KOzy","colab_type":"code","outputId":"82a3f4d0-b8eb-4308-89ad-4934e8c0e5ee","executionInfo":{"status":"ok","timestamp":1579887142540,"user_tz":-60,"elapsed":872,"user":{"displayName":"katrien van meulder","photoUrl":"","userId":"07051660409659315107"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["#value function v\n","number_states = 10\n","v = np.zeros(number_states)\n","\n","number_iterations = 10\n","for iteration in np.arange(number_iterations):\n","    #start of a sweep = loop over all states (except absorbing one) \n","    for s in np.arange(9):\n","        tempValue = 0\n","        for a, probabilityForAction in enumerate(policy1[s]):\n","            #reward yielded when in state s and take action a\n","            r = MDP['transitions'][(s, a)][1]\n","            #nextstate reached when in state s and take action a\n","            nextState = MDP['transitions'][(s, a)][0]\n","            tempValue +=  probabilityForAction*(r + v[nextState])\n","        #update value function for state s\n","        v[s] = tempValue\n","    print(v)\n","\n","for i in range(number_states):\n","  print(f\"State {i}: {v[i]}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[-2.  1.  2. -1. -2. -4.  1. -4. 10.  0.]\n","[ 0.  3.  1. -5. -6.  6.  1.  6. 10.  0.]\n","[-1.  2. -3.  5.  4.  6.  1.  6. 10.  0.]\n","[-5. -2.  7.  5.  4.  6.  1.  6. 10.  0.]\n","[ 5.  8.  7.  5.  4.  6.  1.  6. 10.  0.]\n","[ 5.  8.  7.  5.  4.  6.  1.  6. 10.  0.]\n","[ 5.  8.  7.  5.  4.  6.  1.  6. 10.  0.]\n","[ 5.  8.  7.  5.  4.  6.  1.  6. 10.  0.]\n","[ 5.  8.  7.  5.  4.  6.  1.  6. 10.  0.]\n","[ 5.  8.  7.  5.  4.  6.  1.  6. 10.  0.]\n","State 0: 5.0\n","State 1: 8.0\n","State 2: 7.0\n","State 3: 5.0\n","State 4: 4.0\n","State 5: 6.0\n","State 6: 1.0\n","State 7: 6.0\n","State 8: 10.0\n","State 9: 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6JFBBc0_KOwY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}