{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Coding4_ToThePoint_RLworkshop.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cdQHhikmozqw","colab_type":"text"},"source":["## SETUP\n","*Read the introduction below during the setup of your environment.*"]},{"cell_type":"code","metadata":{"id":"aBWJn7knzuNs","colab_type":"code","cellView":"form","colab":{}},"source":["#@title ##### Imports and downloads\n","# Download and unzip ngrok\n","!apt install mosquitto\n","!pip install paho-mqtt\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -o ngrok-stable-linux-amd64.zip\n","\n","# Download front end & framework\n","! git clone https://gitlab.com/tothepoint/reinforcement-learning-workshop/rl-frontend.git reinforcement-learning\n","! git clone https://gitlab.com/tothepoint/reinforcement-learning-workshop/rl-gridworld.git boarld_root\n","\n","# Run MQTT broker\n","! mosquitto -d -c boarld_root/configs/mosquitto.conf\n","\n","import subprocess\n","import time\n","import os\n","import paho.mqtt.client as mqtt\n","from threading import Thread\n","import time, urllib\n","import json\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lIdfjrZRaHre","colab_type":"code","cellView":"form","colab":{}},"source":["# @title ##### Run ngrok tunnels\n","% cd /content/\n","!pkill ngrok\n","import subprocess\n","subprocess.Popen(['./ngrok', 'start', '-config', 'boarld_root/configs/ngrok.conf', 'frontend'])\n","subprocess.Popen(['./ngrok', 'start', '-config', 'boarld_root/configs/ngrok.conf', 'mqttbroker'])\n","time.sleep(5)\n","\n","# Get ngrok URLs\n","ngrok_data_frontend = json.load(urllib.request.urlopen('http://localhost:4040/api/tunnels'))\n","ngrok_data_mqttbroker = json.load(urllib.request.urlopen('http://localhost:4041/api/tunnels'))\n","frontend_ngrok_url = ngrok_data_frontend['tunnels'][0]['public_url'].split('//')[1]\n","mqtt_ngrok_url = ngrok_data_mqttbroker['tunnels'][0]['public_url'].split('//')[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sSs91iBLXqR-","colab_type":"code","cellView":"form","colab":{}},"source":["# @title ##### Tell front end where to find MQTT broker (through ngrok tunnel)\n","with open(\"/content/reinforcement-learning/assets/tunnel-domain.txt\", \"w\") as f:\n","  f.write(mqtt_ngrok_url)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSgQED8saEfr","colab_type":"code","cellView":"form","colab":{}},"source":["# @title ##### Start front end\n","%cd /content/reinforcement-learning/\n","!npm install \n","\n","thr = Thread(target=os.system, args=('npm run dev', ))\n","thr.start()\n","time.sleep(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RCRhXZSXY_iz","colab_type":"code","cellView":"form","colab":{}},"source":["# @title ##### Install RL framework\n","%cd /content/boarld_root/boarld\n","!pip install .\n","%cd /content/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTUvIDx58qW7","colab_type":"code","cellView":"form","colab":{}},"source":["# @title ##### Import useful elements of the RL framework\n","from boarld.core.rl.arlgorithm.ARLgorithm import ARLgorithm\n","from boarld.core.rl.arlgorithm.QlearningARLgorithm import QlearningARLgorithm\n","from boarld.core.rl.arlgorithm.SarsaARLgorithm import SarsaARLgorithm\n","from boarld.core.rl.arlgorithm.BellmanARLgorithm import BellmanARLgorithm\n","from boarld.core.env.action.Action import *\n","from boarld.core.rl.runner.Runner import Runner\n","from boarld.core.rl.trainer.Trainer import Trainer\n","from boarld.gridworld.env.predefined.predefined_grids import *\n","from boarld.sliding_puzzle.env.board.SlidingPuzzle import SlidingPuzzle\n","from boarld.gridworld.rl.agent.GridAgent import GridAgent\n","from boarld.sliding_puzzle.rl.agent.PuzzleAgent import PuzzleAgent\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SsIgdhm_IIKi","colab_type":"text"},"source":["## Introduction\n","The goal of the exercises below is to implement some reinforcement learning algorithms on an abstract level. We defined an `Agent` interacting with a `Board`. It has a state and can change this state by performing an `Action`. The `Agent` contains a `Qtable` determining the best action in each state to solve the problem at hand. \n","\n","What follows is a brief overview of the objects and methods you'll probably need to implement the learning process. Once done, you can test your algorithm in a gridworld environment, as well as on a sliding puzzle. For a more detailed description of the API, please refer to the documentation.\n","\n","  \n","\n","### `ARLgorithm`\n","`ARLgorithm` is an abstraction of a reinforcement learning algorithm. An `ARLgorithm` contains an `Agent`, which learns how to become good at a certain task. The `ARLgorithm`'s `learn()` method impacts the `Qtable` of the `Agent`, causing a change in the the `Agent`'s behavior.  \n","Important methods and attributes:\n","```\n","- agent: Agent\n","- learn(self, nb_episodes, nb_of_eps_before_table_update, qtable_convergence_threshold, nb_steps_before_timeout, random_rate=0.3, learning_rate=0.2, discount_factor=0.7)\n","```\n","\n","### `Agent`\n","`Agent` represents a reinforcement learning agent. The agent is associated with a board on which it operates, a current state and one or more target states, receives a reward for performing a move or reaching a target state.  \n","Important methods and attributes:\n","```\n","- Qtable: Qtable\n","- choose_action_epsilon_greedily(random_rate, old_state)\n","- get_reward\n","- move(next_action)\n","- reset()\n","- set_agent_to_random_state(nb_actions)\n","- state_is_final(state)\n","```\n","\n","### `Qtable`\n","`Qtable` represents a Q-table in reinforcement learning: it holds a value for tuples (state, action), representing the quality of doing a certain action in a certain state. Since, during training, we want to make a distinction between the Q-table we're updating to, and the Q-table from which we derive our policy, `Qtable` consists of two tables: the regular table, from which we can obtain Q-values, and a copy on which we can do updates using `update_value(state, action, value)`. After a couple of episodes, or if the Q-table has converged (`has_converged_since_last_snapshot`, comparing it to a previously taken snapshot: `take_snapshot()`), one can replace the old table by the new one using `update_table_by_shadow()`.  \n","Important methods and attributes:\n","```\n","- has_converged_since_last_snapshot(threshold)\n","- get_Q_value(state, action, replace_neg_inf_by_zero)\n","- update_value(state, action, value)\n","- take_snapshot()\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"ei0oGh1CS8Z5","colab_type":"text"},"source":["## Exercises"]},{"cell_type":"code","metadata":{"id":"XeIVERxDjVfc","colab_type":"code","cellView":"form","colab":{}},"source":["# @title ##### Get the URL to our agent's visualization\n","print('Front end at http://%s' % frontend_ngrok_url)\n","# print('Mqtt broker at %s' % mqtt_ngrok_url)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2_sit1A6Hdfi","colab_type":"text"},"source":["### Defining my own, very stupid algorithm.."]},{"cell_type":"markdown","metadata":{"id":"liw49gPPA8bF","colab_type":"text"},"source":["Let's define an algorithm that makes our agent move right in every state.  \n","\n","Recall: a Q-table looks something like:  \n","\n","|        \t| UP  \t| DOWN \t| LEFT \t| RIGHT \t|\n","|--------\t|-----\t|------\t|------\t|-------\t|\n","| (0, 0) \t| 1   \t| 1    \t| 1    \t| 10    \t|\n","| (1, 0) \t| 10  \t| 1    \t| 1    \t| 0     \t|\n","| ...    \t| ... \t| ...  \t| ...  \t| ...   \t|"]},{"cell_type":"code","metadata":{"id":"UjIGTxlUpkOA","colab_type":"code","colab":{}},"source":["class StupidARLgorithm(ARLgorithm):\n","  def learn(self, nb_episodes, nb_of_eps_before_table_update, qtable_convergence_threshold,\n","          nb_steps_before_timeout, random_rate=0.3, learning_rate=0.2, discount_factor=0.7):\n","\n","    for i in range(nb_episodes):        \n","      for state in self.agent.get_list_of_possible_states():\n","        self.agent.Qtable.update_value(state, Right(), 100)\n","        self.agent.Qtable.update_value(state, Left(), 1)\n","        self.agent.Qtable.update_value(state, Up(), 1)\n","        self.agent.Qtable.update_value(state, Down(), 1)\n","      self.agent.Qtable.update_table_by_shadow()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-3pIyB8ojuU","colab_type":"text"},"source":["Train the agent and let it try to get to its goal. "]},{"cell_type":"code","metadata":{"id":"99D2SudXBjhV","colab_type":"code","colab":{}},"source":["# -- AGENT SETUP -- #\n","stupid_agent = GridAgent(Grid1.GRID)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSv1x2eeBdhE","colab_type":"code","colab":{}},"source":["# -- TRAINER SETUP -- #\n","stupid_trainer = Trainer() \\\n","    .with_agent(stupid_agent) \\\n","    .with_arlgorithm(StupidARLgorithm) \\\n","    .with_nb_episodes(1) \\\n","    .with_nb_of_eps_before_table_update(1) \\\n","    .with_qtable_convergence_threshold(.001) \\\n","    .with_nb_steps_before_timeout(1) \\\n","    .with_random_rate(0.3) \\\n","    .with_learning_rate(0.2) \\\n","    .with_discount_factor(1) \\\n","\n","# -- TRAIN -- #\n","stupid_trainer.train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_Fa5JD6Bo2X","colab_type":"code","colab":{}},"source":["# -- RUN -- #\n","stupid_agent.reset()\n","stupid_runner = Runner() \\\n","    .with_agent(stupid_agent) \n","stupid_runner.run()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1waDC3sCoLz","colab_type":"text"},"source":["### Running a real RL algorithm"]},{"cell_type":"code","metadata":{"id":"hoh3mBND7GQl","colab_type":"code","colab":{}},"source":["# -- AGENT SETUP -- #\n","grid_agent = GridAgent(Grid1.GRID)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4_Db1TKOUthF","colab_type":"code","colab":{}},"source":["# -- TRAINER SETUP -- #\n","gridworld_trainer = Trainer() \\\n","    .with_agent(grid_agent) \\\n","    .with_arlgorithm(QlearningARLgorithm) \\\n","    .with_nb_episodes(5) \\\n","    .with_nb_of_eps_before_table_update(50) \\\n","    .with_qtable_convergence_threshold(.001) \\\n","    .with_nb_steps_before_timeout(25) \\\n","    .with_random_rate(0.3) \\\n","    .with_learning_rate(0.2) \\\n","    .with_discount_factor(1) \\\n","    \n","### --- DANGER ZONE --- ###\n","# !!! ONLY UNCOMMENT THE LINES BELOW IF NB_EPISODES < 20 !!!\n","# If not, your agent will take forever to train. You'll have to factory reset your runtime and run the entire setup again. \n","\n","### --- --- --- ###\n","# gridworld_trainer \\\n","#     .with_observe_agent(True) \\\n","#     .with_observe_qtable(True) \\\n","### --- --- --- ###\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sL3I_ScPT5u7","colab_type":"code","colab":{}},"source":["# -- TRAIN -- #\n","gridworld_trainer.train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCnJnEhEnT7z","colab_type":"code","colab":{}},"source":["# -- RUN -- #\n","grid_agent.reset()\n","gridworld_runner = Runner() \\\n","    .with_agent(grid_agent)\n","\n","gridworld_runner.run()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l_kEjM6vDTBt","colab_type":"text"},"source":["### Your turn: implement Q-learning\n","\n","![Pseudocode Q-learning](https://www.cse.unsw.edu.au/~cs9417ml/RL1/images/qalg.gif)  \n","[(Image credit)](https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html)"]},{"cell_type":"code","metadata":{"id":"TJBWpbDWDKPX","colab_type":"code","colab":{}},"source":["class MyQlearning(ARLgorithm):\n","  def learn(self, nb_episodes, nb_of_eps_before_table_update, qtable_convergence_threshold,\n","          nb_steps_before_timeout, random_rate=0.3, learning_rate=0.2, discount_factor=0.7):\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8lmRG7QRDnih","colab":{}},"source":["# -- AGENT SETUP -- #\n","my_qlearning_agent = GridAgent(Grid1.GRID)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8833SNVkDnim","colab":{}},"source":["# -- TRAINER SETUP -- #\n","my_qlearning_trainer = Trainer() \\\n","    .with_agent(my_qlearning_agent) \\\n","    .with_arlgorithm(MyQlearning) \\\n","    .with_nb_episodes(5) \\\n","    .with_nb_of_eps_before_table_update(50) \\\n","    .with_qtable_convergence_threshold(.001) \\\n","    .with_nb_steps_before_timeout(25) \\\n","    .with_random_rate(0.3) \\\n","    .with_learning_rate(0.2) \\\n","    .with_discount_factor(1)\n","    \n","### --- DANGER ZONE --- ###\n","# !!! ONLY UNCOMMENT THE LINES BELOW IF NB_EPISODES < 20 !!!\n","# If not, your agent will take forever to train. You'll have to factory reset your runtime and run the entire setup again. \n","\n","### --- --- --- ###\n","# my_qlearning_agent \\\n","#     .with_observe_agent(True) \\\n","#     .with_observe_qtable(True) \\\n","### --- --- --- ###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5fbwHBMVJ3g","colab_type":"code","colab":{}},"source":["# -- TRAIN -- #\n","my_qlearning_trainer.train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fsrkcm3eDnin","colab":{}},"source":["# -- RUN -- #\n","my_qlearning_agent.reset()\n","my_qlearning_runner = Runner() \\\n","    .with_agent(my_qlearning_agent)\n","\n","my_qlearning_runner.run()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EJlPlcPEOZU","colab_type":"text"},"source":["### Now do the same for SARSA, and train a new agent.\n","![Pseudocode SARSA](https://www.cse.unsw.edu.au/~cs9417ml/RL1/images/salg.gif)  \n","[(Image credit)](https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html)"]},{"cell_type":"code","metadata":{"id":"vgrlKGToDfc7","colab_type":"code","colab":{}},"source":["class MySARSA(ARLgorithm):\n","  def learn(self, nb_episodes, nb_of_eps_before_table_update, qtable_convergence_threshold,\n","          nb_steps_before_timeout, random_rate=0.3, learning_rate=0.2, discount_factor=0.7):\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"v0k6dWNbKX1Z","colab":{}},"source":["# -- AGENT SETUP -- #\n","my_sarsa_agent = GridAgent(Grid1.GRID)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mkFQ4gRXKX1i","colab":{}},"source":["# -- TRAINER SETUP -- #\n","my_sarsa_trainer = Trainer() \\\n","    .with_agent(my_sarsa_agent) \\\n","    .with_arlgorithm(MySARSA) \\\n","    .with_nb_episodes(5) \\\n","    .with_nb_of_eps_before_table_update(50) \\\n","    .with_qtable_convergence_threshold(.001) \\\n","    .with_nb_steps_before_timeout(25) \\\n","    .with_random_rate(0.3) \\\n","    .with_learning_rate(0.2) \\\n","    .with_discount_factor(1)\n","    \n","### --- DANGER ZONE --- ###\n","# !!! ONLY UNCOMMENT THE LINES BELOW IF NB_EPISODES < 20 !!!\n","# If not, your agent will take forever to train. If you intterrupt the training process, you'll have to factory reset your runtime and run the entire setup again. \n","\n","### --- --- --- ###\n","# my_sarsa_agent \\\n","#     .with_observe_agent(True) \\\n","#     .with_observe_qtable(True) \\\n","### --- --- --- ###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLSQAPGGf2OU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"64fJm3sMKX1l","colab":{}},"source":["# -- RUN -- #\n","my_sarsa_agent.reset()\n","my_sarsa_runner = Runner() \\\n","    .with_agent(my_sarsa_agent)\n","\n","my_sarsa_runner.run()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGpiYKxbTE8u","colab_type":"text"},"source":["## Extra"]},{"cell_type":"markdown","metadata":{"id":"3nOmV0hLEX9Z","colab_type":"text"},"source":["### Define your own grid"]},{"cell_type":"code","metadata":{"id":"RTenIBm7Ac3F","colab_type":"code","colab":{}},"source":["# You can define your own grid, if you want!\n","class MyOwnGrid:\n","    name='my_own_grid'\n","    board_rows = 8\n","    board_columns = 8\n","    win_state = {Goal(7, 0)}\n","    lose_states = {Trap(4, 2)}\n","    start = Start(1, 4)\n","    blocked_cells = {Obstacle(1, 1), Obstacle(2, 1), Obstacle(3, 1), Obstacle(5, 5), Obstacle(5, 6), Obstacle(5, 3), Obstacle(5, 4), Obstacle(2, 6), Obstacle(3, 6), Obstacle(4, 6)}\n","    GRID = Grid(board_rows, board_columns, start, win_state, lose_states, blocked_cells, name)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6FXluX6Zo-S_","colab_type":"text"},"source":["### Sliding puzzle\n","You can use the algorithms you implemented to teach an agent how to solve a sliding puzzle. "]},{"cell_type":"code","metadata":{"id":"N-f0bYPAEzYP","colab_type":"code","colab":{}},"source":["# -- BOARD SETUP -- #\n","nb_rows = 2\n","nb_cols = 2\n","puzz = SlidingPuzzle(nb_rows, nb_cols)\n","\n","# -- AGENT SETUP -- #\n","puzzle_agent = PuzzleAgent(puzz)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7yiznVfnZmE","colab_type":"code","colab":{}},"source":["\n","# -- TRAINER SETUP -- #\n","puzzle_trainer = Trainer() \\\n","    .with_agent(puzzle_agent) \\\n","    .with_arlgorithm(MyQlearning) \\\n","    .with_nb_episodes(5) \\\n","    .with_nb_of_eps_before_table_update(50) \\\n","    .with_qtable_convergence_threshold(.001) \\\n","    .with_nb_steps_before_timeout(25) \\\n","    .with_random_rate(0.3) \\\n","    .with_learning_rate(0.2) \\\n","    .with_discount_factor(1)\n","\n","### --- DANGER ZONE --- ###\n","# !!! ONLY UNCOMMENT THE LINES BELOW IF NB_EPISODES < 20 !!!\n","# If not, your agent will take forever to train. You'll have to factory reset your runtime and run the entire setup again. \n","\n","### --- --- --- ###\n","# puzzle_trainer \\\n","#     .with_observe_agent(True) \\\n","#     .with_observe_qtable(True) \\\n","### --- --- --- ###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CbJ1O7NuVyxJ","colab_type":"code","colab":{}},"source":["# -- TRAIN -- #\n","puzzle_trainer.train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpYuNjg3pJ-r","colab_type":"code","colab":{}},"source":["# -- RUN -- #\n","puzzle_runner = Runner() \\\n","    .with_agent(puzzle_agent) \\\n","\n","print('Shuffle..')\n","puzzle_agent.set_agent_to_random_state(nb_actions=100)\n","\n","puzzle_runner.run()"],"execution_count":0,"outputs":[]}]}