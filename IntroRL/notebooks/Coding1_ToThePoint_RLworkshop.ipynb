{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Coding1_ToThePoint_RLworkshop.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gN3nLpvaJgqT","colab_type":"text"},"source":["# Determine the value function for a given policy using Monte Carlo"]},{"cell_type":"code","metadata":{"id":"F2Z8BNMEMtUA","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXcc3B04C_mI","colab_type":"text"},"source":["### Define the graph as an MDP with states, actions & transitions\n","![graph](https://drive.google.com/uc?id=11XU7Qm4jlOVhCf5q5126jmS6dKFcXBtP)"]},{"cell_type":"markdown","metadata":{"id":"3OAAII7jOUCZ","colab_type":"text"},"source":["Definition of states and their respective types: \n"," - `0`: absorbing state\n"," - `1`: regular state"]},{"cell_type":"code","metadata":{"id":"H9GksexMM_wQ","colab_type":"code","colab":{}},"source":["states = { 0: 1,\n","            1: 1,\n","            2: 1,\n","            3: 1,\n","            4: 1,\n","            5: 1,\n","            6: 1,\n","            7: 1,\n","            8: 1,\n","            9: 0\n","}\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkBHP1GjOqbr","colab_type":"text"},"source":["Mapping of states to the list of actions that can be taken. Actions are considered in a clockwise order given a state in the graph. "]},{"cell_type":"code","metadata":{"id":"4NmAEoO1OL2p","colab_type":"code","colab":{}},"source":["actions = { 0: (0,1,2),\n","            1: (0,1),\n","            2: (0,1),\n","            3: (0,1),\n","            4: (0,1),\n","            5: (0,1, 2, 3),\n","            6: (0,1),\n","            7: (0,1),\n","            8: (0,1),\n","            9: (0,)\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2d_ZxeIgPObO","colab_type":"text"},"source":["Mapping of a state and an action to the resulting state and reward: `(s1, a) -> (s2, r)` where `s1` is the current state and `a` is the action taken, resulting in state `s2` and yielding reward `r`."]},{"cell_type":"code","metadata":{"id":"zYF2Lef7NIto","colab_type":"code","colab":{}},"source":["transitions = {\n","    # state 0\n","    (0,0) : (1,-5),\n","    (0,1) : (2, -2),\n","    (0,2) : (3, 1),\n","    # state 1\n","    (1,0) : (4, -2),\n","    (1,1) : (2, 1),\n","    # state 2\n","    (2,0) : (5, 3),\n","    (2,1) : (3, 2),\n","    # state 3\n","    (3,0) : (5, -1),\n","    (3,1) : (6, -5),\n","    # state 4\n","    (4,0) : (7, -2),\n","    (4,1) : (5, 5),\n","    # state 5\n","    (5,0) : (7, -2),\n","    (5,1):  (8 , -4),\n","    (5, 2): (9, -7),\n","    (5, 3): (6, -2),\n","    # state 6\n","    (6,0): (5, -3),\n","    (6,1): (9, 1),\n","    # state 7\n","    (7,0) : (8, -4),\n","    (7,1): (5, -2),\n","    # state 8\n","    (8,0) :(9, 10),\n","    (8,1): (5, -4),\n","    # state 9 (absorbing state)\n","    (9,0) :(9, 0)\n","}\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4aITGXSXQUAQ","colab_type":"text"},"source":["`MDP` defines a Markov decision process, based on the graph above."]},{"cell_type":"code","metadata":{"id":"44HdEHdZQTZR","colab_type":"code","colab":{}},"source":["MDP = {\n","    \"states\" : states,\n","    \"actions\": actions,\n","    \"transitions\" : transitions\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvydm56SOS0e","colab_type":"text"},"source":["Some policies..  \n","\n","A policy maps a state `s` to a list of probabilities `p`, in which the `i`*th* probability `p_i` is the probability that the `i`*th* action from `actions[s]` is taken in state `s`."]},{"cell_type":"code","metadata":{"id":"g6tNE1QgSrAI","colab_type":"code","colab":{}},"source":["policy1 = {\n","    0: [0,1,0],\n","    1: [0,1],\n","    2: [0,1],\n","    3: [1, 0],\n","    4: [1, 0],\n","    5: [0, 1, 0, 0],\n","    6: [0, 1],\n","    7: [1, 0],\n","    8: [1, 0]\n","}\n","\n","policy2 = {\n","    0: [0.3,0.5,0.2],\n","    1: [0, 1],\n","    2: [0, 1],\n","    3: [1, 0],\n","    4: [1, 0],\n","    5: [0, 1, 0, 0],\n","    6: [0, 1],\n","    7: [1, 0],\n","    8: [1, 0]\n","}\n","\n","policy3 = {\n","    0: [0.4,0.2,0.4],\n","    1: [0.7,0.3],\n","    2: [0.4,0.6],\n","    3: [0.7,0.3],\n","    4: [0.2,0.8],\n","    5: [0.25,0.25,0.25,0.25],\n","    6: [1, 0],\n","    7: [0.5,0.5],\n","    8: [0.6,0.4]\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6jUr0gI4CIid","colab_type":"text"},"source":["### Determine the value function for deterministic policy `policy1`."]},{"cell_type":"code","metadata":{"id":"OqU8I0j6CNrK","colab_type":"code","colab":{}},"source":["number_states = 10\n","#value function v\n","v = np.zeros(10)\n","#number of rollouts done per state\n","numberRolloutsPerState = np.zeros(number_states)\n","#total number of sweeps\n","numberOfSweeps = 50\n","\n","\n","#start Monte Carlo\n","for sweep in np.arange(numberOfSweeps):\n","    #start of a sweep = loop over all states\n","    for s in np.arange(number_states):\n","        #set cumulutive reward to zero\n","        G = 0\n","        #status of the state, to check if it's absorbing or not\n","        statusOfCurrentState = MDP['states'][s]\n","        currentState = s\n","        while statusOfCurrentState != 0:\n","            #pick an action acoording to its probability\n","            a = np.argwhere(np.random.multinomial(1, policy1[currentState]) == 1)[0][0]\n","            #reward\n","            r = MDP['transitions'][(currentState, a)][1]\n","            #cumulutive reward\n","            G += r\n","            currentState = MDP['transitions'][(currentState, a)][0]\n","            statusOfCurrentState = MDP['states'][currentState]\n","\n","        #update value function in state s\n","        v[s] = (v[s] * numberRolloutsPerState[s]  + G )/(numberRolloutsPerState[s]  +1)\n","        numberRolloutsPerState[s] += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7Nr30gjQNAz","colab_type":"code","outputId":"9f5aac0f-7a74-402b-91f1-eb20beb69494","executionInfo":{"status":"ok","timestamp":1579931458246,"user_tz":-60,"elapsed":617,"user":{"displayName":"katrien van meulder","photoUrl":"","userId":"07051660409659315107"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["for i in range(number_states):\n","  print(f\"State {i}: {v[i]}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["State 0: 5.0\n","State 1: 8.0\n","State 2: 7.0\n","State 3: 5.0\n","State 4: 4.0\n","State 5: 6.0\n","State 6: 1.0\n","State 7: 6.0\n","State 8: 10.0\n","State 9: 0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JpqqZA7dDeZ0","colab_type":"text"},"source":["![policy1](https://drive.google.com/uc?id=1UHIQdZI_aBsncB3KMhcrsbKJBPsCqdZG)"]},{"cell_type":"markdown","metadata":{"id":"NXNoBvAI-FEQ","colab_type":"text"},"source":["### Determine the value function for deterministic policy `policy2`."]},{"cell_type":"code","metadata":{"id":"DXC8ZewZ-Gyz","colab_type":"code","colab":{}},"source":["number_states = 10\n","#value function v\n","v = np.zeros(number_states)\n","#number of rollouts done per state\n","numberRolloutsPerState = np.zeros(number_states)\n","#total number of sweeps\n","numberOfSweeps = 1000\n","\n","\n","#start Monte Carlo\n","for sweep in np.arange(numberOfSweeps):\n","    #start of a sweep = loop over all states (except absorbing one) \n","    for s in np.arange(number_states):\n","        #set cumulutive reward to zero\n","        G = 0\n","        #status of the state, to check if it's absorbing or not\n","        statusOfCurrentState = MDP['states'][s]\n","        currentState = s\n","        while statusOfCurrentState != 0:\n","            #pick an action acoording to its probability\n","            a = np.argwhere(np.random.multinomial(1, policy2[currentState]) == 1)[0][0]\n","            #reward\n","            r = MDP['transitions'][(currentState, a)][1]\n","            #cumulutive reward\n","            G += r\n","            currentState = MDP['transitions'][(currentState, a)][0]\n","            statusOfCurrentState = MDP['states'][currentState]\n","\n","        #update value function in state s\n","        v[s] = (v[s] * numberRolloutsPerState[s]  + G )/(numberRolloutsPerState[s]  +1)\n","        numberRolloutsPerState[s] += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhtggZMMdCNU","colab_type":"code","outputId":"35b193f3-878d-47bf-f0a7-e4bb2498c964","executionInfo":{"status":"ok","timestamp":1579931502885,"user_tz":-60,"elapsed":592,"user":{"displayName":"katrien van meulder","photoUrl":"","userId":"07051660409659315107"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["for i in range(number_states):\n","  print(f\"State {i}: {v[i]}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["State 0: 4.555000000000003\n","State 1: 8.0\n","State 2: 7.0\n","State 3: 5.0\n","State 4: 4.0\n","State 5: 6.0\n","State 6: 1.0\n","State 7: 6.0\n","State 8: 10.0\n","State 9: 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SFYUziDd-TZV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}