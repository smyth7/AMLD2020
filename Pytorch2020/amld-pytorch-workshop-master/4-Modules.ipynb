{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://discuss.pytorch.org/uploads/default/original/2X/3/35226d9fbc661ced1c5d17e374638389178c3176.png\" width=\"400\" style=\"margin: 50px auto; display: block; position: relative; left: -30px;\" />\n",
    "</div>\n",
    "\n",
    "<!--NAVIGATION-->\n",
    "# < [Optimization](3-Optimization.ipynb) | Modules | [CNN](5-CNN.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Modules\n",
    "\n",
    "Modules are a way to build re-usable model components and to manage model parameters.\n",
    "PyTorch has many built-in modules for common operations like convolutions, recurrent neural networks, max pooling, common activation functions, etc.\n",
    "You can also build your own modules.\n",
    "\n",
    "This notenook introduces modules, and you will build a small neural network to perform image classification on MNIST.\n",
    "\n",
    "### Table of Contents\n",
    "#### 1. [Modules](#Modules)\n",
    "#### 2. [Building and Training a Neural Network](#Building-and-Training-a-Neural-Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules manage parameters\n",
    "\n",
    "They help to\n",
    "- keep track of the parameters in your model.\n",
    "- save/load of your model.\n",
    "- reset gradients (with `model.zero_grad()`)\n",
    "- move all parameters to the gpu (with `model.cuda()`)\n",
    "\n",
    "The model parameter's are represented by `torch.nn.Parameter` objects.\n",
    "A `Parameter` is a tensor with `requires_grad` set to `True` by default, and which is automatically added to the list of parameters when used within a model. If you are interested, you can have a look at the [torch.nn.Parameter documentation](https://pytorch.org/docs/stable/_modules/torch/nn/parameter.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, a `Conv1d` module has two parameters: `weight` as `bias`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = torch.nn.Conv1d(5, 2, 3)\n",
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Basic usage\n",
    "\n",
    "Each instance of a model has its own __parameters__.\n",
    "The parameters are initialized randomly when the model is instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_model = torch.nn.Linear(5, 2)  # 5 input dimensions, 2 output dimensions\n",
    "\n",
    "linear_regression_model.weight  # running this cell more than once, you'll see different outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models operate on __batches__ of data. If a model is designed to operate on datapoints with 5 features, the shape of the model's inputs will be `(batch, 5)`. This allows us to process multiple datapoints in parallel and increase efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "x = torch.randn(batch_size, 5)\n",
    "y = torch.randn(batch_size, 2)\n",
    "\n",
    "print(\"x = {}\\ny = {}\".format(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can __call__ the model on an input (forward pass). This evaluation uses the current model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = linear_regression_model(x)\n",
    "print(\"predicted y = {}\".format(predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the model, you compute a __measure of error__ (loss) on the predictions. In this case, we use a squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a loss\n",
    "loss = torch.sum((y - predicted_y)**2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the loss computed, we compute __gradients__ of the loss with respect to all model parameters using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_model.zero_grad()  # clear previous gradient\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradients:\")\n",
    "print(linear_regression_model.weight.grad)\n",
    "print(linear_regression_model.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use these gradients to optimize the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Composing modules with `torch.nn.Sequential`\n",
    "\n",
    "If the model you want to build is a simple chain of other modules, you can compose them using `torch.nn.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 2),\n",
    ")\n",
    "\n",
    "# Run the model:\n",
    "neural_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module parameters\n",
    "\n",
    "You can inspect your network's parameters using `.parameters()` or `.named_parameters()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param, tensor in neural_net.named_parameters():\n",
    "    print(\"{:10s} shape = {}\".format(param, tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Custom modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A module has to implement two functions:\n",
    "\n",
    "- the `__init__` function, where you define all the sub-components that have learnable parameters. This makes sure that your module becomes aware all its parameters. The sub-components (layers) do not need to be defined in order of execution or connceted together. Don't forget to initialize the parent class `torch.nn.Module` with `super().__init__()`.\n",
    "\n",
    "\n",
    "- the `forward` function, which is the method that defines what has to be executed during the forward pass and especially how the layers are connected. This is where you call the layers that you defined inside `__init__`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most basic form of a custom module:\n",
    "class MySuperSimpleModule(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define sub-modules or parameters\n",
    "        # torch.nn.Module takes care of adding their parameters to your new module\n",
    "        self.linear = torch.nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `print` function to list a model's submodules and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MySuperSimpleModule(input_size=20, num_classes=5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `model.parameters()` to get the list of parameters of your model automatically inferred by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():  # Here we use a sligtly different version of the parameters() function\n",
    "    print(name, \":\\n\", p)                 # which also returns the parameter name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Building and Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to implement a neural network now. In this section, you will learn to classify handwritten digits from the widely known MNIST dataset.\n",
    "The dataset consists of 60,000 training images of size 28x28, and another 10,000 images for evaluating the quality of the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST](figures/mnist.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "MNIST is widely used and a dataset and it is available in the `torchvision` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset in PyTorch behalves _like an array_. It has a length, and you can access its entries with `dataset[i]`.\n",
    "\n",
    "__Exercise__<br>\n",
    "Verify how many images there are in the training dataset. How is one training example represented? What is the type and shape of an entry from the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a model, we make multiple passes through all the examples in the training set. Each pass, the data points are shuffled and batched together. For this purpose, we use a `DataLoader`. The `DataLoader` support multi-threading to optize your data-loading pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Loader (Input Batcher)\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate through the dataset in shuffled batches. Everytime you do this, the order will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    assert len(images) == batch_size\n",
    "    assert len(labels) == batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__<br>\n",
    "Search in the documentation how to enable multi-threading in the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fully-connected neural network consists of layers that contain a number of values (neurons) computed as linear combinations of the neurons in the layer before. The first layer contains the network's input (your features), and the last layer contains the prediction. In our case, the last layer contains 10 neurons that are trained to be large when an input image is of the corresponding digit (0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
    "\n",
    "The parameters of this model that are optimized (trained), are the weights that connect the neurons. These are drawn as edges in the illustration below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/ledell/sldm4-h2o/master/mlp_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that neural networks can approximate non-linear functions, each neuron's value is transformed with some non-linear transformation function $\\sigma(\\cdot)$, often called an ‘activation function’ before being fed as input to the next layer. \n",
    "\n",
    "To be precise, the neurons $\\vec x_{i+1}$ in layer $i+1$ are computed from the neurons $\\vec x_i$ in layer $i$ as \n",
    "\n",
    "$$ \\vec x_{i+1} = \\sigma\\left(W_{i+1} \\vec x_i + \\vec b_{i+1} \\right) $$\n",
    "\n",
    "where $W_{i+1}$ encodes the network parameters between each pair of input/output neurons in layer $i+1$, and $\\vec b_{i+1}$ contains 'bias terms'. $\\sigma$ operates element-wise.\n",
    "\n",
    "A layer like that can be implemented using `torch.nn.Linear` followed by an activation function such as `torch.nn.ReLU` or `torch.nn.Sigmoid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Exercise__<br>\n",
    "Implement a multi-layer fully-connected neural network with two hidden layers and the following numbers of neurons in each layer:\n",
    "\n",
    "- Input-size: *input_size*\n",
    "- 1st hidden layer: 75\n",
    "- 2nd hidden layer: 50\n",
    "- Output layer: *num_classes*\n",
    "\n",
    "Use `ReLU`s as ‘activation functions’ in between each pair of layers, but not after the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # provides some helper functions like Relu's, Sigmoids, Tanh, etc.\n",
    "\n",
    "class MyNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(input_size, 75)\n",
    "        self.linear_2 = # <YOUR CODE>\n",
    "        self.linear_3 = torch.nn.Linear(50, num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.linear_1(x))\n",
    "        out = # <YOUR CODE>\n",
    "        out = # <YOUR CODE>\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now feed an input to your network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(16, 28 * 28)  # the first dimension is reserved for the 'batch_size'\n",
    "model = MyNeuralNetwork(input_size=28 * 28, num_classes=10)\n",
    "out = model(x)  # this calls model.forward(x)\n",
    "out[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ <br>\n",
    "What does `out[0, :]` above represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training the model\n",
    "\n",
    "Most of the functions to train a model follow a similar pattern in PyTorch.\n",
    "In most of the cases in consists of the following steps:\n",
    "- Loop over data (in batches)\n",
    "- Create a prediction (forward pass)\n",
    "- Clear previous gradients (!)\n",
    "- Compute gradients (backward pass)\n",
    "- Parameter update (using an optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Copy all model parameters to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the Loss function and Optimizer that you want to use\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # NOTE: model.parameters()\n",
    "\n",
    "# Passes over the whole dataset\n",
    "for epoch in range(5):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Loop over batches in the training set\n",
    "    for (inputs, labels) in train_loader:\n",
    "        \n",
    "        # Move inputs from CPU memory to GPU memory\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # The pixels in our images have a square 28x28 structure, but the network\n",
    "        # accepts a *vector* of inputs. We therefore reshape it.\n",
    "        # -1 is a special number that indicates 'whatever is left'\n",
    "        inputs = inputs.view(-1, 28*28)\n",
    "\n",
    "        # Do a forward pass, loss computation, compute gradient, and optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add up training losses so we can compute an average later\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(\"Epoch %d, Loss=%.4f\" % (epoch+1, total_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- We can use the `.to` function on the model directly. Indeed, since PyTorch knows all the model parameters, it can put all the parameters on the correct device.\n",
    "- We use `model.parameters()` to get all the parameters of the model and we can instantiate an optimizer that will optimize these parameters `torch.optim.SGD(model.parameters())`.\n",
    "- To apply the forward function of the module, we write `model(input)`. In most cases, `model.forward(inputs)` would also work, but there is a slight difference : PyTorch allows you to register hook functions for a model that are automatically called when you do a forward pass on your model. Using `model(input)` will call these hooks and then call the forward function, while using `model.forward(inputs)` will just silently ignore them.\n",
    "\n",
    "Do you see the convenience of Modules?\n",
    "\n",
    "### Loss functions\n",
    "\n",
    "PyTorch comes with a lot of predefined loss functions :\n",
    "- `L1Loss`\n",
    "- `MSELoss`\n",
    "- `CrossEntropyLoss`\n",
    "- `NLLLoss`\n",
    "- `PoissonNLLLoss`\n",
    "- `KLDivLoss`\n",
    "- `BCELoss`\n",
    "- `...`\n",
    "\n",
    "Check out the [PyTorch Documentation](https://pytorch.org/docs/master/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing model performance \n",
    "This function loops over another `data_loader` (usually containing test/validation data) and computes the model's accuracy on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_loader, device):\n",
    "    with torch.no_grad(): # during model evaluation, we don't need the autograd mechanism (speeds things up)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)     \n",
    "            inputs = inputs.view(-1, 28*28)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            correct += (predicted.cpu() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    acc = correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(model, test_loader, device)  # look at: accuracy(model, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get an accuracy of around 97%. Can you improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_loaded = torch.load(\"my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.linear_3.bias)\n",
    "print(my_model_loaded.linear_3.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This intro to modules used [this medium post](https://medium.com/deeplearningbrasilia/deep-learning-introduction-to-pytorch-5bd39421c84) as a resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "# < [Optimization](3-Optimization.ipynb) | Modules | [CNN](5-CNN.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
