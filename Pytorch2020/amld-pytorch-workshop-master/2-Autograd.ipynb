{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://discuss.pytorch.org/uploads/default/original/2X/3/35226d9fbc661ced1c5d17e374638389178c3176.png\" width=\"400\" style=\"margin: 50px auto; display: block; position: relative; left: -30px;\" />\n",
    "</div>\n",
    "\n",
    "<!--NAVIGATION-->\n",
    "# < [Basics](1-Basics.ipynb) | Autograd | [Optimization](3-Optimization.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "Automatic differentation (autodiff) is a key feature of PyTorch.\n",
    "PyTorch can differentiate the outcome of any computation with respect to its inputs. You don't need to compute the gradients yourself. This allows you to express and optimize complex models without worrying about correctly differentiating the model.\n",
    "\n",
    "We will start by discussing a little bit of the math behind autodiff. We then cover PyTorch's `.backward()` method that does everything automatically for you. Finally, we have a quick look under the hood to see how PyTorch does its magic.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "#### 1. [Usage in PyTorch](#Usage-in-PyTorch)\n",
    "#### 2. [Differentiation fundamentals](#Differentiation-fundamentals)\n",
    "#### 3. [Advanced topics](#Advanced-topics)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Usage in PyTorch\n",
    "\n",
    "Let's start with a simple function: $f(x) = 3 x^2 + 4$. It is easy to see that the derivative $\\frac{d}{dx}f(x)$ equals $6 \\cdot x$. PyTorch can compute this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x with some value\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(\"x = {}\".format(x))\n",
    "\n",
    "# Execute f(x)\n",
    "y = 3 * x**2  + 4\n",
    "print(\"y = {}\".format(y))\n",
    "\n",
    "# Compute the gradient of y with respect to all variables that have 'requires_grad' turned on\n",
    "y.backward()\n",
    "\n",
    "# Checking the result\n",
    "computed_gradient = x.grad\n",
    "print(\"PyTorch computed the gradient {}\".format(computed_gradient))\n",
    "print(\"We would expect it to be {}\".format(6 * x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requires_grad` property on a Tensor tells PyTorch to track computations based on this tensor. \n",
    "After you compute a quantity `y` (forward pass), you can compute the gradient of `y` with respect to all tensors that have `requires_grad==True`. \n",
    "The gradient computation (the backward pass) is triggered with `y.backward()`. You will find the computed gradients in `tensor.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple polynomial expression is easy enough to differentiate by hand. When expressions become tensor-valued and more complex, however, computing gradients becomes tedious and error-prone. The power of PyTorch is that it can compute gradients of any tensor with respect to its 'inputs' automatically. This greatly simplifies the optimization of complex, creative ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Differentiation fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this was easy enough to do by ourselves, differentiating more complex expressions takes time and is prone to human errors. Computers are much better at doing this correctly :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule\n",
    "\n",
    "To differentiate an expression like $3x^2 + 4$ in a very methodical way, you can interpret it as a sequence of basic operations. Something like \n",
    "\n",
    "$$y = f(x) = \\text{plus4}(\\text{times3}(\\text{square}(x))).$$\n",
    "\n",
    "The gradient of $y$ with respect to $x$ can now be computed systematically using the chain rule:\n",
    "\n",
    "$$f'(x) = \\text{plus4}'(\\cdots) \\,\\cdot\\, \\text{times3}'(\\cdots) \\,\\cdot\\, \\text{square}'(x).$$\n",
    "\n",
    "### Computation graph\n",
    "\n",
    "When you indicate with `requires_grad` that you will need a gradient of some output w.r.t. some input `x`, PyTorch will track any computations that are based on `x`. This ‘history’ is called the **computation graph**. For our simple polynomial, it would look like this:\n",
    "\n",
    "<img src=\"figures/computation-graph-1.png\" alt=\"Forward pass\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-propagation\n",
    "\n",
    "After you have computed the output $y=3x^2+4$, you can call `y.backward()` to compute the gradients of $y$ with respect to all inputs that have `requires_grad`. You can see how the chain rule is naturally executed backwards:\n",
    "\n",
    "<img src=\"figures/computation-graph-2.png\" alt=\"Backward pass\" width=\"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing old gradients\n",
    "\n",
    "Let's revisit the previous basic example, but call `.backward()` multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x with some value\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = 3 * x**2  + 4\n",
    "\n",
    "y.backward()\n",
    "print(\"PyTorch computed the gradient {}\".format(x.grad))\n",
    "\n",
    "# !ERROR!\n",
    "y = 3 * x**2  + 4\n",
    "y.backward()\n",
    "print(\"PyTorch computed the gradient {}\".format(x.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the second time, the gradient computed is too large. This is because `.backward()` __accumulates__ the gradients. If you want fresh values, you need to set `x.grad` to zero before you call `.backward()`.\n",
    "\n",
    "---\n",
    "\n",
    "__Exercise:__ <br>\n",
    "Fix the second gradient computation above to output the same value twice.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping history tracking with `torch.no_grad()`\n",
    "\n",
    "After you trained a model, you just want to use it without computing gradients.\n",
    "Building a computation graph for every operation would be wasteful if you don't need it.\n",
    "Therefore, you can skip these operations by wrapping your code with the `with torch.no_grad():` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(\"x.requires_grad : \", x.requires_grad)\n",
    "\n",
    "y = (x ** 2)\n",
    "print(\"y.requires_grad : \", y.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = (x ** 2)\n",
    "    print(\"y.requires_grad : \", y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any variable created within the `no_grad` context will have `requires_grad==False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping history with `.detach()`\n",
    "\n",
    "Some tensors are computed from others, but you may want to consider them constants without computation history (leaf variables). For that, you can use the `.detach()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(1,2, requires_grad=True)\n",
    "B = A.mean()\n",
    "\n",
    "print(\"B : \", B)\n",
    "print(\"B.requires_grad :\", B.requires_grad)\n",
    "\n",
    "C = B.detach()\n",
    "print(\"\\n-- C = B.detach() -- \\n\")\n",
    "\n",
    "print(\"C : \", C)\n",
    "print(\"C.requires_grad :\", C.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise: Drawing tangent lines\n",
    "\n",
    "Gradients are not only useful for machine learning and optimization. In this exercise, you will use PyTorch's automatic differentiation to __find tangent lines__ in a 2-dimensional function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider this complicated 2-dimensional function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    radius = torch.sqrt(x**2 + y**2)\n",
    "    angle = torch.atan2(y, x)\n",
    "    return radius * (1 + 0.5 * torch.cos(3.4 * angle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot it using `matplotlib`. The x and y axes represent inputs, and the color indicates the function value of `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def contour_plot():\n",
    "    grid_x, grid_y = torch.meshgrid(torch.arange(-3, 3.1, 0.1), torch.arange(-3, 3.1, 0.1))\n",
    "    outputs = f(grid_x, grid_y)\n",
    "    \n",
    "    plt.contourf(grid_x, grid_y, outputs, 20)\n",
    "    plt.colorbar(label=\"f(x, y)\")\n",
    "    plt.xlabel(\"input x\")\n",
    "    plt.ylabel(\"input y\")\n",
    "\n",
    "contour_plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__<br>\n",
    "Your turn! Use autograd to differentiate `f` with respect to its two inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "# Compute the derivative of f at the point (x, y) with respect to x and y\n",
    "\n",
    "df_dx = # TODO\n",
    "df_dy = # TODO\n",
    "\n",
    "def plot_tangent_line(x, y, df_dx, df_dy):\n",
    "    x_points = torch.linspace(-3, 3, 100)\n",
    "    y_points = -x.grad / y.grad * (x_points - x.detach()) + y.detach()\n",
    "    plt.scatter(x.detach(), y.detach(), color='white')\n",
    "    plt.plot(x_points, y_points, color='white')\n",
    "    plt.xlim([-3, 3])\n",
    "    plt.ylim([-3, 3])\n",
    "\n",
    "\n",
    "# Now play with x and y and check if your gradient computation worked\n",
    "    \n",
    "contour_plot()\n",
    "plot_tangent_line(x, y, df_dx, df_dy)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaves vs Nodes\n",
    "\n",
    "*Advanced*\n",
    "\n",
    "PyTorch's autograd mechanism differentiates between two types of tensors:\n",
    "- __node variables__ are the result of a pytorch operation\n",
    "- __leaf variables__ are directly created by a user\n",
    "\n",
    "Later in the tutorial, we will use the `.is_leaf` property to differentiate between the two types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "B = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True) + 2  # B is the result of an operation (+)\n",
    "C = 5 * A  # C is the result of an operation (*)\n",
    "print(\"A.is_leaf :\", A.is_leaf)\n",
    "print(\"B.is_leaf :\", B.is_leaf)\n",
    "print(\"C.is_leaf :\", C.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiating w.r.t. intermediate values: `.retain_grad()`\n",
    "\n",
    "*Advanced*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing the backward pass, Autograd computes the gradient of the output with respect to every intermediate variables in the computation graph. However, by default, only gradients of variables that were **created by the user** (leaf) and **have the `requires_grad` property to True** are saved.\n",
    "\n",
    "Indeed, most of the time when training a model you only need the gradient of a loss w.r.t. to your model parameters (which are leaf variables). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "C = B.mean()\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)\n",
    "C.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "B.retain_grad()  # <----- This line let us have access to gradient wrt. B after the backward pass\n",
    "C = B.mean()\n",
    "\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)\n",
    "C.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting PyTorch's computation graph\n",
    "\n",
    "*Advanced*\n",
    "\n",
    "\n",
    "You can explore how PyTorch keeps track of history by inspecting the `tensor.grad_fn` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions[0][0])\n",
    "print(y.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value has a `grad_fn` corresponding to the operation that produced the value. \n",
    "Each operation's `grad_fn` points to its inputs through `next_functions`.\n",
    "For each input, `next_functions` contains a tuple of the input's `grad_fn` and, if the operation had multiple outputs, an index of the relevant output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our example, the final `add` operation has two inputs:\n",
    "# - The first is the output of `multiplication`.\n",
    "# - The second is a constant `4` for which we don't require a gradient.\n",
    "y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "# < [Basics](1-Basics.ipynb) | Autograd | [Optimization](3-Optimization.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
