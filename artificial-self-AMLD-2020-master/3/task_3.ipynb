{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task-3-artificial-self-AMLD2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Y3lxoUPJKm",
        "colab_type": "text"
      },
      "source": [
        "# Meet your Artificial Self - AMLD 2020 Workshop\n",
        "## Task 3\n",
        "In task 2 we used somewhat of a \"hack\" to get our model to learn conversations by simply feeding raw text. In this task we will make a few minor adjustments to our method which will potentially have a big impact on our model's performance:\n",
        "* Multi-task learning\n",
        "* Specifying token types for both speakers\n",
        "* Improve data pre-processing\n",
        "\n",
        "## Important resources\n",
        "* [Workshop Github repo](https://github.com/mar-muel/artificial-self-AMLD-2020/tree/master/3)\n",
        "* [PyTorch documentation](https://pytorch.org/docs/stable/index.html)\n",
        "* Huggingface transformers library [ [Github](https://github.com/huggingface/transformers) | [Docs](https://huggingface.co/transformers/) ]\n",
        "* [Blog post by Thomas Wolf on this approach](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313)\n",
        "\n",
        "\n",
        "## Approach\n",
        "This task is heavily influenced by [this blog post](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313) which describes building dialog models based on the PersonaChat dataset, which was the winning approach in the [ConvAI2 challenge](http://convai.io/) in 2018. \n",
        "\n",
        "The main difference of this approach is that we won't be training different personalities. This means the agent's knowledge base will only consist of the recent conversation history (and not on any personality description). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiK8KRDjpfvf",
        "colab_type": "text"
      },
      "source": [
        "# Setting things up\n",
        "The following cells will clone the repository, install all the necessary dependencies, and mount your Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRbM5HvBD3HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi | grep -q 'failed' && echo \"STOP! You are using a runtime without a GPU. Change the runtime type before going further!\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdn1noNApfEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/mar-muel/artificial-self-AMLD-2020.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XEMXqxZmVSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set working directory\n",
        "%cd /content/artificial-self-AMLD-2020/3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbBVm_OvpmzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install all dependencies for this task\n",
        "!pip install -r requirements-colab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM3epItaqS3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0B66P5IqeFH",
        "colab_type": "text"
      },
      "source": [
        "# Import training data\n",
        "\n",
        "This process is the same as in [Task 2](https://colab.research.google.com/drive/1iHcQ8_K0cfRE3v8QX6FMKAzdSSGtf5IX#scrollTo=pRYuNd85O5cl), follow the \"Import training data\" section there if you missed it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aJNMrZ7ynQF",
        "colab_type": "text"
      },
      "source": [
        "## Set data path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sevu8-__yoNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the correct path here\n",
        "data_path = \"/content/drive/My Drive/AMLD/chatistics_data/chatistics_export_2020-01-16_13-46-06.json\" #@param {type:\"string\"}\n",
        "assert os.path.isfile(data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQej_5K0saFv",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the data\n",
        "Again we are going to start from this chat format: \n",
        "\n",
        "\n",
        "| timestamp | conversationId | conversationWithName | senderName | outgoing | text | language | platform |\n",
        "| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n",
        "| 1575463019 | 693342290 | Alice | Bob | True | Hi Alice! | en | whatsapp |\n",
        "| 1575463025 | 693342290 | Alice | Alice | False | Hi Bob! How are you these days? | en | whatsapp |\n",
        "| 1575463030 | 693342290 | Alice | Bob | True | Great! Thanks | en | whatsapp |\n",
        "| 1575574212 | 693342290 | Alice | Alice | False | Hello Bob! Haven't heard from you in a while!| en | whatsapp |\n",
        "\n",
        "As you can see from this example, the last message is not at all related to the previous conversation and was sent several hours later. It likely has not much in common with the previous messages.\n",
        " \n",
        "This time we will improve the pre-processing and group the data into multiple conversations (by taking into account the timestamp). In order to do so, we will define an arbitrary cut-off of 24h for grouping the messages into conversations. Furthermore, we will only consider conversations which consist of at least 10 interactions. If you are curious you can check out the code for this in the function `get_grouped_conversation_data()` in `utils.py`.\n",
        "\n",
        "The output of this function is a JSON file with the following structure:\n",
        "```\n",
        "{\n",
        "  'Alice': [\n",
        "    [\n",
        "      {\"messages\": [\"Hi Alice!\"], \"sender\": \"Bob\", \"senderType\": \"person1\"},\n",
        "      {\"messages\": [\"Hi Bob! How are you these days?\"], \"sender\": \"Alice\", \"senderType\": \"person2\"},\n",
        "      {\"messages\": [\"Great! Thanks\"], \"sender\": \"Bob\", \"senderType\": \"person1\"}\n",
        "    ], [\n",
        "      {\"messages\": [\"Hello Bob! Haven't heard from you in a while!\"], \"sender\": \"Alice\", \"senderType\": \"person2\"},\n",
        "    ...\n",
        "    ]\n",
        "  ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ull0fdABuXhZ",
        "colab_type": "text"
      },
      "source": [
        "# Multi-task learning\n",
        "\n",
        "In recent years many papers have shown that forcing a model to learn multiple objectives at once can greatly improve performance on downstream tasks such as Question Answering, Sentiment Analysis, etc. [This blog post by Sebastian Ruder](https://ruder.io/multi-task/index.html) gives a good introduction to the topic. \n",
        "\n",
        "For our model we will use two tasks to train our transformer model:\n",
        "* **Language model objective**: Like in the previous tasks we are training the next token prediction (language modelling) task. But instead of training on all of the input text we want the model to only train on the reply of `speaker1` given a conversation history with `speaker2` (we will call this the *gold reply*). In different wording: We will project the hidden-state on the word embedding matrix to get logits and apply a cross-entropy loss on the portion of the target corresponding to the gold reply. This will give us the language modelling loss.\n",
        "* **Multiple choice classification**: Additionally to the above, we will generate examples where instead of the gold reply we will give the model a random previous reply of `speaker1`. This reply has nothing to do with the current conversation (we call these replies *distractors*). The model will be tasked to recognize whether the reply is the gold reply (predict `True`) or a distractor (predict `False`) as soon as it hits last token of the reply (i.e. the `eos` ( end-of-sentence token) token). The hidden state of the final layer of the transformer is then passed through a linear layer in order to get binary classification logits. Calculating the cross-entropy gives us our classification loss. \n",
        "\n",
        "The total loss is then calculated as a mixture between both losses ($\\text{loss} = w_{LM} * \\text{loss}_{LM} + w_{MC} * \\text{loss}_{CLF}$) with tunable hyperparameters $w_{LM}$ and $w_{MC}$. From the loss we can compute backpropagation as usual and fine-tune the transformer.\n",
        "\n",
        "## Token types\n",
        "Before we jump into the implementation of multi-task losses, let's quickly cover token types. So far our model encodes the token embeddings and adds positional encoding (for an explanation of how input is encoded by default, check out [this amazing blog post by Jay Alammar](http://jalammar.github.io/illustrated-gpt2/)). \n",
        "\n",
        "However, our model can currently not differentiate between text that belongs to either speaker 1 or speaker 2. Maybe, with some luck, our previous model has learnt to associate the proximity of the `<speaker1>` tag with the text that followed it, but there's a much cleaner way to achieve this by using token types!\n",
        "\n",
        "![Input encoding](https://github.com/mar-muel/artificial-self-AMLD-2020/blob/master/static/task_3_input_encoding.png?raw=true)\n",
        "\n",
        "As in task 2, we will extend our tokenizer with the `<speaker1>` and `<speaker2>` tags. We will use these two tokens to build a vector of `token_type_ids` which specifies which portions of the input vector belong to which speaker and pass it to the model. The model will build an input representation by adding token embeddings, positional encodings, and token types. This will then serve as a single input to the transformer.\n",
        "\n",
        "## Double head models\n",
        "As multi-task learning is now a common feature for transformer models (either for pretraining as in BERT or as ways to tackle certain problems in NLP). The `transformers` library provides us with so called Double Head models which implement a LM head as well as a multiple choice classification head (check the [docs here](https://huggingface.co/transformers/model_doc/gpt2.html#transformers.GPT2DoubleHeadsModel)).\n",
        "\n",
        " The models have the following syntax:\n",
        "```python\n",
        "from transformers import GPT2DoubleHeadsModel\n",
        "\n",
        "model = GPT2DoubleHeadsModel.from_pretrained('gpt2')\n",
        "(lm_loss), (mc_loss), *_ = model(\n",
        "  input_ids,\n",
        "  token_type_ids=token_type_ids,\n",
        "  mc_token_ids=mc_token_ids,\n",
        "  mc_labels=mc_labels,\n",
        "  lm_labels=lm_labels)\n",
        "```\n",
        "The model returns `lm_loss`, which is the language modelling loss, and `mc_loss`, which is the multiple choice loss. The model's input arguments are:\n",
        "* `input_ids`: Input vocabulary IDs of the full input sequence\n",
        "* `token_type_ids`: See explanation above.\n",
        "* `mc_token_ids`: At which token classification should be triggered (in our case at the very last input token (`eos` token))\n",
        "* `mc_labels`: Classification labels for which input is the gold reply\n",
        "* `lm_labels`: Labels for language modelling. So far we had `input_ids = lm_labels`, this time we will only train the language model on the gold reply.\n",
        "\n",
        "![Double Head model](https://github.com/mar-muel/artificial-self-AMLD-2020/blob/master/static/task_3_double_head_model.png?raw=true)\n",
        "\n",
        "As you can see, a single input consists of the gold reply as well as a distractor (therefore the input dimension of `input_ids` is `[2 x input_size]`). Token types, position and embeddings will be added and passed through the transformer. The last hidden layer (corresponding to the gold reply) is the basis for the language model (LM) head. The hidden layer representation of the last token (`eos` token) will serve as a basis for the multiple choice (MC) classification head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MIXYJIxPXq2",
        "colab_type": "text"
      },
      "source": [
        "# Generating training examples\n",
        "\n",
        "From our current structure of conversation data:\n",
        "```\n",
        "{\n",
        "  'Alice': [\n",
        "    [\n",
        "      {\"messages\": [\"Hi Alice!\"], \"sender\": \"Bob\", \"senderType\": \"person1\"},\n",
        "      {\"messages\": [\"Hi Bob! How are you these days?\"], \"sender\": \"Alice\", \"senderType\": \"person2\"},\n",
        "      {\"messages\": [\"Great! Thanks\"], \"sender\": \"Bob\", \"senderType\": \"person1\"}\n",
        "    ], [\n",
        "      {\"messages\": [\"Hello Bob! Haven't heard from you in a while!\"], \"sender\": \"Alice\", \"senderType\": \"person2\"},\n",
        "    ...\n",
        "    ]\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "the `get_input_task3()` method in `utils.py` will generate the following tensors for each training example:\n",
        "* `input_ids`, dimension: `[2 x input_size]`\n",
        "* `lm_labels`, dimension: `[2 x input_size]`\n",
        "* `token_type_ids`, dimension: `[2 x input_size]`\n",
        "* `mc_token_ids`, dimension: `[2]`\n",
        "* `mc_labels`, dimension: `[1]`\n",
        "\n",
        "(note that all dimensions will also have an additional `batch_size` dimension during training)\n",
        "\n",
        "Feel free to check out the code in `utils.py`. The code works as following:\n",
        "1. Read grouped conversation data (see explanation above)\n",
        "2. Generate distractor messages for person 1 (which consists of all replies given by `person1`)\n",
        "3. Iterate through all messages and compile conversation histories of size `2*max_history + 1` (by default `max_history=2`)\n",
        "```\n",
        "<person1> <person2> <person1> <person2> <candidate>\n",
        "```\n",
        "`<candidate>` will be either the gold reply or a distractor\n",
        "4. Tokenize and convert text to vocabulary IDs\n",
        "5. Build input tensors for both a random distractor message as well as the gold reply\n",
        "6. If full length of either sequence is above `max_input_length` discard sample, else pad to the right of all tensors until they reach `max_input_length`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDqSazIhYPlm",
        "colab_type": "text"
      },
      "source": [
        "# Train the model\n",
        "We can now FINALLY! start training the model. You will see that the training code is almost identical to task 2!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzIZtfhYPL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "from argparse import ArgumentParser\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import (\n",
        "    AdamW, \n",
        "    OpenAIGPTDoubleHeadsModel,\n",
        "    OpenAIGPTTokenizer,\n",
        "    GPT2DoubleHeadsModel,\n",
        "    GPT2Tokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from utils import get_input_task3, download_pretrained_model, set_seed\n",
        "\n",
        "# set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s [%(levelname)-5.5s] [%(name)-12.12s]: %(message)s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS6tQcCbZB9R",
        "colab_type": "text"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4OqeqFc8Elr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_name = 'run1'               # The name of the run (subdirectory in ./runs)\n",
        "model_type = 'openai-gpt'       # Initialize model from path to checkpoint or with model name (\"openai-gpt\" or \"gpt2\")\n",
        "save_every = 100                 # Save checkpoint every n updates steps.\n",
        "max_input_length = 200          # Number of tokens which will be fed into the model (reduce this number if you have memory constraints)\n",
        "weight_decay = 0                # Weight decay if we apply some.\n",
        "train_batch_size = 4            # Batch size for training\n",
        "gradient_accumulation_steps = 8 # Accumulate gradients on several steps\n",
        "lr = 6.25e-5                       # Learning rate\n",
        "adam_epsilon = 1e-8             # Epsilon for Adam optimizer.\n",
        "max_norm = 1                    # Clipping gradient norm\n",
        "n_epochs = 3                    # Number of training epochs\n",
        "device = 'cuda'                 # Device (cuda or cpu)\n",
        "warmup_steps = 0                # Linear warmup over warmup_steps.\n",
        "seed = 42                       # random seed for initializatio\n",
        "\n",
        "# New for task 3!\n",
        "num_candidates = 2              # Number of candidates for training\n",
        "max_history = 2                 # Number of previous exchanges to keep in history\n",
        "lm_coef = 1.0                   # LM loss coefficient\n",
        "mc_coef = 1.0                   # Multiple-choice loss coefficient\n",
        "use_huggingface_model = False   # Start fine-tuning from the pre-trained model by Huggingface (see explanation below)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNzUopxqaTR6",
        "colab_type": "text"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "We will use PyTorch's `TensorDataset` and use it to build a Data Loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C0UErKzaCI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_loader(tokenizer, use_cache=True):\n",
        "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
        "    # get dataset of tensors\n",
        "    data = get_input_task3(\n",
        "        data_path, \n",
        "        tokenizer, \n",
        "        max_input_length=max_input_length,\n",
        "        num_candidates=num_candidates,\n",
        "        seed=seed,\n",
        "        max_history=max_history,\n",
        "        use_cache=use_cache)\n",
        "    logger.info(\"Building training data loader\")\n",
        "    train_dataset = TensorDataset(*data)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "    logger.info(\"Train dataset input shape: (Batch size, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\n",
        "    return train_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyDE0vHFaQWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the same seed allows for some reproducibility of the experiments\n",
        "set_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZPhjyAEFwGd",
        "colab_type": "text"
      },
      "source": [
        "### Start from already trained conversational model\n",
        "*Note: This step is more of a fallback in case you have very little training data or you want to start already with a very good model and take it from there.*\n",
        "\n",
        "**If you want to use an this option set `use_huggingface_model = True` above!**\n",
        "\n",
        "This model was fine-tuned on the Personachat corpus and is a SOTA model (or at least it was back in 2018). You can play with it [here](https://convai.huggingface.co/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFvYGKYEFvFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_huggingface_model:\n",
        "  model_type = download_pretrained_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wTu8noKbhu4",
        "colab_type": "text"
      },
      "source": [
        "### Load model and tokenizer\n",
        "As discussed we will use the DoubleHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkz1qT8fauRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load tokenizer\n",
        "logger.info(\"Prepare tokenizer, pretrained model and optimizer.\")\n",
        "tokenizer_class = GPT2Tokenizer if \"gpt2\" in model_type else OpenAIGPTTokenizer # cant use Autotokenizer because checkpoint could be a Path\n",
        "tokenizer = tokenizer_class.from_pretrained(model_type)\n",
        "# Load model\n",
        "model_class = GPT2DoubleHeadsModel if \"gpt2\" in model_type else OpenAIGPTDoubleHeadsModel\n",
        "model = model_class.from_pretrained(model_type)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmLP7C9DtWA0",
        "colab_type": "text"
      },
      "source": [
        "### Add special tokens\n",
        "As in task 2 we will add `<speaker1>` and `<speaker2>` to our list of additional tokens. Additionally, we will add\n",
        "* `<bos>`: Beginning of sequence token\n",
        "* `<eos>`: End of sequence token\n",
        "* `<pad>`: Padding token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCjQddCTtznN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ('<speaker1>', '<speaker2>')}\n",
        "def add_special_tokens_(model, tokenizer):\n",
        "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
        "    orig_num_tokens = len(tokenizer.encoder)\n",
        "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
        "    if num_added_tokens > 0:\n",
        "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens)\n",
        "add_special_tokens_(model, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i6HSRNNxcJ3",
        "colab_type": "text"
      },
      "source": [
        "### Setup for training\n",
        "As before we need:\n",
        "* Our data loader (defined above)\n",
        "* An optimizer\n",
        "* A scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DDqYPQZtLQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get data loaders\n",
        "logger.info(\"Prepare datasets\")\n",
        "data_loader = get_data_loader(tokenizer, use_cache=False)\n",
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [ \n",
        "    {   \n",
        "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": weight_decay,\n",
        "    },  \n",
        "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]   \n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, eps=adam_epsilon)\n",
        "t_total = len(data_loader) // gradient_accumulation_steps * n_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1QuVQuE0Sir",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e946gx1E0R_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger.info(\"***** Running training *****\")\n",
        "global_step = 0\n",
        "epochs_trained = 0\n",
        "steps_trained_in_current_epoch = 0\n",
        "# Check if we are training from a checkpoint or from a pretrained model\n",
        "if os.path.exists(model_type) and not use_huggingface_model:\n",
        "    # set global_step to gobal_step of last saved checkpoint from model path\n",
        "    global_step = int(model_type.split(\"-\")[-1].split(\"/\")[0])\n",
        "    epochs_trained = global_step // (len(data_loader) // gradient_accumulation_steps)\n",
        "    steps_trained_in_current_epoch = global_step % (len(data_loader) // gradient_accumulation_steps)\n",
        "    logger.info(\"Continuing training from checkpoint, will skip to saved global_step\")\n",
        "    logger.info(f\"Continuing training from epoch {epochs_trained}\")\n",
        "    logger.info(f\"Continuing training from global step {global_step}\")\n",
        "    logger.info(f\"Will skip the first {steps_trained_in_current_epoch} steps in the first epoch\")\n",
        "\n",
        "# Training loop\n",
        "model.zero_grad()\n",
        "epoch_pbar = trange(epochs_trained, int(n_epochs)) # epoch progress bar\n",
        "av_loss = 0\n",
        "for current_epoch in epoch_pbar:\n",
        "    epoch_pbar.set_description(f\"Epoch [{current_epoch+1}/{n_epochs}]\") # description of epoch progress bar\n",
        "    pbar = tqdm(data_loader, position=0) # progress bar\n",
        "    for step, batch in enumerate(pbar):\n",
        "        # Skip past any already trained steps if resuming training\n",
        "        if steps_trained_in_current_epoch > 0:\n",
        "            steps_trained_in_current_epoch -= 1\n",
        "            continue\n",
        "\n",
        "        # compute loss\n",
        "        model.train()\n",
        "        batch = tuple(input_tensor.to(device) for input_tensor in batch)\n",
        "        input_ids, mc_token_ids, lm_labels, mc_labels, token_type_ids = batch\n",
        "        (lm_loss), (mc_loss), *_ = model(input_ids, token_type_ids=token_type_ids, mc_token_ids=mc_token_ids, mc_labels=mc_labels, lm_labels=lm_labels)\n",
        "        loss = (lm_loss * lm_coef + mc_loss * mc_coef) / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        tr_loss = loss.item()\n",
        "\n",
        "        # Compute a running average of the loss\n",
        "        av_loss = (step*av_loss + tr_loss)/(step + 1)\n",
        "        pbar.set_description(f\"Average loss: {av_loss:.4f}\")\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()  # Update learning rate schedule\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "            if global_step % save_every == 0 and global_step > 0:\n",
        "                checkpoint_prefix = \"checkpoint\"\n",
        "                output_dir = os.path.join('runs', run_name, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "                if not os.path.exists(output_dir):\n",
        "                    os.makedirs(output_dir)\n",
        "                logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
        "                model.save_pretrained(output_dir)\n",
        "                tokenizer.save_pretrained(output_dir)\n",
        "                logger.info(f\"Saving optimizer and scheduler states to {output_dir}\")\n",
        "                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "\n",
        "# save model\n",
        "output_dir = os.path.join('runs', run_name)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "logger.info(f\"Saving model checkpoint to {output_dir}\")\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr335H6R196t",
        "colab_type": "text"
      },
      "source": [
        "# Tweaking parameters\n",
        "\n",
        "You can change the training parameters to see how they affect the language model: adjust them below, then run the again the cell above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXtY3j191ZHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run_name = 'run1'               # The name of the run (subdirectory in ./runs)\")\n",
        "model_type = 'openai-gpt'       # Initialize model from path to checkpoint or with model name (openai-gpt/openai-gpt2)\"\n",
        "weight_decay = 0                # Weight decay if we apply some.\n",
        "train_batch_size = 4            # Batch size for training\n",
        "gradient_accumulation_steps = 8 # Accumulate gradients on several steps\n",
        "lr = 5e-5                       # Learning rate\n",
        "n_epochs = 1                    # Number of training epochs\n",
        "warmup_steps = 0                # Linear warmup over warmup_steps\n",
        "lm_coef = 1.0                   # LM loss coefficient\n",
        "mc_coef = 1.0                   # Multiple-choice loss coefficien"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liEIS5AC2Efa",
        "colab_type": "text"
      },
      "source": [
        "# Speak with the model\n",
        "This code is again largely identical to the previous code (most explanations can be found there). The main difference is that parsing the output of the model is much cleaner now: The model always spits out a single reply for `person1`. The model finishes the reply by predicting the `eos` token at which point we stop sampling next tokens.\n",
        "\n",
        "Similar to when generating training examples we will use `build_input_from_segments()` to generate the input for the next token prediction task (with the only difference of not adding an `eos` token)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dP-oTy1AnRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import torch.nn.functional as F\n",
        "from utils import build_input_from_segments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsD-il_L3PRw",
        "colab_type": "text"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL0tSsKD3NsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constants\n",
        "max_history = 4                  # Number of previous utterances to keep in history\n",
        "no_sample = False                # Set to use greedy decoding instead of sampling\n",
        "max_length = 80                  # Maximum length of the output utterances\n",
        "min_length = 1                   # Minimum length of the output utterances\n",
        "temperature = 1                # Sampling softmax temperature\n",
        "top_k = 0                        # Filter top-k tokens before sampling (<=0: no filtering)\n",
        "top_p = .8                      # Nucleus filtering (top-p) before sampling (<=0.0: no filtering)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScXPPDB02A2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
        "\n",
        "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
        "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
        "                whose total probability mass is greater than or equal to the threshold top_p.\n",
        "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
        "                the threshold top_p.\n",
        "            threshold: a minimal threshold to keep logits\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
        "    top_k = min(top_k, logits.size(-1))\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    if top_p > 0.0:\n",
        "        # Compute cumulative probabilities of sorted tokens\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0 \n",
        "        # Back to unsorted indices and set them to -infinity\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    indices_to_remove = logits < threshold\n",
        "    logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "def sample_sequence(history, tokenizer, model, current_output=None):\n",
        "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
        "    if current_output is None:\n",
        "        current_output = []\n",
        "    for i in range(max_length):\n",
        "        instance = build_input_from_segments(history, current_output, tokenizer, with_eos=False)\n",
        "        input_ids = torch.tensor(instance[\"input_ids\"], device=device).unsqueeze(0)\n",
        "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=device).unsqueeze(0)\n",
        "        logits = model(input_ids, token_type_ids=token_type_ids)\n",
        "        if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
        "            logits = logits[0]\n",
        "        logits = logits[0, -1, :] / temperature\n",
        "        logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        prev = torch.topk(probs, 1)[1] if no_sample else torch.multinomial(probs, 1)\n",
        "        if i < min_length and prev.item() in special_tokens_ids:\n",
        "            while prev.item() in special_tokens_ids:\n",
        "                if probs.max().item() == 1:\n",
        "                    warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
        "                    break  # avoid infinitely looping over special token\n",
        "                prev = torch.multinomial(probs, num_samples=1)\n",
        "        if prev.item() in special_tokens_ids:\n",
        "            break\n",
        "        current_output.append(prev.item())\n",
        "    return current_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyxFJVW65x2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = []\n",
        "while True:\n",
        "    raw_text = input(\">>> \")\n",
        "    while not raw_text:\n",
        "        print('Prompt should not be empty!')\n",
        "        raw_text = input(\">>> \")\n",
        "    history.append(tokenizer.encode(raw_text))\n",
        "    with torch.no_grad():\n",
        "        out_ids = sample_sequence(history, tokenizer, model)\n",
        "    history.append(out_ids)\n",
        "    history = history[-(2*max_history+1):]\n",
        "    out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "    print(out_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEgjtJ8XOhmx",
        "colab_type": "text"
      },
      "source": [
        "### Save model to Google Drive\n",
        "If you are happy with your model consider saving it to your Google Drive. Note that all data on this notebook will be lost after a certain time of inactivity. Note that the model size is quite big (~500MB) so make sure you have enough space in your Google Drive.\n",
        "\n",
        "This will save only your final model state (from your directory `run_name` directory).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4XE9e7BCeTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "#@title Save to Google Drive\n",
        "save_to_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "source_directory = f'./runs/{run_name}'\n",
        "target_directory = f\"/content/drive/My Drive/AMLD/models/task3/{run_name}/\"\n",
        "include_checkpoints = False\n",
        "\n",
        "if save_to_drive:\n",
        "  logger.info(f'Copying from {source_directory} to {target_directory}...')\n",
        "  ignore_pattern = None\n",
        "  if not include_checkpoints:\n",
        "    ignore_pattern = shutil.ignore_patterns('checkpoint-*')\n",
        "  shutil.copytree(source_directory, target_directory, ignore=ignore_pattern)\n",
        "  logger.info('Successfully copied your model!')  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8s8-rGJUB8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}