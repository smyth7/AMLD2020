{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task-1-artificial-self-AMLD2020",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U1YVYBk_YIi",
        "colab_type": "text"
      },
      "source": [
        "# Meet your Artificial Self - AMLD 2020 Workshop\n",
        "### Task 1\n",
        "In this task we will explore the power of modern language models. We will use the [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple/) library by Max Woolf to fine-tune OpenAI's GPT-2 model to generate text that has the same style as the training samples.\n",
        "\n",
        "### Important resources\n",
        "* [Workshop Github repo](https://github.com/mar-muel/artificial-self-AMLD-2020/tree/master/3)\n",
        "* [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple/)\n",
        "\n",
        "\n",
        "### Approach\n",
        "We will use example data sets to fine-tune a model and explore the text generated after training dependent on a few parameters. We will also be able to provide a seed sequence to the model and see how the generated text is influenced by it.\n",
        "For fine-tuning, the model takes the path to a single plain text file with one text sample per line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UG84QEEAgoG",
        "colab_type": "text"
      },
      "source": [
        "# Setting things up\n",
        "The following cell will clone the repository and install all the necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi-_rAUclGCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi | grep -q 'failed' && echo \"STOP! You are using a runtime without a GPU. Change the runtime type before going further!\"\n",
        "%tensorflow_version 1.x\n",
        "!git clone https://github.com/mar-muel/artificial-self-AMLD-2020.git\n",
        "%cd artificial-self-AMLD-2020/1\n",
        "!pip install -r requirements-colab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCv1Sb2cArOz",
        "colab_type": "text"
      },
      "source": [
        "The next cell will prepare the data sets we can use in this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YspfgUvAreD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python prepare.py all --short-filename true --preserve-lines true\n",
        "!cp -r data ../..\n",
        "%cd ../.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgKHVSSfGnkZ",
        "colab_type": "text"
      },
      "source": [
        "Import the packages needed for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SlB3ulTld47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import requests\n",
        "import glob\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import argparse\n",
        "import logging\n",
        "from functools import partial\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)-5.5s] [%(name)-12.12s]: %(message)s')\n",
        "log = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swFVqi4FGugZ",
        "colab_type": "text"
      },
      "source": [
        "We define the following two helpers we will use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lEsmkfd2-91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class color:\n",
        "   BLUE = '\\033[94m'\n",
        "   BOLD = '\\033[1m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "def print_params(title, **kwargs):\n",
        "    print(color.BOLD + title + color.END)\n",
        "    print(30*'-')\n",
        "    for key, value in kwargs.items():\n",
        "        print(key, \"=\", color.BLUE + str(value) + color.END)\n",
        "    print(30*'-')\n",
        "\n",
        "def start_session(sess):\n",
        "    try:\n",
        "        gpt2.reset_session(sess)\n",
        "    except:\n",
        "        pass\n",
        "    return gpt2.start_tf_sess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9MQK06ZHSaT",
        "colab_type": "text"
      },
      "source": [
        "# Specification of input data set we will use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiEoMpuIn5Pa",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Choose existing or define own dataset { run: \"auto\" }\n",
        "\n",
        "#@markdown Specify run_name and input file (located in 'data/').\n",
        "run_name = 'run1' #@param {type: \"string\"}\n",
        "data_file = 'data.txt' #@param {type: \"string\"}\n",
        "\n",
        "#@markdown Choose 'custom' to use settings above. Other usecases will override 'run_name' and 'data_file'.\n",
        "usecase = 'javascript' #@param [\"custom…\",\"chess\",\"tweets\",\"music\",\"shakespeare\",\"javascript\",\"typescript\",\"json\",\"html\"] {allow-input: true}\n",
        "model_name = '124M' #@param [\"124M\"] {allow-input: true}\n",
        "\n",
        "if usecase != 'custom…':\n",
        "    run_name = usecase\n",
        "    data_file = usecase + '.txt'\n",
        "data_path = os.path.join('data', data_file)\n",
        "sess = None\n",
        "\n",
        "print_params(usecase, run_name=run_name, model_name=model_name, data_path=data_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSmhxDYIHYpL",
        "colab_type": "text"
      },
      "source": [
        "# Let's fine-tune the GPT-2 model!\n",
        "Choose the number of steps the model will be fine-tuned for. You can adjust the parameters on the right to specifiy how often you get updates on the training process, how often samples of the current model are printed, and every how many steps the model is saved.\n",
        "\n",
        "Beside the number of steps, these parameters do not influence the training. The model will be saved automatically when done fine-tuning with the amount of steps specified. You can stop the fine-tuning anytime and the current training state of the model will be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4XyUN5GnWXe",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Finetuning { run: \"auto\" }\n",
        "\n",
        "#@markdown Number of steps the model will be finetuned for.\n",
        "steps = 250 #@param {type:\"slider\", min:10, max:500, step:10}\n",
        "\n",
        "#@markdown Specification of how many steps output will be produced.\n",
        "sample_every = 100 #@param {type:\"slider\", min:10, max:200, step:20}\n",
        "save_every = 50 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "print_every = 20 #@param {type:\"slider\", min:0, max:50, step:5}\n",
        "\n",
        "def finetune(sess, run_name, model_name, data_path, steps, sample_every, save_every, print_every, b):\n",
        "    log.info(f'Run fine-tuning for run {run_name} using GPT2 model {model_name}...')\n",
        "    if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "        log.info(f\"Downloading {model_name} model...\")\n",
        "        gpt2.download_gpt2(model_name=model_name)\n",
        "    sess = start_session(sess)\n",
        "    gpt2.finetune(sess, data_path, checkpoint_dir='runs', model_name=model_name, run_name=run_name, steps=steps, sample_every=sample_every, save_every=save_every, print_every=print_every)\n",
        "\n",
        "print_params('Fine-tuning ' + run_name, steps=steps, sample_every=sample_every, save_every=save_every, print_every=print_every)\n",
        "\n",
        "finetune_handler = partial(finetune, sess, run_name, model_name, data_path, steps, sample_every, save_every, print_every)\n",
        "button = widgets.Button(description=\"Start fine-tuning\")\n",
        "button.on_click(finetune_handler)\n",
        "display(button)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KROd9OAWHd4T",
        "colab_type": "text"
      },
      "source": [
        "# Text generation\n",
        "We can now generate text mimiking the style of the learned samples.\n",
        "\n",
        "You can play around with the three parameters `length`, `temperature`, and `top_k` to influnce the generated text. Further, you can provide a seed sequence that will be the beginning of the generated text.\n",
        "\n",
        "Use the different data sets to explore how the fine-tuning works and what its' limits are. You can also use custom data sets. Just copy them to the data folder and specify the path above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwLoBxjOxfwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Text Generation { run: \"auto\" }\n",
        "\n",
        "#@markdown Parameters for text generation.\n",
        "length = 800 #@param {type:\"slider\", min:0, max:1000, step:5}\n",
        "temperature = 0.7 #@param {type:\"slider\", in:0, max:2, step:0.1}\n",
        "top_k = 0 #@param {type:\"slider\", min:0, max:5, step:0.1}\n",
        "\n",
        "def generate(sess, run_name, length, temperature, top_k, message, b):\n",
        "    print('Input: ', message)\n",
        "    output = gpt2.generate(sess, checkpoint_dir='runs', run_name=run_name, prefix=message, length=length, temperature=temperature, top_k=top_k, return_as_list=True)\n",
        "    text = output[0].split(\"\\n\")[0]\n",
        "    print('Output:', color.BLUE + text + color.END, '\\n')\n",
        "\n",
        "sess = start_session(sess)\n",
        "gpt2.load_gpt2(sess, checkpoint_dir='runs', run_name=run_name)\n",
        "\n",
        "text = widgets.Text(value='', placeholder='Beginning of sequence...', disabled=False)\n",
        "button = widgets.Button(description=\"Start text generation\")\n",
        "\n",
        "generate_handler = partial(generate, sess, run_name, length, temperature, top_k)\n",
        "button.on_click(lambda b : generate_handler(text.value, b))\n",
        "\n",
        "print()\n",
        "print_params(usecase, length=length, temperature=temperature, top_k=top_k)\n",
        "box = widgets.GridBox([text, button], layout=widgets.Layout(grid_template_columns=\"repeat(2, 350px)\"))\n",
        "display(box)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH1lMySsV91m",
        "colab_type": "text"
      },
      "source": [
        "### Save model to Google Drive\n",
        "If you are happy with your model consider saving it to your Google Drive. Note that all data on this notebook will be lost after a certain time of inactivity. Note that the model size is quite big (~500MB) so make sure you have enough space in your Google Drive.\n",
        "\n",
        "This will save only your final model state (from your directory `run_name` directory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukz2b12MV64o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Save model to Google Drive { run: \"auto\" }\n",
        "\n",
        "#@markdown Directory (within your Google Drive) where you want to save the model to.\n",
        "drive_location = \"My Drive/AMLD/models/task1/\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Do you want to mount your Google Drive?\n",
        "mount_drive = True #@param {type:\"boolean\"}\n",
        "#@markdown Follow instructions below this cell for mounting Google Drive and click the **[Save to Google Drive]** button when it appears.\n",
        "\n",
        "if mount_drive:\n",
        "    from google.colab import drive\n",
        "    import shutil\n",
        "\n",
        "    mount_location = \"./drive/\"\n",
        "    log.info(f'Mount Google Drive...')\n",
        "    drive.mount(mount_location)\n",
        "\n",
        "    source_directory = os.path.join('./runs', run_name)\n",
        "    target_directory = os.path.join(mount_location, drive_location, run_name)\n",
        "\n",
        "    def save_model(mount_location, source_directory, target_directory, b):\n",
        "        log.info(f'Copying from {source_directory} to {target_directory}...')\n",
        "        shutil.copytree(source_directory, target_directory)\n",
        "        log.info('Successfully copied your model!')\n",
        "\n",
        "    print_params('Save model to Google Drive', source_directory=source_directory, target_directory=target_directory)\n",
        "\n",
        "    save_model_handler = partial(save_model, mount_location, source_directory, target_directory)\n",
        "    button = widgets.Button(description=\"Save to Google Drive\")\n",
        "    button.on_click(save_model_handler)\n",
        "    display(button)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NEA3nUogkto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}