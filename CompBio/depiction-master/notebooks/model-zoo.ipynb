{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n=16, samples_per_class=1000):\n",
    "    \"\"\"\n",
    "    Generate some classification data\n",
    "    \n",
    "    Args:\n",
    "        n (int): square root of the number of features.\n",
    "        samples_per_class (int): number of samples per class.\n",
    "    \n",
    "    Returns:\n",
    "        a tuple containing data and labels.\n",
    "    \"\"\"\n",
    "    # data for a class\n",
    "    a_class_samples = np.random.rand(samples_per_class, n, n).astype(np.float32)\n",
    "    a_class_labels = np.zeros(samples_per_class, dtype=int)\n",
    "    # data for another class\n",
    "    another_class_samples = np.array([\n",
    "        np.eye(n)*np.random.rand(1).item()\n",
    "        for _ in range(samples_per_class)\n",
    "    ]).astype(np.float32)\n",
    "    another_class_labels = np.ones(samples_per_class, dtype=int)\n",
    "    # aggregate data\n",
    "    data = np.vstack([a_class_samples, another_class_samples])\n",
    "    labels = np.hstack([a_class_labels, another_class_labels])\n",
    "    # prepare a shuffled index\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return data[indices], labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "n = 16\n",
    "features = n*n\n",
    "number_of_classes = 2\n",
    "X_train, y_train = generate_data(n=n)\n",
    "X_test, y_test = generate_data(n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "units = [32, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"A MultiLayer Perceptron class.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, features,\n",
    "        units=[8], number_of_classes=2,\n",
    "        activation_module=torch.nn.ReLU\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inititalize the MLP.\n",
    "        \n",
    "        Args:\n",
    "            features (int): number of features.\n",
    "            units (list): list of hidden layer units.\n",
    "            number_of_classes (int): number of classes to predict.\n",
    "            activation_module (torch.nn.Module): module representing\n",
    "                the activation function to apply in the hidden layers.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.units = [features] + units\n",
    "        self.activation_module = activation_module\n",
    "        self.hidden_layers = torch.nn.Sequential(*[\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_size, output_size),\n",
    "                self.activation_module()\n",
    "            )\n",
    "            for input_size, output_size in zip(\n",
    "                self.units, self.units[1:]\n",
    "            )\n",
    "        ])\n",
    "        self.last_layer = self.last_layer = torch.nn.Sequential(*[\n",
    "            torch.nn.Linear(self.units[-1], number_of_classes),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, sample):\n",
    "        \"\"\"\n",
    "        Apply the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            sample (torch.Tensor): a torch.Tensor representing a sample.\n",
    "        Returns:\n",
    "            a torch.Tensor containing softmaxed predictions.\n",
    "        \"\"\"\n",
    "        encoded_sample =  self.hidden_layers(sample)\n",
    "        return self.last_layer(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_train.reshape(-1, features))\n",
    "model = MLP(features=features, units=units, number_of_classes=number_of_classes)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(\n",
    "    features,\n",
    "    units=[8], number_of_classes=2,\n",
    "    activation='relu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a MLP.\n",
    "\n",
    "    Args:\n",
    "        features (int): number of features.\n",
    "        units (list): list of hidden layer units.\n",
    "        number_of_classes (int): number of classes to predict.\n",
    "        activation (str): string identifying the activation used.\n",
    "    \n",
    "    Returns:\n",
    "        a tf.keras.Model.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units[0], activation=activation, input_shape=(features,)))\n",
    "    for unit in units[1:]:\n",
    "        model.add(tf.keras.layers.Dense(unit, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(number_of_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.reshape(-1, features)\n",
    "model = mlp(features=features, units=units, number_of_classes=number_of_classes)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "units = [32, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    \"\"\"An AutoEncoder class.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, features,\n",
    "        units=[8], activation_module=torch.nn.ReLU\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inititalize the AE.\n",
    "        \n",
    "        Args:\n",
    "            features (int): number of features.\n",
    "            units (list): list of hidden layer units.\n",
    "            activation_module (torch.nn.Module): module representing\n",
    "                the activation function to apply in the hidden layers.\n",
    "        \"\"\"\n",
    "        super(AE, self).__init__()\n",
    "        self.units = [features] + units\n",
    "        self.activation_module = activation_module\n",
    "        zipped_units = list(zip(\n",
    "            self.units, self.units[1:]\n",
    "        ))\n",
    "        # encoding\n",
    "        self.encoder = torch.nn.Sequential(*[\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_size, output_size),\n",
    "                self.activation_module()\n",
    "            )\n",
    "            for input_size, output_size in zipped_units\n",
    "        ])\n",
    "        # decoding\n",
    "        last_decoder_units, *hidden_decoder_units = zipped_units\n",
    "        self.decoder = torch.nn.Sequential(*[\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_size, output_size),\n",
    "                self.activation_module()\n",
    "            )\n",
    "            for input_size, output_size in map(\n",
    "                lambda t: t[::-1],\n",
    "                hidden_decoder_units[::-1]\n",
    "            )\n",
    "        ])\n",
    "        self.last_layer = torch.nn.Linear(*last_decoder_units[::-1])\n",
    "    \n",
    "    def forward(self, sample):\n",
    "        \"\"\"\n",
    "        Apply the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            sample (torch.Tensor): a torch.Tensor representing a sample.\n",
    "        Returns:\n",
    "            a torch.Tensor containing the reconstructed example.\n",
    "        \"\"\"\n",
    "        encoded_sample = self.encoder(sample)\n",
    "        decoded_sample = self.decoder(encoded_sample)\n",
    "        return self.last_layer(decoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_train.reshape(-1, features))\n",
    "model = AE(features=features, units=units)\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get encoded representation\n",
    "model.encoder(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae(features, units=[8], activation='relu'):\n",
    "    \"\"\"\n",
    "    Build an AE.\n",
    "\n",
    "    Args:\n",
    "        features (int): number of features.\n",
    "        units (list): list of hidden layer units.\n",
    "        number_of_classes (int): number of classes to predict.\n",
    "        activation (str): string identifying the activation used.\n",
    "    \n",
    "    Returns:\n",
    "        a tf.keras.Model.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    # encoding\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units[0], activation=activation, input_shape=(features,)\n",
    "    ))\n",
    "    for unit in units[1:]:\n",
    "        model.add(tf.keras.layers.Dense(unit, activation=activation))\n",
    "    # decoding\n",
    "    for unit in units[::-1][1:]:\n",
    "        model.add(tf.keras.layers.Dense(unit, activation=activation))\n",
    "    model.add(tf.keras.layers.Dense(features))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X_train.reshape(-1, features)\n",
    "model = ae(features=features, units=units)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get encoded representation\n",
    "encoder = tf.keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.layers[len(units) - 1].output\n",
    ")\n",
    "encoder.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "filters = [64, 32]\n",
    "kernel_size = (3, 3)\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \"\"\"A Convolutional Neural Network class.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, channels,\n",
    "        filters=[8], kernel_size=(3,3),\n",
    "        number_of_classes=2,\n",
    "        activation_module=torch.nn.ReLU\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inititalize the CNN.\n",
    "        \n",
    "        Args:\n",
    "            channels (int): number of input channels.\n",
    "            filters (list): list of filters.\n",
    "            kernel_size (tuple): size of the kernel.\n",
    "            number_of_classes (int): number of classes to predict.\n",
    "            activation_module (torch.nn.Module): module representing\n",
    "                the activation function to apply in the hidden layers.\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.filters = [channels] + filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation_module = activation_module\n",
    "        self.stacked_convolutions = torch.nn.Sequential(*[\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(input_size, output_size, kernel_size),\n",
    "                self.activation_module(),\n",
    "                torch.nn.MaxPool2d((2,2), stride=2)\n",
    "            )\n",
    "            for input_size, output_size in zip(\n",
    "                self.filters, self.filters[1:]\n",
    "            )\n",
    "        ])\n",
    "        self.last_layer = torch.nn.Sequential(*[\n",
    "            torch.nn.Linear(self.filters[-1], number_of_classes),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, sample):\n",
    "        \"\"\"\n",
    "        Apply the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            sample (torch.Tensor): a torch.Tensor representing a sample.\n",
    "        Returns:\n",
    "            a torch.Tensor containing softmaxed predictions.\n",
    "        \"\"\"\n",
    "        encoded_sample = self.stacked_convolutions(sample)\n",
    "        return self.last_layer(encoded_sample.mean((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(np.expand_dims(X_train, 1))\n",
    "model = CNN(\n",
    "    channels=channels, filters=filters,\n",
    "    kernel_size=kernel_size,\n",
    "    number_of_classes=number_of_classes\n",
    ")\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(\n",
    "    channels, input_shape,\n",
    "    filters=[8], kernel_size=(3,3),\n",
    "    number_of_classes=2, activation='relu'):\n",
    "    \"\"\"\n",
    "    Build a CNN.\n",
    "\n",
    "    Args:\n",
    "        channels (int): number of input channels.\n",
    "        input_shape (tuple): input shape.\n",
    "        filters (list): list of filters.\n",
    "        kernel_size (tuple): size of the kernel.\n",
    "        number_of_classes (int): number of classes to predict.\n",
    "        activation (str): string identifying the activation used.\n",
    "    \n",
    "    Returns:\n",
    "        a tf.keras.Model.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "        filters[0], kernel_size, activation=activation,\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    for a_filter in filters[1:]:\n",
    "        model.add(tf.keras.layers.Conv2D(\n",
    "            a_filter, kernel_size, activation=activation\n",
    "        ))\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dense(number_of_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X_train, 3)\n",
    "model = cnn(\n",
    "    channels=channels, input_shape=X.shape[1:],\n",
    "    filters=filters, kernel_size=kernel_size,\n",
    "    number_of_classes=number_of_classes\n",
    ")\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "units = [32, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"A Recurrent Neural Network class.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_size, units=[8],\n",
    "        number_of_classes=2, rnn_cell=torch.nn.GRU\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inititalize the RNN.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): size of the input.\n",
    "            units (list): list of hidden layer units.\n",
    "            number_of_classes (int): number of classes to predict.\n",
    "            rnn_cell (torch.nn.RNNBase): a RNN cell.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.units = [input_size] + units\n",
    "        self.rnn_layers = [\n",
    "            rnn_cell(input_size, output_size)\n",
    "            for input_size, output_size in zip(\n",
    "                self.units, self.units[1:]\n",
    "            )\n",
    "        ]\n",
    "        self.last_layer = torch.nn.Sequential(*[\n",
    "            torch.nn.Linear(self.units[-1], number_of_classes),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, sample):\n",
    "        \"\"\"\n",
    "        Apply the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            sample (torch.Tensor): a torch.Tensor representing a sample.\n",
    "        Returns:\n",
    "            a torch.Tensor containing softmaxed predictions.\n",
    "        \"\"\"\n",
    "        encoded_sample = sample\n",
    "        for rnn_layer in self.rnn_layers[:-1]:\n",
    "            encoded_sample, _ = rnn_layer(encoded_sample)\n",
    "        encoded_sample = self.rnn_layers[-1](encoded_sample)[1].squeeze(0)\n",
    "        return self.last_layer(encoded_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_train.transpose((1,0,2)))\n",
    "model = RNN(\n",
    "    input_size=n, units=units,\n",
    "    number_of_classes=number_of_classes\n",
    ")\n",
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(\n",
    "    sequence_length, input_size,\n",
    "    units=[8], number_of_classes=2,\n",
    "    rnn_cell=tf.keras.layers.GRU\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a RNN.\n",
    "\n",
    "    Args:\n",
    "        sequence_length (int): length of the sequence.\n",
    "        input_size (int): size of the input.\n",
    "        units (list): list of hidden layer units.\n",
    "        number_of_classes (int): number of classes to predict.\n",
    "        rnn_cell (tf.keras.layers.RNN): a RNN cell.\n",
    "\n",
    "    Returns:\n",
    "        a tf.keras.Model.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    is_stacked = len(units) > 1\n",
    "    model.add(rnn_cell(units=units[0], input_shape=(16, 16,), return_sequences=is_stacked))\n",
    "    for unit in units[1:-1]:\n",
    "        model.add(rnn_cell(units=unit, return_sequences=True))\n",
    "    if is_stacked:\n",
    "        model.add(rnn_cell(units=units[-1]))\n",
    "    model.add(tf.keras.layers.Dense(number_of_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "model = rnn(\n",
    "    sequence_length=n, input_size=n, units=units,\n",
    "    number_of_classes=number_of_classes\n",
    ")\n",
    "model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "0e2c7ec5-4435-4930-84fa-5616134ef42b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
