{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary package imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.pix2pixHD.util.image_pool import ImagePool\n",
    "from src.pix2pixHD.models.base_model import BaseModel\n",
    "from src.pix2pixHD.models import networks\n",
    "import src.config.train_opt as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Teaching Objective\n",
    "\n",
    "The aim of this notebook is to show the participants how the ***GAN loss*** are computed and what are the ***inputs*** of the models which implicitely affect the losses,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forwards Pass Pix2Pix\n",
    "\n",
    "In this case the temporal smoothing was not implemented.\n",
    "\n",
    "In the forward pass the different losses used for the training are defined. The most important components of the forward pass are the following:\n",
    "\n",
    "Networks to be trained to generate and discriminate\n",
    "1. ``self.netG``: the generator network\n",
    "2. ``self.netD``: the discriminator network\n",
    "\n",
    "Loss Discriminator\n",
    "1. ``loss_D_fake``, discriminator loss from discriminating fake generated images: -log(1-D(G(z)))\n",
    "2. ``loss_D_real``, discriminator loss from discriminating real images sampled from data: -log(D(x))\n",
    "\n",
    "Loss Generator\n",
    "1. ``loss_G_Gan``, generator loss from the \"Fake Pass Loss\" which label the fake images as correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Original Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, label, inst, image, feat, infer=False):\n",
    "    # Encode Inputs\n",
    "    input_label, inst_map, real_image, feat_map = self.encode_input(label, inst, image, feat)  \n",
    "\n",
    "    # Fake Generation\n",
    "    if self.use_features:\n",
    "        if not self.opt.load_features:\n",
    "            feat_map = self.netE.forward(real_image, inst_map)                     \n",
    "        input_concat = torch.cat((input_label, feat_map), dim=1)                        \n",
    "    else:\n",
    "        input_concat = input_label\n",
    "    # TODO----------------------#    \n",
    "    fake_image = self.netG.forward(input_concat.float())\n",
    "\n",
    "    # Fake Detection and Loss\n",
    "    pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n",
    "    loss_D_fake = self.criterionGAN(pred_fake_pool, False)        \n",
    "\n",
    "    # Real Detection and Loss        \n",
    "    pred_real = self.discriminate(input_label, real_image)\n",
    "    loss_D_real = self.criterionGAN(pred_real, True)\n",
    "\n",
    "    # GAN loss (Fake Passability Loss)        \n",
    "    pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))        \n",
    "    loss_G_GAN = self.criterionGAN(pred_fake, True)               \n",
    "\n",
    "    # GAN feature matching loss\n",
    "    loss_G_GAN_Feat = 0\n",
    "    if not self.opt.no_ganFeat_loss:\n",
    "        feat_weights = 4.0 / (self.opt.n_layers_D + 1)\n",
    "        D_weights = 1.0 / self.opt.num_D\n",
    "        for i in range(self.opt.num_D):\n",
    "            for j in range(len(pred_fake[i])-1):\n",
    "                loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
    "                    self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n",
    "\n",
    "    # VGG feature matching loss\n",
    "    loss_G_VGG = 0\n",
    "    if not self.opt.no_vgg_loss:\n",
    "        loss_G_VGG = self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat\n",
    "\n",
    "    # Only return the fake_B image if necessary to save BW\n",
    "    return [ self.loss_filter( loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake ), None if not infer else fake_image ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the notebook of easier comprehension, the functions have been simplified and commented.\n",
    "\n",
    "``forward`` function modifications:\n",
    "\n",
    "-``netE``: not in use\n",
    "\n",
    "-``nwtG`` and ``netD`` become functions themselves which are initialized before\n",
    "\n",
    "-``self.encode_input`` become a separate function to explain what goes into the model and is placed outside the forward to teach the audience what goes in the forward path and needs to be encoded. The encoding part itself is probably irrelevant, but a independent function has to be created if an example of forward pass has to be created.\n",
    "\n",
    "The forward pass is the most interesting one, here we can show it detached from the class itself, and then when we show the training loop, we highlight where the model calls it.\n",
    "\n",
    "\n",
    "\n",
    "*Ideas to clean up the function more*:\n",
    "\n",
    "-``criterionGAN`` becomes a function\n",
    "\n",
    "-condition of training the gan based on the generator and discriminator features' neurons.\n",
    "\n",
    "-condition of training the gan using VGG to recognize images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Easy to understand break down of the training pass***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that we use the cells below to illustrate one pass of the training. Hence, these are static cells which are detached from the class whose parameters are updated as the training happens\n",
    "\n",
    "Running the cell below on Cuda available machine, the structure of the network will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalGenerator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(18, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (12): ReLU(inplace)\n",
      "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (15): ReLU(inplace)\n",
      "    (16): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (20): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (21): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (22): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (23): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (24): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (27): ReLU(inplace)\n",
      "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (30): ReLU(inplace)\n",
      "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (33): ReLU(inplace)\n",
      "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (36): ReLU(inplace)\n",
      "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (39): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1930adea3fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m generator_model=networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n\u001b[1;32m      3\u001b[0m                                       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_downsample_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_blocks_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_local_enhancers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                       opt.n_blocks_local, opt.norm, gpu_ids=opt.gpu_ids)      \n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnetD_input_nc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_nc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_nc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m discriminator_model=networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n",
      "\u001b[0;32m~/Desktop/Visium/01-VISI-Amld-Jan2020/src/pix2pixHD/models/networks.py\u001b[0m in \u001b[0;36mdefine_G\u001b[0;34m(input_nc, output_nc, ngf, netG, n_downsample_global, n_blocks_global, n_local_enhancers, n_blocks_local, norm, gpu_ids)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "netG_input_nc = opt.label_nc if opt.label_nc != 0 else opt.input_nc\n",
    "generator_model=networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n",
    "                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n",
    "                                      opt.n_blocks_local, opt.norm, gpu_ids=opt.gpu_ids)      \n",
    "netD_input_nc = input_nc + opt.output_nc\n",
    "discriminator_model=networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n",
    "                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=opt.gpu_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(self, label_map, inst_map=None, real_image=None, feat_map=None, infer=False): \n",
    "    \n",
    "    if opt.label_nc == 0:\n",
    "        input_label = label_map.data.to(self.device)\n",
    "    else:\n",
    "        # create one-hot vector for label map \n",
    "        size = label_map.size()\n",
    "        oneHot_size = (size[0], opt.label_nc, size[2], size[3])\n",
    "        input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_().to(self.device)\n",
    "        input_label = input_label.scatter_(1, label_map.data.long().to(self.device), 1.0)\n",
    "        if opt.data_type == 16:\n",
    "            input_label = input_label.half()\n",
    "\n",
    "    # get edges from instance map\n",
    "    if not opt.no_instance:\n",
    "        inst_map = inst_map.data.to(self.device)\n",
    "        \n",
    "        edge = torch.ByteTensor(inst_map.size()).zero_().to(self.device)\n",
    "        edge[:,:,:,1:] = edge[:,:,:,1:] | (t[:,:,:,1:] != t[:,:,:,:-1])\n",
    "        edge[:,:,:,:-1] = edge[:,:,:,:-1] | (t[:,:,:,1:] != t[:,:,:,:-1])\n",
    "        edge[:,:,1:,:] = edge[:,:,1:,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n",
    "        edge[:,:,:-1,:] = edge[:,:,:-1,:] | (t[:,:,1:,:] != t[:,:,:-1,:])\n",
    "        \n",
    "        if opt.data_type==16:\n",
    "            edge_map = edge.half()\n",
    "        else:\n",
    "            edge_map = edge.float()\n",
    "        \n",
    "        input_label = torch.cat((input_label, edge_map), dim=1)\n",
    "        \n",
    "    input_label = Variable(input_label, volatile=infer)\n",
    "\n",
    "    # real images for training\n",
    "    if real_image is not None:\n",
    "        real_image = Variable(real_image.data.to(self.device))\n",
    "\n",
    "    # instance map for feature encoding\n",
    "    if self.use_features:\n",
    "        # get precomputed feature maps\n",
    "        if opt.load_features:\n",
    "            feat_map = Variable(feat_map.data.to(self.device))\n",
    "\n",
    "    return input_label, inst_map, real_image, feat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By completing the cell below the participants will learn what are the labels, what are the instances, what are the images and what are the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-87c887113609>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-87c887113609>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    label = # input the label\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Encode Inputs\n",
    "\n",
    "label = # input the label\n",
    "inst = # input the instance\n",
    "image = # input the image\n",
    "feat = # input the feature\n",
    "\n",
    "input_label, inst_map, real_image, feat_map = encode_input(label, inst, image, feat)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By going through the forward pass, the participants will get acquainted with the loss involved in the GAN and they will understand how the input previously encoded are used to generate or discriminate fake and real images. Additionally the Feature Matching Loss and VGG Loss will be presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(label, inst, image, feat, infer=False):\n",
    "\n",
    "    input_concat = input_label\n",
    "    \n",
    "    ### Fake Generation\n",
    "    fake_image = generator_model.forward(input_concat.float())\n",
    "\n",
    "    ### Fake Detection\n",
    "    \n",
    "    #concatenate the condition input_label used to generate the image and the fake image \n",
    "    input_concat = torch.cat((input_label, fake_image.detach()), dim=1) \n",
    "    \n",
    "    #adapt the image dimension\n",
    "    fake_query = ImagePool(opt.pool_size).query(input_concat)\n",
    "        \n",
    "    #predict if the generate image is False or True\n",
    "    pred_fake_pool = discriminator_model.forward(fake_query)\n",
    "    \n",
    "    #predict if the real images is False or True      \n",
    "    pred_real = discriminator_model.forward(input_label, real_image)\n",
    "    \n",
    "    ### Dicriminator Loss Based on Fake Images\n",
    "    \n",
    "    # label=  .....the ground truth to compute the discriminator loss for fake images......\n",
    "    \n",
    "    loss_D_fake = self.criterionGAN(pred_fake_pool, label)      \n",
    "    \n",
    "    ### Dicriminator Loss Based on Fake Images\n",
    "    \n",
    "    # label=  .....the ground truth to compute the discriminator loss for real images......\n",
    "    \n",
    "    loss_D_real = self.criterionGAN(pred_real, label)\n",
    "\n",
    "    ### Generator loss (Fake Passability Loss)        \n",
    "    pred_fake = discriminator_model.forward(torch.cat((input_label, fake_image), dim=1))  \n",
    "    \n",
    "    # label=  .....the ground truth to compute the generate loss to teach it to generate fake images......\n",
    "    \n",
    "    loss_G_GAN = self.criterionGAN(pred_fake, label)\n",
    "    \n",
    "\n",
    "    ### Feature Matching Loss\n",
    "    loss_G_GAN_Feat = 0\n",
    "    feat_weights = 4.0 / (opt.n_layers_D + 1)\n",
    "    D_weights = 1.0 / opt.num_D\n",
    "    for i in range(opt.num_D):\n",
    "        for j in range(len(pred_fake[i])-1):\n",
    "            loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
    "                self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * opt.lambda_feat\n",
    "            \n",
    "    \n",
    "\n",
    "    ### VGG Loss\n",
    "    loss_G_VGG = 0\n",
    "    \n",
    "    input1, input2 = # ...... the inputs of to calculate the VGG loss using the L1 function to ensure VGG classifies fake and real images equally .........\n",
    "    \n",
    "    \n",
    "    loss_G_VGG = self.criterionVGG(fake_image, real_image) * opt.lambda_feat\n",
    "    \n",
    "    \n",
    "    # set the flags based on if the VGG and the Feature Matching Losses are used\n",
    "    flags = (True, not opt.no_ganFeat_loss, not opt.no_vgg_loss, True, True)\n",
    "    \n",
    "    # only return the fake_B image if necessary to save BW\n",
    "    losses_to_return=[[l for (l,f) in zip((loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake),flags) if f], None if not infer else fake_image]\n",
    "    \n",
    "\n",
    "    return losses_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "\n",
    "\n",
    "1) Contextualize the GAN pass in the training loop (train_pose2vid.py function)\n",
    "\n",
    "2) Simplify the functions based on the input parameter of ``opt``.\n",
    "\n",
    "3) Change self.device according to the machine is going to be used (even if the public won't be able to train the model live, it should point to the right machine)\n",
    "\n",
    "4) Understand what are the input and how in the notebook they can be loaded to run one pass of the forward\n",
    "\n",
    "5) Simplify function for criterionGAN, criterionFeat and criterionVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amld",
   "language": "python",
   "name": "amld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
