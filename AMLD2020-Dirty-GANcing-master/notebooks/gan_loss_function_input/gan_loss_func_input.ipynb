{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANcing Model\n",
    "\n",
    "The aim of this notebook is to show the participants how the ***GAN loss*** are computed and what are the ***inputs*** of the models which implicitely affect the losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "!pip install googledrivedownloader\n",
    "\n",
    "!git clone https://github.com/VisiumCH/AMLD2020-Dirty-Gancing.git dancing\n",
    "\n",
    "!rm -rf dancing/data/targets/gianluca\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1NnV2OCYV3EchLSDdMTfI3KwGZkwENqvv',\n",
    "                                    dest_path='./gianluca.zip',\n",
    "                                    unzip=True)\n",
    "\n",
    "!mv gianluca_1000 dancing/data/targets/gianluca\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1HO3YQJlumkyA-7RVuR_TCAhqFZg1vY8J',\n",
    "                                    dest_path='./gianluca_pretrained.zip',\n",
    "                                    unzip=True)\n",
    "\n",
    "!mv gianluca_pretrained dancing/checkpoints/gianluca\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1ljUgt6XGvv5pCMhzFChkhe0BYXkHKEoq',\n",
    "                                    dest_path='./gianluca.mp4',\n",
    "                                    unzip=False)\n",
    "\n",
    "!mv gianluca.mp4 dancing/results\n",
    "\n",
    "!rm -rf dancing/checkpoints/testnotebook\n",
    "\n",
    "%cd dancing/notebooks/gan_loss_function_input\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "sys.path.append('../..')\n",
    "sys.path.append('../../src/pix2pixHD/')\n",
    "\n",
    "import src.config.train_opt_notebook as opt\n",
    "import src.pix2pixHD.util.util as util\n",
    "\n",
    "from src.utils.torch_utils import get_torch_device\n",
    "from src.utils.plt_utils import init_figure, plot_current_results\n",
    "from src.pix2pixHD.data.data_loader import CreateDataLoader\n",
    "from src.pix2pixHD.models import networks\n",
    "from src.pix2pixHD.models.base_model import BaseModel\n",
    "from src.pix2pixHD.models.models import create_model\n",
    "from src.pix2pixHD.util.image_pool import ImagePool\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = get_torch_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make people Dance\n",
    "\n",
    "The model that we implemented for this workshop is taken from [Chan et. al.](https://arxiv.org/pdf/1808.07371.pdf), who first developed this architecture at UC Berkeley.\n",
    "\n",
    "![GAN Architecture](imgs/gan_architecture.png)\n",
    "\n",
    "The main idea behind the model is relatively simple. We want to teach a GAN to generate pictures of a person in a given environment, conditioned on the pose that this person should have. Just like for the conditional GAN, we have two models that will ompete against each other:\n",
    "\n",
    "- The **Generator** which receives as input a target **pose**.\n",
    "- The **Discriminator:** Which receives as input the target **pose** as well as in image of the subject, which can be real or generated\n",
    "\n",
    "In order to train thesetwo models, the Loss functions that were designed are a bit more complex. You will implement three of them:\n",
    "\n",
    "- The **Adversarial Loss** which represents the competition between **G** and **D**.\n",
    "- The **Feature Matching Loss** which represents how similar the generated images are to the real images.\n",
    "- The **Perceptual Loss** which represents how realistic the generated images are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before initializing the model class, the losses used to train GAN are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Loss (LSGAN)\n",
    "\n",
    "<blockquote>X. Mao, Q. Li, H. Xie, R. Y. Lau, and Z. Wang. Least Squares Generative Adversarial Networks. 2016. <a href=\"https://arxiv.org/abs/1611.04076\">https://arxiv.org/abs/1611.04076</a></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the original GAN, the discriminator acts as a classifier. The sigmoid of its output is seen as the probability that its input is real. The adversarial loss is based on a binary cross entropy loss. The objective of the generator is for fake samples to be classified as real, i.e. to elicit a high discriminator output.</p>\n",
    "\n",
    "<p>In LSGAN, the discriminator acts as a regressor. The two classes are assigned a code, for instance 0 for fake and 1 for real. The output of the discriminator is seen as an estimation of its input class. The adversarial loss is based on a mean squared error (MSE). The objective of the generator is for fake samples to elicit a discriminator output close to 1.</p>\n",
    "\n",
    "<img src=\"../images/gan_losses.png\" width=\"500px\">\n",
    "\n",
    "<p>What is the advantage of using such a loss when training a GAN?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<p>The core intuition is that, in a vanilla GAN, some fake samples can be classified as real by the discriminator even if they are not close to the real data. Using the crossentropy loss, the gradient vanishes for these samples, and the generator learns barely anything from them.</p>\n",
    "\n",
    "<p>Using the mean squared error, fake samples that are 'on the right side of the decision boundary' (closer to real data than to typical fake data) are still penalized if they are too far from the real data.</p>\n",
    "\n",
    "<p>The two plots below illustrate that idea in a simple setup. The real data is one-dimensional and its underlying distibution is a Gaussian. The generator learns the parameters of this Gaussian, and the discriminator output is an affine transformation of the sample value.</p>\n",
    "\n",
    "<img src=\"../images/gan_loss_crossentropy.png\" width=\"500px\">\n",
    "<img src=\"../images/gan_loss_mse.png\" width=\"500px\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Fill in the missing parts of the Adversarial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_Loss(x,\n",
    "            target_is_real):\n",
    "    \"\"\" Adversarial loss function for vanilla GAN or LS-GAN\n",
    "    \n",
    "    Args:\n",
    "        x (iterable of iterables of torch.Tensors): discriminator activations\n",
    "        target_is_real (bool): whether x should be treated as fake or real samples\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: adversarial loss value for x\n",
    "    \"\"\"\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    target_value = ... # Implement me\n",
    "\n",
    "    loss = 0\n",
    "    for activations in x:\n",
    "        pred = activations[-1].to(device)\n",
    "        target_tensor = torch.FloatTensor(pred.size()).requires_grad_(False) \\\n",
    "                             .fill_(target_value).to(device)\n",
    "        \n",
    "        loss += ... # Implement me\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching Loss\n",
    "\n",
    "<blockquote>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X.. Improved Techniques for Training GANs. In Advances in neural information processing systems, 2016. <a href=\"https://arxiv.org/abs/1606.03498\">https://arxiv.org/abs/1606.03498</a></blockquote>\n",
    "\n",
    "The purpose of the **Feature Matching Loss** is to help the generator create images that look similar to the real images of the **training set**. The idea is to compare the **feature value** of the different layers of **D** when it is fed with real images and fake images. If the generated images are close to the real ones, the features of **D** will take approximately the same values, and the loss will be small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Fill in the missing parts of the Feature Matching Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feat_Match_loss(pred_fake,\n",
    "                    pred_real):\n",
    "    \"\"\" Feature Matching loss for Pix2Pix\n",
    "    \n",
    "    Args:\n",
    "        pred_fake (iterable of iterables of torch.Tensors): discriminator activations for fake samples\n",
    "        pred_real (iterable of iterables of torch.Tensors): discriminator activations for real samples\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: feature matching loss value for pred_fake and pred_real\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    D_weights = 1 / opt.num_D\n",
    "    feat_weights = 4 / (opt.n_layers_D + 1)\n",
    "    factor = D_weights * feat_weights * opt.lambda_feat\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(opt.num_D):\n",
    "        for j in range(len(pred_fake[i]) - 1):\n",
    "            loss += factor * ... # Implement me\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Loss (based on VGG19)\n",
    "\n",
    "<blockquote>J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. <a href=\"https://arxiv.org/abs/1603.08155\">https://arxiv.org/abs/1603.08155</a></blockquote>\n",
    "\n",
    "<img src=\"../images/vgg19.png\" width=\"600px\">\n",
    "\n",
    "The purpose of the **Perceptual Loss** is to help the generator create images that look **realistic** in general. To compute this loss, we feed the generated and the real images to a **pretrained VGG16** network and we compare the value of the features at different layers. If the generated images look real, the value of the loss will be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Fill in the missing parts of the Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self, gpu_ids):\n",
    "        super(VGGLoss, self).__init__()\n",
    "        if len(gpu_ids) > 0:\n",
    "            self.vgg = networks.Vgg19().cuda()\n",
    "        else:\n",
    "            self.vgg = networks.Vgg19()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\" Perceptual loss for Pix2Pix\n",
    "        Args:\n",
    "            x, y (torch.Tensor): samples to compare\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: perceptual loss value for x, y\n",
    "        \"\"\" \n",
    "        x_vgg, y_vgg = ... # Implement me\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * ... # Implement me\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model class is initialized.\n",
    "\n",
    "First, the generator (``self.net_G``) and the discriminator (``self.net_D``) models are initialized in the ``initialize`` function:\n",
    "\n",
    "Among the config variables which are used to define the network, some which are worth noticing are:\n",
    "\n",
    "- ``netG_input_nc`` is the number of classes which are fed in the generator model. In other words it is the number of coordinates to define a pose. ``netG_input_nc`` is the number of channels which correspond to the number of joints. The poses are fed as images with ``netG_input_nc`` number of channels.\n",
    "- ``netD_input_nc`` is the number of input classes going into the discriminator which are the classes defining a pose and those defining the image. i.e the 3 RGB channels are added to the input.\n",
    "\n",
    "Second, the forward pass is defined in ``forward``:\n",
    "\n",
    "The forward pass is the essential one which allows the network to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do: Implement the missing parts of the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gancing(BaseModel):\n",
    "    \n",
    "    def initialize(self, opt):\n",
    "        BaseModel.initialize(self, opt)\n",
    "        if opt.resize_or_crop != 'none' or not opt.isTrain: # when training at full res this causes OOM\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.isTrain = opt.isTrain\n",
    "\n",
    "        ######check for gpu\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        self.device = 'cuda' if (len(self.gpu_ids) > 0) else 'cpu'\n",
    "        \n",
    "        input_nc = opt.label_nc if opt.label_nc != 0 else opt.input_nc\n",
    "\n",
    "        # Generator network\n",
    "        netG_input_nc = input_nc        \n",
    "        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n",
    "                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n",
    "                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)        \n",
    "\n",
    "        # Discriminator network\n",
    "        if self.isTrain:\n",
    "            use_sigmoid = opt.no_lsgan\n",
    "            netD_input_nc = input_nc + opt.output_nc\n",
    "            self.netD = networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n",
    "                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=self.gpu_ids)\n",
    "\n",
    "\n",
    "        # load networks\n",
    "        if not self.isTrain or opt.continue_train or opt.load_pretrain:\n",
    "            pretrained_path = '' if not self.isTrain else opt.load_pretrain\n",
    "            self.load_network(self.netG, 'G', opt.which_epoch, pretrained_path)            \n",
    "            if self.isTrain:\n",
    "                self.load_network(self.netD, 'D', opt.which_epoch, pretrained_path)         \n",
    "\n",
    "        # set loss functions and optimizers\n",
    "        if self.isTrain:            \n",
    "            \n",
    "            # Names so we can breakout loss\n",
    "            self.loss_names = ['G_GAN','G_GAN_Feat','G_VGG','D_real', 'D_fake']\n",
    "            \n",
    "            # Define VGGloss so we don't load VGG at each batch\n",
    "            self.VGG_Loss = VGGLoss(opt.gpu_ids)\n",
    "\n",
    "            # initialize optimizers\n",
    "            params = list(self.netG.parameters())\n",
    "            self.optimizer_G = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))                            \n",
    "\n",
    "            # optimizer D                        \n",
    "            params = list(self.netD.parameters())    \n",
    "            self.optimizer_D = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            \n",
    "    \n",
    "    def encode_input(self, label_map, real_image): \n",
    "    \n",
    "        # create one-hot vector for the pose\n",
    "        size = label_map.size()\n",
    "        oneHot_size = (size[0], opt.label_nc, size[2], size[3])\n",
    "        input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_().to(self.device)\n",
    "        input_label = input_label.scatter_(1, label_map.data.long().to(self.device), 1.0)\n",
    "        if opt.data_type == 16:\n",
    "            input_label = input_label.half()\n",
    "\n",
    "        # real images for training\n",
    "        if real_image is not None:\n",
    "            real_image = real_image.to(self.device)\n",
    "\n",
    "        return input_label, real_image\n",
    "\n",
    "    def forward(self, label, image, infer=False):\n",
    "        \n",
    "        \n",
    "        # Encode Inputs\n",
    "        input_label, real_image = self.encode_input(label, image)  \n",
    "\n",
    "        # Fake Generation\n",
    "        fake_image = self.netG.forward(input_label.float())\n",
    "        \n",
    "        ### Detection\n",
    "\n",
    "        #predict if the real images as False or True\n",
    "        pred_real = self.netD.forward(torch.cat((input_label, real_image.detach()), dim=1))\n",
    "        \n",
    "        #predict if the fake image as False or True\n",
    "        pred_fake_D = self.netD.forward(torch.cat((input_label, fake_image.detach()), dim=1))\n",
    "\n",
    "        # Get fake Generation for backprop in Generator\n",
    "        pred_fake_G = self.netD.forward(torch.cat((input_label, fake_image), dim=1))\n",
    "\n",
    "        ##################### Dicriminator Loss Based on Fake Images #####################\n",
    "\n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the discriminator for fake images\n",
    "        GAN_label_Discriminator_Fake = ... # Implement me\n",
    "        \n",
    "        loss_D_fake = GAN_Loss(pred_fake_D, GAN_label_Discriminator_Fake)  \n",
    "\n",
    "        ##################### Dicriminator Loss Based on Real Images #####################\n",
    "        \n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the discriminator for real images\n",
    "        GAN_label_Discriminator_Real =  ... # Implement me\n",
    "\n",
    "        loss_D_real = GAN_Loss(pred_real, GAN_label_Discriminator_Real)\n",
    "\n",
    "        ##################### Generator Loss #####################       \n",
    "        \n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the generator\n",
    "        GAN_label_Generator =  ... # Implement me\n",
    "\n",
    "        loss_G_GAN = GAN_Loss(pred_fake_G, GAN_label_Generator)  \n",
    "        \n",
    "        ##################### Feature Matching Loss ##################### \n",
    "        \n",
    "        # Try to find out by yourself what should be the inputs of the Feature Matching loss\n",
    "        loss_G_GAN_Feat = Feat_Match_loss(..., ...) # Implement me\n",
    "\n",
    "        ##################### VGG Loss ##################### \n",
    "        loss_G_VGG = 0\n",
    "\n",
    "        # Try to find out by yourself the inputs needed to calculate the VGG loss using the L1 function to ensure VGG classifies fake and real images equally\n",
    "        input1, input2 = ... , ...\n",
    "\n",
    "        loss_G_VGG = self.VGG_Loss(input1, input2) * opt.lambda_feat\n",
    "\n",
    "        # only return the fake_B image if necessary to save BW\n",
    "        losses_to_return=[[loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake], None if not infer else fake_image]\n",
    "\n",
    "        return losses_to_return\n",
    "\n",
    "    def inference(self, label):\n",
    "        # Encode Inputs\n",
    "        input_label, _ = self.encode_input(label) \n",
    "\n",
    "        # Fake Generation\n",
    "        fake_image = self.netG.forward(input_label)\n",
    "        return fake_image\n",
    "\n",
    "\n",
    "    def save(self, which_epoch):\n",
    "        self.save_network(self.netG, 'G', which_epoch, self.gpu_ids)\n",
    "        self.save_network(self.netD, 'D', which_epoch, self.gpu_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our model, we can train it and make people dance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = os.path.abspath('')\n",
    "\n",
    "def update_opt(opt, target_dir, run_name):\n",
    "    opt.dataroot = os.path.join(target_dir, 'train')\n",
    "    opt.name = run_name\n",
    "    opt.checkpoints_dir = os.path.join(dir_name, \"../../checkpoints\")\n",
    "\n",
    "    if os.path.isdir(os.path.join(opt.checkpoints_dir, run_name)):\n",
    "        print(\"Run already exists, will try to resume training\")\n",
    "        opt.load_pretrain = os.path.join(dir_name, \"../../checkpoints\", run_name)\n",
    "\n",
    "    if device == torch.device('cpu'):\n",
    "        opt.gpu_ids = []\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "    opt.temporal_smoothing = False\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "def train_pose2vid(target_dir, run_name):\n",
    "    import src.config.train_opt_notebook as opt\n",
    "\n",
    "    opt = update_opt(opt, target_dir, run_name)\n",
    "\n",
    "    os.makedirs(os.path.join(opt.checkpoints_dir, run_name), exist_ok=True)\n",
    "    iter_path = os.path.join(opt.checkpoints_dir, opt.name, 'iter.json')\n",
    "    data_loader = CreateDataLoader(opt)\n",
    "    dataset = data_loader.load_data()\n",
    "    dataset_size = len(data_loader)\n",
    "    print('#training images = %d' % dataset_size)\n",
    "\n",
    "    if opt.load_pretrain != '':\n",
    "        with open(iter_path, 'r') as f:\n",
    "            iter_json = json.load(f)\n",
    "    else:\n",
    "        iter_json = {'start_epoch': 1, 'epoch_iter': 0}\n",
    "\n",
    "    start_epoch = iter_json['start_epoch']\n",
    "    epoch_iter = iter_json['epoch_iter']\n",
    "    total_steps = (start_epoch - 1) * dataset_size + epoch_iter\n",
    "    display_delta = total_steps % opt.display_freq\n",
    "    print_delta = total_steps % opt.print_freq\n",
    "    save_delta = total_steps % opt.save_latest_freq\n",
    "\n",
    "    model = Gancing()\n",
    "    model.initialize(opt)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        if epoch != start_epoch:\n",
    "            epoch_iter = epoch_iter % dataset_size\n",
    "        for i, data in enumerate(dataset, start=epoch_iter):\n",
    "            iter_start_time = time.time()\n",
    "            total_steps += opt.batchSize\n",
    "            epoch_iter += opt.batchSize\n",
    "\n",
    "            ############## Forward Pass ######################\n",
    "\n",
    "            losses, generated = model(data['label'], data['image'], infer=True)\n",
    "\n",
    "            # sum per device losses\n",
    "            losses = [torch.mean(x) if not isinstance(x, int) else x for x in losses]\n",
    "            loss_dict = dict(zip(model.loss_names, losses))\n",
    "\n",
    "            # calculate final loss scalar\n",
    "            loss_D = (loss_dict['D_fake'] + loss_dict['D_real']) * 0.5\n",
    "            loss_G = loss_dict['G_GAN'] + loss_dict.get('G_GAN_Feat', 0) + loss_dict.get('G_VGG', 0)\n",
    "\n",
    "            ############### Backward Pass ####################\n",
    "            # update generator weights\n",
    "            model.optimizer_G.zero_grad()\n",
    "            loss_G.backward()\n",
    "            model.optimizer_G.step()\n",
    "\n",
    "            # update discriminator weights\n",
    "            model.optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            model.optimizer_D.step()\n",
    "\n",
    "\n",
    "            ############## Display results and errors ##########\n",
    "\n",
    "            print(f\"Epoch {epoch} batch {i}:\")\n",
    "            print(f\"loss_D: {loss_D}, loss_G: {loss_G}\")\n",
    "            print(f\"loss_D_fake: {loss_dict['D_fake']}, loss_D_real: {loss_dict['D_real']}\")\n",
    "            print(f\"loss_G_GAN {loss_dict['G_GAN']}, loss_G_GAN_Feat: {loss_dict.get('G_GAN_Feat', 0)}, loss_G_VGG: {loss_dict.get('G_VGG', 0)}\\n\")\n",
    "\n",
    "            ### print out errors\n",
    "            if total_steps % opt.print_freq == print_delta:\n",
    "                errors = {k: v.item() if not isinstance(v, int) else v for k, v in loss_dict.items()}\n",
    "                t = (time.time() - iter_start_time) / opt.batchSize\n",
    "\n",
    "\n",
    "            ### display output images\n",
    "            visuals = OrderedDict([('input_label', util.tensor2label(data['label'][0], opt.label_nc)),\n",
    "                                    ('synthesized_image', util.tensor2im(generated.data[0])),\n",
    "                                    ('real_image', util.tensor2im(data['image'][0]))])\n",
    "            fig, axes = init_figure()\n",
    "            plot_current_results(visuals, fig, axes)\n",
    "\n",
    "            ### save latest model\n",
    "            if total_steps % opt.save_latest_freq == save_delta:\n",
    "                print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n",
    "                model.save('latest')\n",
    "                iter_json['start_epoch'] = epoch\n",
    "                iter_json['epoch_iter'] = epoch_iter\n",
    "                with open(iter_path, 'w') as f:\n",
    "                    json.dump(iter_json, f)\n",
    "\n",
    "            if epoch_iter >= dataset_size:\n",
    "                break\n",
    "\n",
    "        # end of epoch\n",
    "        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n",
    "\n",
    "        ### save model for this epoch\n",
    "        if epoch % opt.save_epoch_freq == 0:\n",
    "            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_steps))\n",
    "            model.save('latest')\n",
    "            model.save(epoch)\n",
    "            iter_json['start_epoch'] = epoch + 1\n",
    "            iter_json['epoch_iter'] = 0\n",
    "            with open(iter_path, 'w') as f:\n",
    "                json.dump(iter_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../../data/targets/gianluca/'\n",
    "run_name = 'testnotebook'\n",
    "\n",
    "train_pose2vid(target_dir, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference of pretrained model\n",
    "\n",
    "We can quickly see that the training is working. The model slowly grasps the background of the picture, and then understands how to generate the missing figure. Let's see what happens later in the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../../data/targets/gianluca/'\n",
    "run_name = 'gianluca'\n",
    "\n",
    "train_pose2vid(target_dir, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Transfer\n",
    "\n",
    "For the moment we have been training our model to recreate a picture of a person based on the pose of this same person. But since the pose can be extracted from the picture of anyone, we can take a video of someone dancing, extract the pose of the dancer on each of the frames and ask the model to create a new frame based on this pose!\n",
    "\n",
    "Here is an example of what it could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4 = open('../../results/gianluca.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
