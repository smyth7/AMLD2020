{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**: This notebook provides a walk through the process of estimating the position of individuals on an image with a pretrained lightweight OpenPose model proposed in (https://arxiv.org/pdf/1811.12004.pdf). For a more detailed explanation of the OpenPose pipeline, please refer to the original paper (https://arxiv.org/pdf/1812.08008.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "!git clone https://github.com/VisiumCH/AMLD2020-Dirty-Gancing.git dancing\n",
    "    \n",
    "%cd dancing/notebooks/position_estimation\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
    "from modules.keypoints import extract_keypoints, group_keypoints\n",
    "from modules.load_state import load_state\n",
    "from modules.pose import Pose\n",
    "from matplotlib import cm\n",
    "from demo import extract_keypoints, group_keypoints, infer_fast, get_final_keypoint_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pose Estimation task aims to determine the position of the different body parts of individuals that can be found on an image.\n",
    "\n",
    "While most of the Multi-Person Pose Estimation approaches have used a top-down strategy, that first detects where the persons are located and then estimates the pose of them at each location, the OpenPose model proposes to solve the task with a bottom-up approach.\n",
    "\n",
    "A multi-stage CNN outputs confidence maps for body parts locations and vector fields that show the probable locations of body parts connections (limbs). These vector fields are called Part Affinity Fields (PAFs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/openpose_pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenPose pipeline first performs a feature extraction step by feeding the image into a Convolutional Neural Network (CNN), generating a set of feature maps **F**.\n",
    "\n",
    "The network produces a first-stage set of PAFs given **F**, and then performs $T_p - 1$ refinement iterations to obtain the final PAFs (for this workshop we use a lightweight model, in which $T_p$ was set to 2):\n",
    "\n",
    "$$\\begin{align}\n",
    "& \\bf{L}^1 = \\phi^1(\\bf{F}) \\\\ \n",
    "& \\bf{L}^t = \\phi^t(\\bf{F}, \\bf{L}^{t-1}), \\forall 2 \\leq t \\leq T_p\n",
    "\\end{align}$$\n",
    "\n",
    "The same process is then repeated to create the confidence maps:\n",
    "\n",
    "$$\\begin{align}\n",
    "& \\bf{S}^{1} = \\rho^{1}(\\bf{F}, \\bf{L}^{T_p}) \\\\ \n",
    "& \\bf{S}^t = \\rho^t(\\bf{F}, \\bf{L}^{T_p}, \\bf{S}^{t-1}), \\forall 2 \\leq t \\leq T_p\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/openpose-body-architecture-1024x291.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download and load the pretrained CNN that computes the PAFs and extracts the body parts confidence maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model'):\n",
    "    os.mkdir('model')\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://download.01.org/opencv/\" +\\\n",
    "    \"openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\",\n",
    "                    \"model/pretrained.pth\")\n",
    "\n",
    "net = PoseEstimationWithMobileNet()\n",
    "checkpoint = torch.load('model/pretrained.pth', map_location='cpu')\n",
    "load_state(net, checkpoint)\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this workshop, we will apply the OpenPose pipeline to the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = cv2.imread('images/joker.jpeg', cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=[5, 5], dpi=150)\n",
    "plt.title('Original image', {'fontsize': 8})\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `infer_fast` function takes as input the image and the pretrained model, and outputs the refined set of PAFs the refined set of body parts confidence maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 8\n",
    "upsample_ratio = 8\n",
    "num_keypoints = Pose.num_kpts\n",
    "orig_img = img.copy()\n",
    "orig_img_shape = img.shape[1], img.shape[0]\n",
    "\n",
    "heatmaps, pafs, scale, pad = infer_fast(net, input_img, 512, stride, upsample_ratio, cpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `heatmaps` object returned by the `infer_fast` function contains the heatmap of the location of each body part. The figure below shows which heatmap returned by the model corresponds to which body part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/keypoints.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = {\n",
    "    0: \"nose\", 1: \"neck\", 2: \"left_shoulder\", 3: \"left_elbow\", 4: \"left_wrist\", 5: \"right_shoulder\",\n",
    "    6: \"right_elbow\", 7: \"right_wrist\", 8: \"left_hip\", 9: \"left_knee\", 10: \"left_ankle\", 11: \"right_hip\",\n",
    "    12: \"right_knee\", 13: \"right_ankle\", 14: \"left_eye\", 15: \"right_eye\", 16: \"left_ear\", 17: \"right_ear\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Now plot the heatmaps of the different bodyparts on the same image. The resulting picture should look similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/heatmap_keypoints.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5], dpi=100)\n",
    "plt.title('Heatmap of all the different keypoints')\n",
    "plt.axis('off')\n",
    "all_keypoints_heatmap = ... # Implement me!\n",
    "plt.imshow(cv2.resize(all_keypoints_heatmap, orig_img_shape))\n",
    "plt.imshow(img, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PAFs are stored in the `pafs` object as vector fields. For each limb, the vectors are stored in two arrays: one for the x direction, and one for the y direction. We can thus draw the vectors on every pixels of the image to visualize the vector field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip=(slice(None,None,2),slice(None,None,2))\n",
    "\n",
    "plt.figure(figsize=[5, 5], dpi=150)\n",
    "plt.title('Vector field of the PAF (Part Affinity Fields) for the right arm', {'fontsize': 10})\n",
    "plt.axis('off')\n",
    "plt.quiver(pafs[160:220, 320:420, 14][skip], - pafs[160:220, 320:420, 15][skip], color='cyan',scale=30)\n",
    "plt.imshow(cv2.resize(img, (776,512))[160:220, 320:420], extent=(-0.5, 50-0.5, 30-0.5, -0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Now from this vector field that you see above, plot the heatmap corresponding to the magnitude of the vectors for the upper part of the right arm. The resulting image should look similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/heatmap_paf.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[5, 5], dpi=100)\n",
    "plt.title('Heatmap of the PAF (Part Affinity Fields) for the right arm', {\"fontsize\":10})\n",
    "plt.axis('off')\n",
    "upper_right_arm_pafs_norm = ... # Implement me!\n",
    "plt.imshow(cv2.resize(upper_right_arm_pafs_norm, orig_img_shape))\n",
    "plt.imshow(img, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `extract_keypoints` computes the most likely positions for the different keypoints according to the body part heatmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keypoints_num = 0\n",
    "all_keypoints_by_type = []\n",
    "for kpt_idx in range(num_keypoints):  # 19th for bg\n",
    "    total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keypoints positions and their confidence scores are stored in the `all_keypoints_by_type` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of keypoints per type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Now, list the number of keypoints per body part that have been detected. The output should be as follow:\n",
    "\n",
    "```\n",
    "['1 keypoints extracted for nose',\n",
    " '1 keypoints extracted for neck',\n",
    " '1 keypoints extracted for left_shoulder',\n",
    " '1 keypoints extracted for left_elbow',\n",
    " '1 keypoints extracted for left_wrist',\n",
    " '1 keypoints extracted for right_shoulder',\n",
    " '1 keypoints extracted for right_elbow',\n",
    " '1 keypoints extracted for right_wrist',\n",
    " '1 keypoints extracted for left_hip',\n",
    " '1 keypoints extracted for left_knee',\n",
    " '3 keypoints extracted for left_ankle',\n",
    " '1 keypoints extracted for right_hip',\n",
    " '1 keypoints extracted for right_knee',\n",
    " '3 keypoints extracted for right_ankle',\n",
    " '1 keypoints extracted for left_eye',\n",
    " '1 keypoints extracted for right_eye',\n",
    " '0 keypoints extracted for left_ear',\n",
    " '1 keypoints extracted for right_ear']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that too many points are detected for the ankles. Namely, three instead of 1 per type.\n",
    "\n",
    "**TODO**: Let's now see how it looks like on the original picture. You can draw a circle for every right ankle keypoints detected. The resulting picture should look similar to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ankle_keypoints.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_ankle_keypoints = cv2.resize(img.copy(), (776,512))\n",
    "\n",
    "for ankle_keypoint in all_keypoints_by_type[10]:\n",
    "    img_with_ankle_keypoints = ... # Implement me!\n",
    "\n",
    "plt.figure(figsize=[5, 5], dpi=100)\n",
    "plt.title('Ankles keypoints detected', {\"fontsize\": 10})\n",
    "plt.axis('off')\n",
    "plt.imshow(img_with_ankle_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To associate the body parts between them and to filter the potential false positives, it is common to compute a score for each candidate limb from the PAFs. \n",
    "Then, a K-dimensional matching problem is solved by a greedy relaxation method (More details in the OpenPose paper (https://arxiv.org/pdf/1812.08008.pdf)).\n",
    "\n",
    "The function `get_final_keypoint_positions` will perform these steps and store the final result in a `Pose` object which can be drawn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pose = get_final_keypoint_positions(all_keypoints_by_type, pafs,\n",
    "                                             num_keypoints, stride, scale, upsample_ratio, pad)\n",
    "colormap = cm.tab10.colors\n",
    "\n",
    "img_final = img.copy()\n",
    "\n",
    "all_pose[0].draw(img_final, (0, 255, 255))\n",
    "\n",
    "plt.figure(figsize=[5, 5], dpi=150)\n",
    "plt.title('Resulting image with pose skeleton', {\"fontsize\": 7})\n",
    "plt.axis('off')\n",
    "plt.imshow(img_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple persons pose estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline on a picture containing multiple persons. Here is a beautiful picture if you are looking for inspiration ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bonus_img = cv2.imread('images/multi_persons.jpg', cv2.IMREAD_COLOR)\n",
    "bonus_img = cv2.cvtColor(input_bonus_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=[2,5], dpi=750)\n",
    "plt.title('Beautiful image', fontdict = {'fontsize':2})\n",
    "plt.axis('off')\n",
    "plt.imshow(bonus_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 8\n",
    "upsample_ratio = 8\n",
    "num_keypoints = Pose.num_kpts\n",
    "orig_bonus_img = bonus_img.copy()\n",
    "orig_bonus_img_shape = bonus_img.shape[1], bonus_img.shape[0]\n",
    "\n",
    "heatmaps, pafs, scale, pad = infer_fast(net, input_bonus_img, 512, stride, upsample_ratio, cpu=True)\n",
    "\n",
    "total_keypoints_num = 0\n",
    "all_keypoints_by_type = []\n",
    "for kpt_idx in range(num_keypoints):  # 19th for bg\n",
    "    total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Code the rest of the pipeline to show the final result of the multi-pose detection. The final image should look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/multi_final_image.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement me!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
