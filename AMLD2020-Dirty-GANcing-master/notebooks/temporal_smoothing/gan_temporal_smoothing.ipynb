{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Smoothing GAN\n",
    "\n",
    "In this notebook, we will implement the same model as before, with the temporal smoothing component in addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "!pip install googledrivedownloader\n",
    "\n",
    "!git clone https://github.com/VisiumCH/AMLD2020-Dirty-Gancing.git dancing\n",
    "\n",
    "!rm -rf dancing/data/targets/gianluca\n",
    "\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1NnV2OCYV3EchLSDdMTfI3KwGZkwENqvv',\n",
    "                                    dest_path='./gianluca.zip',\n",
    "                                    unzip=True)\n",
    "\n",
    "!mv gianluca_1000 dancing/data/targets/gianluca\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1KvLXCvse8hUATR8ZbZgYGwHVQOCcqMdS',\n",
    "                                    dest_path='./gianluca_pretrained_ts.zip',\n",
    "                                    unzip=True)\n",
    "\n",
    "!mv gianluca_pretrained_ts dancing/checkpoints/gianluca_ts\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='19jq_fuGfPjbEI67dpo01e5ka2JJW8uTO',\n",
    "                                    dest_path='./gianluca_ts.mp4',\n",
    "                                    unzip=False)\n",
    "\n",
    "!mv gianluca_ts.mp4 dancing/results\n",
    "\n",
    "!rm -rf dancing/checkpoints/testnotebook_ts\n",
    "\n",
    "%cd dancing/notebooks/temporal_smoothing\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import OrderedDict\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "sys.path.append('../..')\n",
    "sys.path.append('../../src/pix2pixHD/')\n",
    "\n",
    "import src.config.train_opt_notebook as opt\n",
    "import src.pix2pixHD.util.util as util\n",
    "\n",
    "from src.utils.torch_utils import get_torch_device\n",
    "from src.utils.plt_utils import init_figure, plot_current_results\n",
    "from src.pix2pixHD.data.data_loader import CreateDataLoader\n",
    "from src.pix2pixHD.models import networks\n",
    "from src.pix2pixHD.models.base_model import BaseModel\n",
    "from src.pix2pixHD.models.models import create_model\n",
    "from src.pix2pixHD.util.image_pool import ImagePool\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = get_torch_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make people Dance (a bit better)\n",
    "\n",
    "As you may have noticed in the previous notebook, there are a few problems with the model that was implemented. For example:\n",
    "\n",
    "- The background is flickering from time to time: This occurs because of the objects in the pictures. Our subject could not provide training images on top of these objects, so the model does not know what to do when we ask him to draw a person at this position.\n",
    "- Missed detections of the Pose Estimation algorithm causes missing limbs in the generated frames: This can be improved if we use a better (but slower) Pose Estimation model\n",
    "- The face of the generated subject doesn't look very natural: This is addressed by [Chan et. al.](https://arxiv.org/pdf/1808.07371.pdf) with an additional Face Generator. It is included in this repo for those who are curious to try.\n",
    "- There are artifacts on the clothes and alack of temporal coherence: This is the problem we are going to address now!\n",
    "\n",
    "The lack of temporal coherence occurs mostly because all the frames are generated independently. The model has no notion of temporality. It creates frames, not videos.\n",
    "The model of [Chan et. al.](https://arxiv.org/pdf/1808.07371.pdf) includes a temporal smoothing component, which we will now detail.\n",
    "\n",
    "![GAN Architecture](imgs/gan_architecture.png)\n",
    "\n",
    "The core of the model is the same as before. The main difference is that we want the **Generator** to create a frame that is **temporally** coherent with the previous frame. To do so, we have to change a bit the inputs of the two networks:\n",
    "\n",
    "- The **Generator** will receive as input the **target pose** and the **previously generated frame**. Moreover, we will always generate **two** frames so as to create a sequence!\n",
    "- The **Discriminator** will receive a sequence of **two images** and **two poses**\n",
    "\n",
    "In order to force the discriminator to learn to use the temporal aspect, we have to concatenate the images and the poses along the width or height, but not the channels!\n",
    "\n",
    "You feel like you are ready? First let's define our loss functions again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_Loss(x,\n",
    "            target_is_real):\n",
    "    \"\"\" Adversarial loss function for LS-GAN\n",
    "    \n",
    "    Args:\n",
    "        x (iterable of iterables of torch.Tensors): discriminator activations\n",
    "        target_is_real (bool): whether x should be treated as fake or real samples\n",
    "        use_ls_gan (bool): if True, use the LS-GAN loss, else, use the vanilla GAN loss\n",
    "        gpu_ids (list): list of available GPU ids\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: adversarial loss value for x\n",
    "    \"\"\"\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    target_value = 1.0 if target_is_real else 0.0\n",
    "\n",
    "    loss = 0\n",
    "    for activations in x:\n",
    "        pred = activations[-1].to(device)\n",
    "        target_tensor = torch.FloatTensor(pred.size()).requires_grad_(False) \\\n",
    "                             .fill_(target_value).to(device)\n",
    "        loss += criterion(pred, target_tensor)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feat_Match_loss(pred_fake,\n",
    "                    pred_real,\n",
    "                    opt):\n",
    "    \"\"\" Feature Matching loss for Pix2Pix\n",
    "    \n",
    "    Args:\n",
    "        pred_fake (iterable of iterables of torch.Tensors): discriminator activations for fake samples\n",
    "        pred_real (iterable of iterables of torch.Tensors): discriminator activations for real samples\n",
    "        n_layers_D (int): number of layers in discriminator\n",
    "        num_D (int): number of discriminators (multi-scale)\n",
    "        lambda_feat (float): scaling factor\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: feature matching loss value for pred_fake and pred_real\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    D_weights = 1 / opt.num_D\n",
    "    feat_weights = 4 / (opt.n_layers_D + 1)\n",
    "    factor = D_weights * feat_weights * opt.lambda_feat\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(opt.num_D):\n",
    "        for j in range(len(pred_fake[i]) - 1):\n",
    "            loss += factor * criterion(pred_fake[i][j], pred_real[i][j].detach())\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self, gpu_ids):\n",
    "        super(VGGLoss, self).__init__()\n",
    "        if len(gpu_ids) > 0:\n",
    "            self.vgg = networks.Vgg19().cuda()\n",
    "        else:\n",
    "            self.vgg = networks.Vgg19()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\" Perceptual loss for Pix2Pix\n",
    "        Args:\n",
    "            x, y (torch.Tensor): samples to compare\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: perceptual loss value for x, y\n",
    "        \"\"\" \n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the **forward pass** of the model with the temporal smoothing.\n",
    "\n",
    "Keep in mind that:\n",
    "\n",
    "- The Generator needs to create a sequence of two images (hint: For the very first generation, you can give a blank image with torch.zeros(...))\n",
    "- The Discriminator need to receive two images and two poses (hint: Try to first concatenate the two labels, then the two poses, and then the two resulting tensors together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothGancing(BaseModel):\n",
    "    \n",
    "    def initialize(self, opt):\n",
    "        BaseModel.initialize(self, opt)\n",
    "        if opt.resize_or_crop != 'none' or not opt.isTrain: # when training at full res this causes OOM\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.isTrain = opt.isTrain\n",
    "\n",
    "        ######check for gpu\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        if len(self.gpu_ids) > 0:\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')        \n",
    "        input_nc = opt.label_nc if opt.label_nc != 0 else opt.input_nc\n",
    "\n",
    "        # Generator network\n",
    "        netG_input_nc = input_nc + 3       \n",
    "        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n",
    "                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n",
    "                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)        \n",
    "\n",
    "        # Discriminator network\n",
    "        if self.isTrain:\n",
    "            use_sigmoid = opt.no_lsgan\n",
    "            netD_input_nc = input_nc + opt.output_nc\n",
    "            self.netD = networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n",
    "                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=self.gpu_ids)\n",
    "\n",
    "\n",
    "        # load networks\n",
    "        if not self.isTrain or opt.continue_train or opt.load_pretrain:\n",
    "            pretrained_path = '' if not self.isTrain else opt.load_pretrain\n",
    "            self.load_network(self.netG, 'G', opt.which_epoch, pretrained_path)            \n",
    "            if self.isTrain:\n",
    "                self.load_network(self.netD, 'D', opt.which_epoch, pretrained_path)         \n",
    "\n",
    "        # set loss functions and optimizers\n",
    "        if self.isTrain:            \n",
    "            \n",
    "            # Names so we can breakout loss\n",
    "            self.loss_names = ['G_GAN','G_GAN_Feat','G_VGG','D_real', 'D_fake']\n",
    "            \n",
    "            # Define VGGloss so we don't load VGG at each batch\n",
    "            self.VGG_Loss = VGGLoss(opt.gpu_ids)\n",
    "            \n",
    "            # initialize optimizers\n",
    "            params = list(self.netG.parameters())\n",
    "            self.optimizer_G = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))                            \n",
    "\n",
    "            # optimizer D                        \n",
    "            params = list(self.netD.parameters())    \n",
    "            self.optimizer_D = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            \n",
    "    \n",
    "    def encode_input(self, label_map, real_image): \n",
    "    \n",
    "        # create one-hot vector for the pose\n",
    "        size = label_map.size()\n",
    "        oneHot_size = (size[0], opt.label_nc, size[2], size[3])\n",
    "        input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_().to(self.device)\n",
    "        input_label = input_label.scatter_(1, label_map.data.long().to(self.device), 1.0).to(self.device)\n",
    "        if opt.data_type == 16:\n",
    "            input_label = input_label.half()\n",
    "\n",
    "        # real images for training\n",
    "        if real_image is not None:\n",
    "            real_image = real_image.to(self.device)\n",
    "\n",
    "        return input_label, real_image\n",
    "\n",
    "    def forward(self, label, image, previous_label=None, previous_image=None, infer=False):\n",
    "        \n",
    "        \n",
    "        # Encode Inputs\n",
    "        input_label, real_image = self.encode_input(label, image)  \n",
    "        \n",
    "        if previous_label is not None and previous_image is not None:\n",
    "            previous_input_label, previous_real_image = self.encode_input(label, image)\n",
    "\n",
    "        # Fake Generation\n",
    "        previous_fake_image = ... # Implement me \n",
    "        \n",
    "        fake_image = ... # Implement me\n",
    "\n",
    "        ### Detection\n",
    "\n",
    "        #predict if the real images as False or True\n",
    "        real_inputD = ... # Implement me\n",
    "        \n",
    "        pred_real = self.netD.forward(real_inputD)\n",
    "        \n",
    "        #predict if the fake image as False or True\n",
    "        fake_inputD_D = ... # Implement me\n",
    "        \n",
    "        pred_fake_D = self.netD.forward(fake_inputD_D)\n",
    "        \n",
    "        #Getfake Generation for backprop in Generator\n",
    "        fake_inputD_G = ... # Implement me\n",
    "        \n",
    "        pred_fake_G = self.netD.forward(fake_inputD_G)\n",
    "\n",
    "        ##################### Dicriminator Loss Based on Fake Images #####################\n",
    "\n",
    "        GAN_label_Discriminator_Fake = False\n",
    "        \n",
    "        loss_D_fake = GAN_Loss(pred_fake_D, GAN_label_Discriminator_Fake)  \n",
    "\n",
    "        ##################### Dicriminator Loss Based on Real Images #####################\n",
    "        \n",
    "        GAN_label_Discriminator_Real =  True\n",
    "\n",
    "        loss_D_real = GAN_Loss(pred_real, GAN_label_Discriminator_Real)\n",
    "\n",
    "        ##################### Generator Loss #####################       \n",
    "        \n",
    "        GAN_label_Generator=  True\n",
    "\n",
    "        loss_G_GAN = GAN_Loss(pred_fake_G, GAN_label_Generator)  \n",
    "                   \n",
    "        ##################### Feature Matching Loss ##################### \n",
    "        \n",
    "        loss_G_GAN_Feat = Feat_Match_loss(pred_fake_G, pred_real, opt)\n",
    "\n",
    "        ##################### VGG Loss ##################### \n",
    "        loss_G_VGG = 0\n",
    "\n",
    "        # Try to find out by yourself the inputs needed to calculate the VGG loss using the L1 function to ensure VGG classifies fake and real images equally\n",
    "        input1, input2 = ... , ...\n",
    "        input3, input4 = ... , ...\n",
    "\n",
    "\n",
    "        loss_G_VGG = (self.VGG_Loss(input1, input2) + self.VGG_Loss(input3, input4)) * opt.lambda_feat\n",
    "\n",
    "\n",
    "        # only return the fake_B image if necessary to save BW\n",
    "        losses_to_return=[[loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake], None if not infer else fake_image]\n",
    "\n",
    "\n",
    "        return losses_to_return\n",
    "\n",
    "    def inference(self, label, previous_image):\n",
    "        # Encode Inputs\n",
    "        input_label, previous_image_input = self.encode_input(label, previous_image) \n",
    "\n",
    "        # Fake Generation\n",
    "        fake_image = self.netG.forward(torch.cat((input_label, previous_image_input)), dim=1)\n",
    "        return fake_image\n",
    "\n",
    "\n",
    "    def save(self, which_epoch):\n",
    "        self.save_network(self.netG, 'G', which_epoch, self.gpu_ids)\n",
    "        self.save_network(self.netD, 'D', which_epoch, self.gpu_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = os.path.abspath('')\n",
    "\n",
    "def update_opt(opt, target_dir, run_name):\n",
    "    opt.dataroot = os.path.join(target_dir, 'train')\n",
    "    opt.checkpoints_dir = os.path.join(dir_name, \"../../checkpoints\")\n",
    "    opt.name = run_name\n",
    "    if os.path.isdir(os.path.join(opt.checkpoints_dir, run_name)):\n",
    "        print(\"Run already exists, will try to resume training\")\n",
    "        opt.load_pretrain = os.path.join(dir_name, \"../../checkpoints\", run_name)\n",
    "\n",
    "    if device == torch.device('cpu'):\n",
    "        opt.gpu_ids = []\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "    opt.temporal_smoothing = True\n",
    "\n",
    "    return opt\n",
    "    \n",
    "def train_pose2vid_temporal_smoothing(target_dir, run_name):\n",
    "    import src.config.train_opt_notebook as opt\n",
    "\n",
    "    opt = update_opt(opt, target_dir, run_name)\n",
    "    \n",
    "    os.makedirs(os.path.join(opt.checkpoints_dir, run_name), exist_ok=True)\n",
    "    iter_path = os.path.join(opt.checkpoints_dir, run_name, 'iter.json')\n",
    "    data_loader = CreateDataLoader(opt)\n",
    "    dataset = data_loader.load_data()\n",
    "    dataset_size = len(data_loader)\n",
    "    print('#training images = %d' % dataset_size)\n",
    "\n",
    "    if opt.load_pretrain != '':\n",
    "        with open(iter_path, 'r') as f:\n",
    "            iter_json = json.load(f)\n",
    "    else:\n",
    "        iter_json = {'start_epoch': 1, 'epoch_iter': 0}\n",
    "\n",
    "    start_epoch = iter_json['start_epoch']\n",
    "    epoch_iter = iter_json['epoch_iter']\n",
    "    total_steps = (start_epoch - 1) * dataset_size + epoch_iter\n",
    "    display_delta = total_steps % opt.display_freq\n",
    "    print_delta = total_steps % opt.print_freq\n",
    "    save_delta = total_steps % opt.save_latest_freq\n",
    "\n",
    "    model = SmoothGancing()\n",
    "    model.initialize(opt)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        if epoch != start_epoch:\n",
    "            epoch_iter = epoch_iter % dataset_size\n",
    "        for i, data in enumerate(dataset, start=epoch_iter):\n",
    "            iter_start_time = time.time()\n",
    "            total_steps += opt.batchSize\n",
    "            epoch_iter += opt.batchSize\n",
    "\n",
    "            ############## Forward Pass ######################\n",
    "            losses, generated = model(data['label'], data['image'],\n",
    "                                      data['previous_label'], data['previous_image'], infer=True)\n",
    "\n",
    "\n",
    "            # sum per device losses\n",
    "            losses = [torch.mean(x) if not isinstance(x, int) else x for x in losses]\n",
    "            loss_dict = dict(zip(model.loss_names, losses))\n",
    "\n",
    "            # calculate final loss scalar\n",
    "            loss_D = (loss_dict['D_fake'] + loss_dict['D_real']) * 0.5\n",
    "            loss_G = loss_dict['G_GAN'] + loss_dict.get('G_GAN_Feat', 0) + loss_dict.get('G_VGG', 0)\n",
    "\n",
    "            ############### Backward Pass ####################\n",
    "            # update generator weights\n",
    "            model.optimizer_G.zero_grad()\n",
    "            loss_G.backward()\n",
    "            model.optimizer_G.step()\n",
    "\n",
    "            # update discriminator weights\n",
    "            model.optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            model.optimizer_D.step()\n",
    "\n",
    "\n",
    "            ############## Display results and errors ##########\n",
    "\n",
    "            print(f\"Epoch {epoch} batch {i}:\")\n",
    "            print(f\"loss_D: {loss_D}, loss_G: {loss_G}\")\n",
    "            print(f\"loss_D_fake: {loss_dict['D_fake']}, loss_D_real: {loss_dict['D_real']}\")\n",
    "            print(f\"loss_G_GAN {loss_dict['G_GAN']}, loss_G_GAN_Feat: {loss_dict.get('G_GAN_Feat', 0)}, loss_G_VGG: {loss_dict.get('G_VGG', 0)}\\n\")\n",
    "\n",
    "            ### print out errors\n",
    "            if total_steps % opt.print_freq == print_delta:\n",
    "                errors = {k: v.item() if not isinstance(v, int) else v for k, v in loss_dict.items()}\n",
    "                t = (time.time() - iter_start_time) / opt.batchSize\n",
    "\n",
    "\n",
    "            ### display output images\n",
    "            visuals = OrderedDict([('input_label', util.tensor2label(data['label'][0], opt.label_nc)),\n",
    "                                    ('synthesized_image', util.tensor2im(generated.data[0])),\n",
    "                                    ('real_image', util.tensor2im(data['image'][0]))])\n",
    "            fig, axes = init_figure()\n",
    "            plot_current_results(visuals, fig, axes)\n",
    "\n",
    "            ### save latest model\n",
    "            if total_steps % opt.save_latest_freq == save_delta:\n",
    "                print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n",
    "                model.save('latest')\n",
    "                iter_json['start_epoch'] = epoch\n",
    "                iter_json['epoch_iter'] = epoch_iter\n",
    "                with open(iter_path, 'w') as f:\n",
    "                    json.dump(iter_json, f)\n",
    "\n",
    "            if epoch_iter >= dataset_size:\n",
    "                break\n",
    "\n",
    "        # end of epoch\n",
    "        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n",
    "\n",
    "        ### save model for this epoch\n",
    "        if epoch % opt.save_epoch_freq == 0:\n",
    "            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_steps))\n",
    "            model.save('latest')\n",
    "            model.save(epoch)\n",
    "            iter_json['start_epoch'] = epoch + 1\n",
    "            iter_json['epoch_iter'] = 0\n",
    "            with open(iter_path, 'w') as f:\n",
    "                json.dump(iter_json, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../../data/targets/gianluca/'\n",
    "run_name = 'testnotebook_ts'\n",
    "\n",
    "train_pose2vid_temporal_smoothing(target_dir, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../../data/targets/gianluca/'\n",
    "run_name = 'gianluca_ts'\n",
    "\n",
    "train_pose2vid_temporal_smoothing(target_dir, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4 = open('../../results/gianluca_ts.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
