{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AMLD_WORKSHOP_TRANSFER_NLP.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GAnjZSHn8D1_"},"source":["# Install the necessary packages"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting pytorch_pretrained_bert\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n\u001b[K     |████████████████████████████████| 133kB 2.2MB/s \n\u001b[?25hCollecting pytorch-nlp\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n\u001b[K     |████████████████████████████████| 92kB 5.5MB/s \n\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/envs/amld-pytorch/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.15.3)\nCollecting tqdm\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/2e/4307206db63f05ed37e21d4c0d843d0fbcacd62479f8ce99ba0f2c0875e0/tqdm-4.42.0-py2.py3-none-any.whl (59kB)\n\u001b[K     |████████████████████████████████| 61kB 18.9MB/s \n\u001b[?25hCollecting regex\n  Using cached https://files.pythonhosted.org/packages/73/d9/b58289d885180b5d538aa6df07974b5fe6088547ac846c0f76f77259c304/regex-2020.1.8.tar.gz\nCollecting requests\n  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\nCollecting boto3\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/57/e9675a5a8d0ee586594ff19cb9a601334fbf24fa2fb29052d2a900ee5d23/boto3-1.11.9-py2.py3-none-any.whl (128kB)\n\u001b[K     |████████████████████████████████| 133kB 4.0MB/s \n\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /opt/anaconda3/envs/amld-pytorch/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.1.0)\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n\u001b[K     |████████████████████████████████| 133kB 5.3MB/s \n\u001b[?25hCollecting idna<2.9,>=2.5\n  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\nRequirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/amld-pytorch/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\nCollecting chardet<3.1.0,>=3.0.2\n  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\nCollecting s3transfer<0.4.0,>=0.3.0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/48/a8252b6b3cd31774eab312b19d58a6ac55f296240c206617dcd38cd93bf8/s3transfer-0.3.2-py2.py3-none-any.whl (69kB)\n\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\nCollecting botocore<1.15.0,>=1.14.9\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/4c/b0b0d3b6f84a05f9135051b56d3eb8708012a289c4b82ee21c8c766f47b5/botocore-1.14.9-py2.py3-none-any.whl (5.9MB)\n\u001b[K     |████████████████████████████████| 5.9MB 4.2MB/s \n\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/envs/amld-pytorch/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.9->boto3->pytorch_pretrained_bert) (2.8.1)\nCollecting docutils<0.16,>=0.10\n  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\nRequirement already satisfied: six>=1.5 in /opt/anaconda3/envs/amld-pytorch/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.9->boto3->pytorch_pretrained_bert) (1.13.0)\nBuilding wheels for collected packages: regex\n  Building wheel for regex (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for regex: filename=regex-2020.1.8-cp36-cp36m-macosx_10_7_x86_64.whl size=295313 sha256=af7a56eda1e4e3f4f7a5dbcda4782215fa53f200d8bdb720934613d5fca63d13\n  Stored in directory: /Users/paul/Library/Caches/pip/wheels/1c/78/87/21be0303007ee5d1483df56703c9c7e5a44873e8f0c51d65f8\nSuccessfully built regex\nInstalling collected packages: tqdm, regex, urllib3, idna, chardet, requests, jmespath, docutils, botocore, s3transfer, boto3, pytorch-pretrained-bert, pytorch-nlp\nSuccessfully installed boto3-1.11.9 botocore-1.14.9 chardet-3.0.4 docutils-0.15.2 idna-2.8 jmespath-0.9.4 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2 regex-2020.1.8 requests-2.22.0 s3transfer-0.3.2 tqdm-4.42.0 urllib3-1.25.8\nCollecting tokenizers\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/99/9c097638488f1da35c90ce483eec7c1c8beb264b42be2981c56d09b274d3/tokenizers-0.2.1-cp36-cp36m-macosx_10_13_x86_64.whl (1.1MB)\n\u001b[K     |████████████████████████████████| 1.1MB 2.2MB/s \n\u001b[?25hInstalling collected packages: tokenizers\nSuccessfully installed tokenizers-0.2.1\n"}],"source":["!pip install pytorch_pretrained_bert pytorch-nlp\n","!pip install tokenizers"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"ram\nsoloists\n411\nnoses\n417\ncoping\nfission\nhardin\ninca\n##cen\n1717\nmobilized\nvhf\n##raf\nbiscuits\ncurate\n##85\n##anial\n331\ngaunt\nneighbourhoods\n1540\n##abas\nblanca\nbypassed\nsockets\nbehold\ncoincidentally\n##bane\nnara\nshave\nsplinter\nterrific\n##arion\n##erian\ncommonplace\njuris\nredwood\nwaistband\nboxed\ncaitlin\nfingerprints\njennie\nnaturalized\n##ired\nbalfour\ncraters\njody\nbungalow\nhugely\nquilt\nglitter\npigeons\nundertaker\nbulging\nconstrained\ngoo\n##sil\n##akh\nassimilation\nreworked\n##person\npersuasion\n##pants\nfelicia\n##cliff\n##ulent\n1732\nexplodes\n##dun\n##inium\n##zic\nlyman\nvulture\nhog\noverlook\nbegs\nnorthwards\now\nspoil\n##urer\nfatima\nfavorably\naccumulate\nsargent\nsorority\ncorresponded\ndispersal\nkochi\ntoned\n##imi\n##lita\ninternacional\nnewfound\n##agger\n##lynn\n##rigue\nbooths\npeanuts\n##eborg\nmedicare\nmuriel\nnur\n##uram\ncrates\nmillennia\npajamas\nworsened\n##breakers\njimi\nvanuatu\nyawned\n##udeau\ncarousel\n##hony\nhurdle\n##ccus\n##mounted\n##pod\nrv\n##eche\nairship\nambiguity\ncompulsion\nrecapture\n##claiming\narthritis\n##osomal\n1667\nasserting\nngc\nsniffing\ndade\ndiscontent\nglendale\nported\n##amina\ndefamation\nrammed\n##scent\nfling\nlivingstone\n##fleet\n875\n##ppy\napocalyptic\ncomrade\nlcd\n##lowe\ncessna\neine\npersecuted\nsubsistence\ndemi\nhoop\nreliefs\n710\ncoptic\nprogressing\nstemmed\nperpetrators\n1665\npriestess\n##nio\ndobson\nebony\nrooster\nitf\ntortricidae\n##bbon\n##jian\ncleanup\n##jean\n##øy\n1721\neighties\ntaxonomic\nholiness\n##hearted\n##spar\nantilles\nshowcasing\nstabilized\n##nb\ngia\nmascara\nmichelangelo\ndawned\n##uria\n##vinsky\nextinguished\nfitz\ngrotesque\n£100\n##fera\n##loid\n##mous\nbarges\nneue\nthrobbed\ncipher\njohnnie\n##a1\n##mpt\noutburst\n##swick\nspearheaded\nadministrations\nc1\nheartbreak\npixels\npleasantly\n##enay\nlombardy\nplush\n##nsed\nbobbie\n##hly\nreapers\ntremor\nxiang\nminogue\nsubstantive\nhitch\nbarak\n##wyl\nkwan\n##encia\n910\nobscene\nelegance\nindus\nsurfer\nbribery\nconserve\n##hyllum\n##masters\nhoratio\n##fat\napes\nrebound\npsychotic\n##pour\niteration\n##mium\n##vani\nbotanic\nhorribly\nantiques\ndispose\npaxton\n##hli\n##wg\ntimeless\n1704\ndisregard\nengraver\nhounds\n##bau\n##version\nlooted\nuno\nfacilitates\ngroans\nmasjid\nrutland\nantibody\ndisqualification\ndecatur\nfootballers\nquake\nslacks\n48th\nrein\nscribe\nstabilize\ncommits\nexemplary\ntho\n##hort\n##chison\npantry\ntraversed\n##hiti\ndisrepair\nidentifiable\nvibrated\nbaccalaureate\n##nnis\ncsa\ninterviewing\n##iensis\n##raße\ngreaves\nwealthiest\n343\nclassed\njogged\n£5\n##58\n##atal\nilluminating\nknicks\nrespecting\n##uno\nscrubbed\n##iji\n##dles\nkruger\nmoods\ngrowls\nraider\nsilvia\nchefs\nkam\nvr\ncree\npercival\n##terol\ngunter\ncounterattack\ndefiant\nhenan\nze\n##rasia\n##riety\nequivalence\nsubmissions\n##fra\n##thor\nbautista\nmechanically\n##heater\ncornice\nherbal\ntemplar\n##mering\noutputs\nruining\nligand\nrenumbered\nextravagant\nmika\nblockbuster\neta\ninsurrection\n##ilia\ndarkening\nferocious\npianos\nstrife\nkinship\n##aer\nmelee\n##anor\n##iste\n##may\n##oue\ndecidedly\nweep\n##jad\n##missive\n##ppel\n354\npuget\nunease\n##gnant\n1629\nhammering\nkassel\nob\nwessex\n##lga\nbromwich\negan\nparanoia\nutilization\n##atable\n##idad\ncontradictory\nprovoke\n##ols\n##ouring\n##tangled\nknesset\n##very\n##lette\nplumbing\n##sden\n##¹\ngreensboro\noccult\nsniff\n338\nzev\nbeaming\ngamer\nhaggard\nmahal\n##olt\n##pins\nmendes\nutmost\nbriefing\ngunnery\n##gut\n##pher\n##zh\n##rok\n1679\nkhalifa\nsonya\n##boot\nprincipals\nurbana\nwiring\n##liffe\n##minating\n##rrado\ndahl\nnyu\nskepticism\nnp\ntownspeople\nithaca\nlobster\nsomethin\n##fur\n##arina\n##−1\nfreighter\nzimmerman\nbiceps\ncontractual\n##herton\namend\nhurrying\nsubconscious\n##anal\n336\nmeng\nclermont\nspawning\n##eia\n##lub\ndignitaries\nimpetus\nsnacks\nspotting\ntwigs\n##bilis\n##cz\n##ouk\nlibertadores\nnic\nskylar\n##aina\n##firm\ngustave\nasean\n##anum\ndieter\nlegislatures\nflirt\nbromley\ntrolls\numar\n##bbies\n##tyle\nblah\nparc\nbridgeport\ncrank\nnegligence\n##nction\n46th\nconstantin\nmolded\nbandages\nseriousness\n00pm\nsiegel\ncarpets\ncompartments\nupbeat\nstatehood\n##dner\n##edging\nmarko\n730\nplatt\n##hane\npaving\n##iy\n1738\nabbess\nimpatience\nlimousine\nnbl\n##talk\n441\nlucille\nmojo\nnightfall\nrobbers\n##nais\nkarel\nbrisk\ncalves\nreplicate\nascribed\ntelescopes\n##olf\nintimidated\n##reen\nballast\nspecialization\n##sit\naerodynamic\ncaliphate\nrainer\nvisionary\n##arded\nepsilon\n##aday\n##onte\naggregation\nauditory\nboosted\nreunification\nkathmandu\nloco\nrobyn\n402\nacknowledges\nappointing\nhumanoid\nnewell\nredeveloped\nrestraints\n##tained\nbarbarians\nchopper\n1609\nitaliana\n##lez\n##lho\ninvestigates\nwrestlemania\n##anies\n##bib\n690\n##falls\ncreaked\ndragoons\ngravely\nminions\nstupidity\nvolley\n##harat\n##week\nmusik\n##eries\n##uously\nfungal\nmassimo\nsemantics\nmalvern\n##ahl\n##pee\ndiscourage\nembryo\nimperialism\n1910s\nprofoundly\n##ddled\njiangsu\nsparkled\nstat\n##holz\nsweatshirt\ntobin\n##iction\nsneered\n##cheon\n##oit\nbrit\ncausal\nsmyth\n##neuve\ndiffuse\nperrin\nsilvio\n##ipes\n##recht\ndetonated\niqbal\nselma\n##nism\n##zumi\nroasted\n##riders\ntay\n##ados\n##mament\n##mut\n##rud\n840\ncompletes\nnipples\ncfa\nflavour\nhirsch\n##laus\ncalderon\nsneakers\nmoravian\n##ksha\n1622\nrq\n294\n##imeters\nbodo\n##isance\n##pre\n##ronia\nanatomical\nexcerpt\n##lke\ndh\nkunst\n##tablished\n##scoe\nbiomass\npanted\nunharmed\ngael\nhousemates\nmontpellier\n##59\ncoa\nrodents\ntonic\nhickory\nsingleton\n##taro\n451\n1719\naldo\nbreaststroke\ndempsey\noch\nrocco\n##cuit\nmerton\ndissemination\nmidsummer\nserials\n##idi\nhaji\npolynomials\n##rdon\ngs\nenoch\nprematurely\nshutter\ntaunton\n£3\n##grating\n##inates\narchangel\nharassed\n##asco\n326\narchway\ndazzling\n##ecin\n1736\nsumo\nwat\n##kovich\n1086\nhonneur\n##ently\n##nostic\n##ttal\n##idon\n1605\n403\n1716\nblogger\nrents\n##gnan\nhires\n##ikh\n##dant\nhowie\n##rons\nhandler\nretracted\nshocks\n1632\narun\nduluth\nkepler\ntrumpeter\n##lary\npeeking\nseasoned\ntrooper\n##mara\nlaszlo\n##iciencies\n##rti\nheterosexual\n##inatory\n##ssion\nindira\njogging\n##inga\n##lism\nbeit\ndissatisfaction\nmalice\n##ately\nnedra\npeeling\n##rgeon\n47th\nstadiums\n475\nvertigo\n##ains\niced\nrestroom\n##plify\n##tub\nillustrating\npear\n##chner\n##sibility\ninorganic\nrappers\nreceipts\nwatery\n##kura\nlucinda\n##oulos\nreintroduced\n##8th\n##tched\ngracefully\nsaxons\nnutritional\nwastewater\nrained\nfavourites\nbedrock\nfisted\nhallways\nlikeness\nupscale\n##lateral\n1580\nblinds\nprequel\n##pps\n##tama\ndeter\nhumiliating\nrestraining\ntn\nvents\n1659\nlaundering\nrecess\nrosary\ntractors\ncoulter\nfederer\n##ifiers\n##plin\npersistence\n##quitable\ngeschichte\npendulum\nquakers\n##beam\nbassett\npictorial\nbuffet\nkoln\n##sitor\ndrills\nreciprocal\nshooters\n##57\n##cton\n##tees\nconverge\npip\ndmitri\ndonnelly\nyamamoto\naqua\nazores\ndemographics\nhypnotic\nspitfire\nsuspend\nwryly\nroderick\n##rran\nsebastien\n##asurable\nmavericks\n##fles\n##200\nhimalayan\nprodigy\n##iance\ntransvaal\ndemonstrators\nhandcuffs\ndodged\nmcnamara\nsublime\n1726\ncrazed\n##efined\n##till\nivo\npondered\nreconciled\nshrill\nsava\n##duk\nbal\ncad\nheresy\njaipur\ngoran\n##nished\n341\nlux\nshelly\nwhitehall\n##hre\nisraelis\npeacekeeping\n##wled\n1703\ndemetrius\nousted\n##arians\n##zos\nbeale\nanwar\nbackstroke\nraged\nshrinking\ncremated\n##yck\nbenign\ntowing\nwadi\ndarmstadt\nlandfill\nparana\nsoothe\ncolleen\nsidewalks\nmayfair\ntumble\nhepatitis\nferrer\nsuperstructure\n##gingly\n##urse\n##wee\nanthropological\ntranslators\n##mies\ncloseness\nhooves\n##pw\nmondays\n##roll\n##vita\nlandscaping\n##urized\npurification\nsock\nthorns\nthwarted\njalan\ntiberius\n##taka\nsaline\n##rito\nconfidently\nkhyber\nsculptors\n##ij\nbrahms\nhammersmith\ninspectors\nbattista\nfivb\nfragmentation\nhackney\n##uls\narresting\nexercising\nantoinette\nbedfordshire\n##zily\ndyed\n##hema\n1656\nracetrack\nvariability\n##tique\n1655\naustrians\ndeteriorating\nmadman\ntheorists\naix\nlehman\nweathered\n1731\ndecreed\neruptions\n1729\nflaw\nquinlan\nsorbonne\nflutes\nnunez\n1711\nadored\ndownwards\nfable\nrasped\n1712\nmoritz\nmouthful\nrenegade\nshivers\nstunts\ndysfunction\nrestrain\ntranslit\n327\npancakes\n##avio\n##cision\n##tray\n351\nvial\n##lden\nbain\n##maid\n##oxide\nchihuahua\nmalacca\nvimes\n##rba\n##rnier\n1664\ndonnie\nplaques\n##ually\n337\nbangs\nfloppy\nhuntsville\nloretta\nnikolay\n##otte\neater\nhandgun\nubiquitous\n##hett\neras\nzodiac\n1634\n##omorphic\n1820s\n##zog\ncochran\n##bula\n##lithic\nwarring\n##rada\ndalai\nexcused\nblazers\nmcconnell\nreeling\nbot\neste\n##abi\ngeese\nhoax\ntaxon\n##bla\nguitarists\n##icon\ncondemning\nhunts\ninversion\nmoffat\ntaekwondo\n##lvis\n1624\nstammered\n##rest\n##rzy\nsousa\nfundraiser\nmarylebone\nnavigable\nuptown\ncabbage\ndaniela\nsalman\nshitty\nwhimper\n##kian\n##utive\nprogrammers\nprotections\nrm\n##rmi\n##rued\nforceful\n##enes\nfuss\n##tao\n##wash\nbrat\noppressive\nreykjavik\nspartak\nticking\n##inkles\n##kiewicz\nadolph\nhorst\nmaui\nprotege\nstraighten\ncpc\nlandau\nconcourse\nclements\nresultant\n##ando\nimaginative\njoo\nreactivated\n##rem\n##ffled\n##uising\nconsultative\n##guide\nflop\nkaitlyn\nmergers\nparenting\nsomber\n##vron\nsupervise\nvidhan\n##imum\ncourtship\nexemplified\nharmonies\nmedallist\nrefining\n##rrow\n##ка\namara\n##hum\n780\ngoalscorer\nsited\novershadowed\nrohan\ndispleasure\nsecretive\nmultiplied\nosman\n##orth\nengravings\npadre\n##kali\n##veda\nminiatures\nmis\n##yala\nclap\npali\nrook\n##cana\n1692\n57th\nantennae\nastro\noskar\n1628\nbulldog\ncrotch\nhackett\nyucatan\n##sure\namplifiers\nbrno\nferrara\nmigrating\n##gree\nthanking\nturing\n##eza\nmccann\nting\nandersson\nonslaught\ngaines\nganga\nincense\nstandardization\n##mation\nsentai\nscuba\nstuffing\nturquoise\nwaivers\nalloys\n##vitt\nregaining\nvaults\n##clops\n##gizing\ndigger\nfurry\nmemorabilia\nprobing\n##iad\npayton\nrec\ndeutschland\nfilippo\nopaque\nseamen\nzenith\nafrikaans\n##filtration\ndisciplined\ninspirational\n##merie\nbanco\nconfuse\ngrafton\ntod\n##dgets\nchampioned\nsimi\nanomaly\nbiplane\n##ceptive\nelectrode\n##para\n1697\ncleavage\ncrossbow\nswirl\ninformant\n##lars\n##osta\nafi\nbonfire\nspec\n##oux\nlakeside\nslump\n##culus\n##lais\n##qvist\n##rrigan\n1016\nfacades\nborg\ninwardly\ncervical\nxl\npointedly\n050\nstabilization\n##odon\nchests\n1699\nhacked\nctv\northogonal\nsuzy\n##lastic\ngaulle\njacobite\nrearview\n##cam\n##erted\nashby\n##drik\n##igate\n##mise\n##zbek\naffectionately\ncanine\ndisperse\nlatham\n##istles\n##ivar\nspielberg\n##orin\n##idium\nezekiel\ncid\n##sg\ndurga\nmiddletown\n##cina\ncustomized\nfrontiers\nharden\n##etano\n##zzy\n1604\nbolsheviks\n##66\ncoloration\nyoko\n##bedo\nbriefs\nslabs\ndebra\nliquidation\nplumage\n##oin\nblossoms\ndementia\nsubsidy\n1611\nproctor\nrelational\njerseys\nparochial\nter\n##ici\nesa\npeshawar\ncavalier\nloren\ncpi\nidiots\nshamrock\n1646\ndutton\nmalabar\nmustache\n##endez\n##ocytes\nreferencing\nterminates\nmarche\nyarmouth\n##sop\nacton\nmated\nseton\nsubtly\nbaptised\nbeige\nextremes\njolted\nkristina\ntelecast\n##actic\nsafeguard\nwaldo\n##baldi\n##bular\nendeavors\nsloppy\nsubterranean\n##ensburg\n##itung\ndelicately\npigment\ntq\n##scu\n1626\n##ound\ncollisions\ncoveted\nherds\n##personal\n##meister\n##nberger\nchopra\n##ricting\nabnormalities\ndefective\ngalician\nlucie\n##dilly\nalligator\nlikened\n##genase\nburundi\nclears\ncomplexion\nderelict\ndeafening\ndiablo\nfingered\nchampaign\ndogg\nenlist\nisotope\nlabeling\nmrna\n##erre\nbrilliance\nmarvelous\n##ayo\n1652\ncrawley\nether\nfooted\ndwellers\ndeserts\nhamish\nrubs\nwarlock\nskimmed\n##lizer\n870\nbuick\nembark\nheraldic\nirregularities\n##ajan\nkiara\n##kulam\n##ieg\nantigen\nkowalski\n##lge\noakley\nvisitation\n##mbit\nvt\n##suit\n1570\nmurderers\n##miento\n##rites\nchimneys\n##sling\ncondemn\ncuster\nexchequer\nhavre\n##ghi\nfluctuations\n##rations\ndfb\nhendricks\nvaccines\n##tarian\nnietzsche\nbiking\njuicy\n##duced\nbrooding\nscrolling\nselangor\n##ragan\n352\nannum\nboomed\nseminole\nsugarcane\n##dna\ndepartmental\ndismissing\ninnsbruck\narteries\nashok\nbatavia\ndaze\nkun\novertook\n##rga\n##tlan\nbeheaded\ngaddafi\nholm\nelectronically\nfaulty\ngalilee\nfractures\nkobayashi\n##lized\ngunmen\nmagma\naramaic\nmala\neastenders\ninference\nmessengers\nbf\n##qu\n407\nbathrooms\n##vere\n1658\nflashbacks\nideally\nmisunderstood\n##jali\n##weather\nmendez\n##grounds\n505\nuncanny\n##iii\n1709\nfriendships\n##nbc\nsacrament\naccommodated\nreiterated\nlogistical\npebbles\nthumped\n##escence\nadministering\ndecrees\ndrafts\n##flight\n##cased\n##tula\nfuturistic\npicket\nintimidation\nwinthrop\n##fahan\ninterfered\n339\nafar\nfrancoise\nmorally\nuta\ncochin\ncroft\ndwarfs\n##bruck\n##dents\n##nami\nbiker\n##hner\n##meral\nnano\n##isen\n##ometric\n##pres\n##ан\nbrightened\nmeek\nparcels\nsecurely\ngunners\n##jhl\n##zko\nagile\nhysteria\n##lten\n##rcus\nbukit\nchamps\nchevy\ncuckoo\nleith\nsadler\ntheologians\nwelded\n##section\n1663\njj\nplurality\nxander\n##rooms\n##formed\nshredded\ntemps\nintimately\npau\ntormented\n##lok\n##stellar\n1618\ncharred\nems\nessen\n##mmel\nalarms\nspraying\nascot\nblooms\ntwinkle\n##abia\n##apes\ninternment\nobsidian\n##chaft\nsnoop\n##dav\n##ooping\nmalibu\n##tension\nquiver\n##itia\nhays\nmcintosh\ntravers\nwalsall\n##ffie\n1623\nbeverley\nschwarz\nplunging\nstructurally\nm3\nrosenthal\nvikram\n##tsk\n770\nghz\n##onda\n##tiv\nchalmers\ngroningen\npew\nreckon\nunicef\n##rvis\n55th\n##gni\n1651\nsulawesi\navila\ncai\nmetaphysical\nscrewing\nturbulence\n##mberg\naugusto\nsamba\n56th\nbaffled\nmomentary\ntoxin\n##urian\n##wani\naachen\ncondoms\ndali\nsteppe\n##3d\n##app\n##oed\n##year\nadolescence\ndauphin\nelectrically\ninaccessible\nmicroscopy\nnikita\n##ega\natv\n##cel\n##enter\n##oles\n##oteric\n##ы\naccountants\npunishments\nwrongly\nbribes\nadventurous\nclinch\nflinders\nsouthland\n##hem\n##kata\ngough\n##ciency\nlads\nsoared\n##ה\nundergoes\ndeformation\noutlawed\nrubbish\n##arus\n##mussen\n##nidae\n##rzburg\narcs\n##ingdon\n##tituted\n1695\nwheelbase\nwheeling\nbombardier\ncampground\nzebra\n##lices\n##oj\n##bain\nlullaby\n##ecure\ndonetsk\nwylie\ngrenada\n##arding\n##ης\nsquinting\neireann\nopposes\n##andra\nmaximal\nrunes\n##broken\n##cuting\n##iface\n##ror\n##rosis\nadditive\nbritney\nadultery\ntriggering\n##drome\ndetrimental\naarhus\ncontainment\njc\nswapped\nvichy\n##ioms\nmadly\n##oric\n##rag\nbrant\n##ckey\n##trix\n1560\n1612\nbroughton\nrustling\n##stems\n##uder\nasbestos\nmentoring\n##nivorous\nfinley\nleaps\n##isan\napical\npry\nslits\nsubstitutes\n##dict\nintuitive\nfantasia\ninsistent\nunreasonable\n##igen\n##vna\ndomed\nhannover\nmargot\nponder\n##zziness\nimpromptu\njian\nlc\nrampage\nstemming\n##eft\nandrey\ngerais\nwhichever\namnesia\nappropriated\nanzac\nclicks\nmodifying\nultimatum\ncambrian\nmaids\nverve\nyellowstone\n##mbs\nconservatoire\n##scribe\nadherence\ndinners\nspectra\nimperfect\nmysteriously\nsidekick\ntatar\ntuba\n##aks\n##ifolia\ndistrust\n##athan\n##zle\nc2\nronin\nzac\n##pse\ncelaena\ninstrumentalist\nscents\nskopje\n##mbling\ncomical\ncompensated\nvidal\ncondor\nintersect\njingle\nwavelengths\n##urrent\nmcqueen\n##izzly\ncarp\nweasel\n422\nkanye\nmilitias\npostdoctoral\neugen\ngunslinger\n##ɛ\nfaux\nhospice\n##for\nappalled\nderivation\ndwarves\n##elis\ndilapidated\n##folk\nastoria\nphilology\n##lwyn\n##otho\n##saka\ninducing\nphilanthropy\n##bf\n##itative\ngeek\nmarkedly\nsql\n##yce\nbessie\nindices\nrn\n##flict\n495\nfrowns\nresolving\nweightlifting\ntugs\ncleric\ncontentious\n1653\nmania\nrms\n##miya\n##reate\n##ruck\n##tucket\nbien\neels\nmarek\n##ayton\n##cence\ndiscreet\nunofficially\n##ife\nleaks\n##bber\n1705\n332\ndung\ncompressor\nhillsborough\npandit\nshillings\ndistal\n##skin\n381\n##tat\n##you\nnosed\n##nir\nmangrove\nundeveloped\n##idia\ntextures\n##inho\n##500\n##rise\nae\nirritating\nnay\namazingly\nbancroft\napologetic\ncompassionate\nkata\nsymphonies\n##lovic\nairspace\n##lch\n930\ngifford\nprecautions\nfulfillment\nsevilla\nvulgar\nmartinique\n##urities\nlooting\npiccolo\ntidy\n##dermott\nquadrant\narmchair\nincomes\nmathematicians\nstampede\nnilsson\n##inking\n##scan\nfoo\nquarterfinal\n##ostal\nshang\nshouldered\nsquirrels\n##owe\n344\nvinegar\n##bner\n##rchy\n##systems\ndelaying\n##trics\nars\ndwyer\nrhapsody\nsponsoring\n##gration\nbipolar\ncinder\nstarters\n##olio\n##urst\n421\nsignage\n##nty\naground\nfigurative\nmons\nacquaintances\nduets\nerroneously\nsoyuz\nelliptic\nrecreated\n##cultural\n##quette\n##ssed\n##tma\n##zcz\nmoderator\nscares\n##itaire\n##stones\n##udence\njuniper\nsighting\n##just\n##nsen\nbritten\ncalabria\nry\nbop\ncramer\nforsyth\nstillness\n##л\nairmen\ngathers\nunfit\n##umber\n##upt\ntaunting\n##rip\nseeker\nstreamlined\n##bution\nholster\nschumann\ntread\nvox\n##gano\n##onzo\nstrive\ndil\nreforming\ncovent\nnewbury\npredicting\n##orro\ndecorate\ntre\n##puted\nandover\nie\nasahi\ndept\ndunkirk\ngills\n##tori\nburen\nhuskies\n##stis\n##stov\nabstracts\nbets\nloosen\n##opa\n1682\nyearning\n##glio\n##sir\nberman\neffortlessly\nenamel\nnapoli\npersist\n##peration\n##uez\nattache\nelisa\nb1\ninvitations\n##kic\naccelerating\nreindeer\nboardwalk\nclutches\nnelly\npolka\nstarbucks\n##kei\nadamant\nhuey\nlough\nunbroken\nadventurer\nembroidery\ninspecting\nstanza\n##ducted\nnaia\ntaluka\n##pone\n##roids\nchases\ndeprivation\nflorian\n##jing\n##ppet\nearthly\n##lib\n##ssee\ncolossal\nforeigner\nvet\nfreaks\npatrice\nrosewood\ntriassic\nupstate\n##pkins\ndominates\nata\nchants\nks\nvo\n##400\n##bley\n##raya\n##rmed\n555\nagra\ninfiltrate\n##ailing\n##ilation\n##tzer\n##uppe\n##werk\nbinoculars\nenthusiast\nfujian\nsqueak\n##avs\nabolitionist\nalmeida\nboredom\nhampstead\nmarsden\nrations\n##ands\ninflated\n334\nbonuses\nrosalie\npatna\n##rco\n329\ndetachments\npenitentiary\n54th\nflourishing\nwoolf\n##dion\n##etched\npapyrus\n##lster\n##nsor\n##toy\nbobbed\ndismounted\nendelle\ninhuman\nmotorola\ntbs\nwince\nwreath\n##ticus\nhideout\ninspections\nsanjay\ndisgrace\ninfused\npudding\nstalks\n##urbed\narsenic\nleases\n##hyl\n##rrard\ncollarbone\n##waite\n##wil\ndowry\n##bant\n##edance\ngenealogical\nnitrate\nsalamanca\nscandals\nthyroid\nnecessitated\n##!\n##\"\n###\n##$\n##%\n##&\n##'\n##(\n##)\n##*\n##+\n##,\n##-\n##.\n##/\n##:\n##;\n##<\n##=\n##>\n##?\n##@\n##[\n##\\\n##]\n##^\n##_\n##`\n##{\n##|\n##}\n##~\n##¡\n##¢\n##£\n##¤\n##¥\n##¦\n##§\n##¨\n##©\n##ª\n##«\n##¬\n##®\n##±\n##´\n##µ\n##¶\n##·\n##º\n##»\n##¼\n##¾\n##¿\n##æ\n##ð\n##÷\n##þ\n##đ\n##ħ\n##ŋ\n##œ\n##ƒ\n##ɐ\n##ɑ\n##ɒ\n##ɔ\n##ɕ\n##ə\n##ɡ\n##ɣ\n##ɨ\n##ɪ\n##ɫ\n##ɬ\n##ɯ\n##ɲ\n##ɴ\n##ɹ\n##ɾ\n##ʀ\n##ʁ\n##ʂ\n##ʃ\n##ʉ\n##ʊ\n##ʋ\n##ʌ\n##ʎ\n##ʐ\n##ʑ\n##ʒ\n##ʔ\n##ʰ\n##ʲ\n##ʳ\n##ʷ\n##ʸ\n##ʻ\n##ʼ\n##ʾ\n##ʿ\n##ˈ\n##ˡ\n##ˢ\n##ˣ\n##ˤ\n##β\n##γ\n##δ\n##ε\n##ζ\n##θ\n##κ\n##λ\n##μ\n##ξ\n##ο\n##π\n##ρ\n##σ\n##τ\n##υ\n##φ\n##χ\n##ψ\n##ω\n##б\n##г\n##д\n##ж\n##з\n##м\n##п\n##с\n##у\n##ф\n##х\n##ц\n##ч\n##ш\n##щ\n##ъ\n##э\n##ю\n##ђ\n##є\n##і\n##ј\n##љ\n##њ\n##ћ\n##ӏ\n##ա\n##բ\n##գ\n##դ\n##ե\n##թ\n##ի\n##լ\n##կ\n##հ\n##մ\n##յ\n##ն\n##ո\n##պ\n##ս\n##վ\n##տ\n##ր\n##ւ\n##ք\n##־\n##א\n##ב\n##ג\n##ד\n##ו\n##ז\n##ח\n##ט\n##י\n##ך\n##כ\n##ל\n##ם\n##מ\n##ן\n##נ\n##ס\n##ע\n##ף\n##פ\n##ץ\n##צ\n##ק\n##ר\n##ש\n##ת\n##،\n##ء\n##ب\n##ت\n##ث\n##ج\n##ح\n##خ\n##ذ\n##ز\n##س\n##ش\n##ص\n##ض\n##ط\n##ظ\n##ع\n##غ\n##ـ\n##ف\n##ق\n##ك\n##و\n##ى\n##ٹ\n##پ\n##چ\n##ک\n##گ\n##ں\n##ھ\n##ہ\n##ے\n##अ\n##आ\n##उ\n##ए\n##क\n##ख\n##ग\n##च\n##ज\n##ट\n##ड\n##ण\n##त\n##थ\n##द\n##ध\n##न\n##प\n##ब\n##भ\n##म\n##य\n##र\n##ल\n##व\n##श\n##ष\n##स\n##ह\n##ा\n##ि\n##ी\n##ो\n##।\n##॥\n##ং\n##অ\n##আ\n##ই\n##উ\n##এ\n##ও\n##ক\n##খ\n##গ\n##চ\n##ছ\n##জ\n##ট\n##ড\n##ণ\n##ত\n##থ\n##দ\n##ধ\n##ন\n##প\n##ব\n##ভ\n##ম\n##য\n##র\n##ল\n##শ\n##ষ\n##স\n##হ\n##া\n##ি\n##ী\n##ে\n##க\n##ச\n##ட\n##த\n##ந\n##ன\n##ப\n##ம\n##ய\n##ர\n##ல\n##ள\n##வ\n##ா\n##ி\n##ு\n##ே\n##ை\n##ನ\n##ರ\n##ಾ\n##ක\n##ය\n##ර\n##ල\n##ව\n##ා\n##ก\n##ง\n##ต\n##ท\n##น\n##พ\n##ม\n##ย\n##ร\n##ล\n##ว\n##ส\n##อ\n##า\n##เ\n##་\n##།\n##ག\n##ང\n##ད\n##ན\n##པ\n##བ\n##མ\n##འ\n##ར\n##ལ\n##ས\n##မ\n##ა\n##ბ\n##გ\n##დ\n##ე\n##ვ\n##თ\n##ი\n##კ\n##ლ\n##მ\n##ნ\n##ო\n##რ\n##ს\n##ტ\n##უ\n##ᄀ\n##ᄂ\n##ᄃ\n##ᄅ\n##ᄆ\n##ᄇ\n##ᄉ\n##ᄊ\n##ᄋ\n##ᄌ\n##ᄎ\n##ᄏ\n##ᄐ\n##ᄑ\n##ᄒ\n##ᅡ\n##ᅢ\n##ᅥ\n##ᅦ\n##ᅧ\n##ᅩ\n##ᅪ\n##ᅭ\n##ᅮ\n##ᅯ\n##ᅲ\n##ᅳ\n##ᅴ\n##ᅵ\n##ᆨ\n##ᆫ\n##ᆯ\n##ᆷ\n##ᆸ\n##ᆼ\n##ᴬ\n##ᴮ\n##ᴰ\n##ᴵ\n##ᴺ\n##ᵀ\n##ᵃ\n##ᵇ\n##ᵈ\n##ᵉ\n##ᵍ\n##ᵏ\n##ᵐ\n##ᵒ\n##ᵖ\n##ᵗ\n##ᵘ\n##ᵣ\n##ᵤ\n##ᵥ\n##ᶜ\n##ᶠ\n##‐\n##‑\n##‒\n##–\n##—\n##―\n##‖\n##‘\n##’\n##‚\n##“\n##”\n##„\n##†\n##‡\n##•\n##…\n##‰\n##′\n##″\n##›\n##‿\n##⁄\n##⁰\n##ⁱ\n##⁴\n##⁵\n##⁶\n##⁷\n##⁸\n##⁹\n##⁻\n##ⁿ\n##₅\n##₆\n##₇\n##₈\n##₉\n##₊\n##₍\n##₎\n##ₐ\n##ₑ\n##ₒ\n##ₓ\n##ₕ\n##ₖ\n##ₗ\n##ₘ\n##ₚ\n##ₛ\n##ₜ\n##₤\n##₩\n##€\n##₱\n##₹\n##ℓ\n##№\n##ℝ\n##™\n##⅓\n##⅔\n##←\n##↑\n##→\n##↓\n##↔\n##↦\n##⇄\n##⇌\n##⇒\n##∂\n##∅\n##∆\n##∇\n##∈\n##∗\n##∘\n##√\n##∞\n##∧\n##∨\n##∩\n##∪\n##≈\n##≡\n##≤\n##≥\n##⊂\n##⊆\n##⊕\n##⊗\n##⋅\n##─\n##│\n##■\n##▪\n##●\n##★\n##☆\n##☉\n##♠\n##♣\n##♥\n##♦\n##♯\n##⟨\n##⟩\n##ⱼ\n##⺩\n##⺼\n##⽥\n##、\n##。\n##〈\n##〉\n##《\n##》\n##「\n##」\n##『\n##』\n##〜\n##あ\n##い\n##う\n##え\n##お\n##か\n##き\n##く\n##け\n##こ\n##さ\n##し\n##す\n##せ\n##そ\n##た\n##ち\n##っ\n##つ\n##て\n##と\n##な\n##に\n##ぬ\n##ね\n##の\n##は\n##ひ\n##ふ\n##へ\n##ほ\n##ま\n##み\n##む\n##め\n##も\n##や\n##ゆ\n##よ\n##ら\n##り\n##る\n##れ\n##ろ\n##を\n##ん\n##ァ\n##ア\n##ィ\n##イ\n##ウ\n##ェ\n##エ\n##オ\n##カ\n##キ\n##ク\n##ケ\n##コ\n##サ\n##シ\n##ス\n##セ\n##タ\n##チ\n##ッ\n##ツ\n##テ\n##ト\n##ナ\n##ニ\n##ノ\n##ハ\n##ヒ\n##フ\n##ヘ\n##ホ\n##マ\n##ミ\n##ム\n##メ\n##モ\n##ャ\n##ュ\n##ョ\n##ラ\n##リ\n##ル\n##レ\n##ロ\n##ワ\n##ン\n##・\n##ー\n##一\n##三\n##上\n##下\n##不\n##世\n##中\n##主\n##久\n##之\n##也\n##事\n##二\n##五\n##井\n##京\n##人\n##亻\n##仁\n##介\n##代\n##仮\n##伊\n##会\n##佐\n##侍\n##保\n##信\n##健\n##元\n##光\n##八\n##公\n##内\n##出\n##分\n##前\n##劉\n##力\n##加\n##勝\n##北\n##区\n##十\n##千\n##南\n##博\n##原\n##口\n##古\n##史\n##司\n##合\n##吉\n##同\n##名\n##和\n##囗\n##四\n##国\n##國\n##土\n##地\n##坂\n##城\n##堂\n##場\n##士\n##夏\n##外\n##大\n##天\n##太\n##夫\n##奈\n##女\n##子\n##学\n##宀\n##宇\n##安\n##宗\n##定\n##宣\n##宮\n##家\n##宿\n##寺\n##將\n##小\n##尚\n##山\n##岡\n##島\n##崎\n##川\n##州\n##巿\n##帝\n##平\n##年\n##幸\n##广\n##弘\n##張\n##彳\n##後\n##御\n##德\n##心\n##忄\n##志\n##忠\n##愛\n##成\n##我\n##戦\n##戸\n##手\n##扌\n##政\n##文\n##新\n##方\n##日\n##明\n##星\n##春\n##昭\n##智\n##曲\n##書\n##月\n##有\n##朝\n##木\n##本\n##李\n##村\n##東\n##松\n##林\n##森\n##楊\n##樹\n##橋\n##歌\n##止\n##正\n##武\n##比\n##氏\n##民\n##水\n##氵\n##氷\n##永\n##江\n##沢\n##河\n##治\n##法\n##海\n##清\n##漢\n##瀬\n##火\n##版\n##犬\n##王\n##生\n##田\n##男\n##疒\n##発\n##白\n##的\n##皇\n##目\n##相\n##省\n##真\n##石\n##示\n##社\n##神\n##福\n##禾\n##秀\n##秋\n##空\n##立\n##章\n##竹\n##糹\n##美\n##義\n##耳\n##良\n##艹\n##花\n##英\n##華\n##葉\n##藤\n##行\n##街\n##西\n##見\n##訁\n##語\n##谷\n##貝\n##貴\n##車\n##軍\n##辶\n##道\n##郎\n##郡\n##部\n##都\n##里\n##野\n##金\n##鈴\n##镇\n##長\n##門\n##間\n##阝\n##阿\n##陳\n##陽\n##雄\n##青\n##面\n##風\n##食\n##香\n##馬\n##高\n##龍\n##龸\n##ﬁ\n##ﬂ\n##！\n##（\n##）\n##，\n##－\n##．\n##／\n##：\n##？\n##～\n"}],"source":["!curl -0 https://raw.githubusercontent.com/sayankotor/BERT_botcamp19/master/bert-base-uncased-vocab.txt"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b5chcV-nSXF-"},"source":["# Check hardware specs"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"/bin/sh: nvidia-smi: command not found\n"}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":11250,"status":"ok","timestamp":1579990402833,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"mfYZBx9MhqA-","outputId":"064b252f-02b1-450c-ac24-d716ad929469"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n"]}],"source":["!lscpu |grep 'Model name'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":12421,"status":"ok","timestamp":1579990404013,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"yhu_Fkgphprq","outputId":"ec468938-9f4b-418e-d266-ed984e325a86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thread(s) per core:  2\n"]}],"source":["!lscpu | grep 'Thread'"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lh_W-rYlSbjj"},"source":["# Finetuning Hands-on"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"colab_type":"code","executionInfo":{"elapsed":14409,"status":"ok","timestamp":1579990406011,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"vWWepa2JeXOP","outputId":"a21c6ce6-db41-4d21-d83d-0aa6901ae653"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\n"]},{"data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"output_type":"display_data"}],"source":["import torch\n","import torchtext\n","import random\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","import torch.nn.functional as F\n","from dataclasses import dataclass\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from pytorch_pretrained_bert import BertModel, BertTokenizer\n","import os\n","import time\n","import random\n","from torch.nn.utils import clip_grad_norm_\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"8tm6B20MEK3I"},"outputs":[],"source":["random.seed(10)\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed(10)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y7V8wid9tgh5"},"source":["## Parameter description\n","\n","Here are some of the main parameters you will want to consider when finetuning BERT:\n","\n","* **Gradient Clipping**: If the norm of the gradient gets above max_grad_norm, We divide the gradient by its L2 norm. This gradient clipping method avoids exploiding gradients.\n","* **Learning rate**: The `learning_rate` parameter is very important as it controls how we update the already trained parameters from the Language Modelling Task. If this parameter is too high, we will notice a forgetting of the previous task. It needs to be carefully tuned.\n","* **Sequence length**: The attention mechanism scales in O(L^2). So you should avoid handling sequences larger than what you really need. "]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"yJ3qmcecqXpd"},"outputs":[],"source":["@dataclass\n","class ArgsBert:\n","    max_seq_length: int = 256 # The maximum total input sequence length after WordPiece tokenization.\n","    learning_rate: float = 3e-6 # Initial Learning rate for Adam\n","    num_train_epochs: int = 3 # epochs\n","    batch_size: int = 4\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    loss = torch.nn.BCEWithLogitsLoss().cuda()\n","    clip_gradient_max_norm: float = 1.0 "]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"PaCu4-GTryN5"},"outputs":[],"source":["args = ArgsBert()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oJeo9Iqg6-u5"},"source":["## Preparing the Data\n","\n","The dataset we will be using is the IMDB movie review sentiment analysis dataset. The data looks like this:\n","\n","  - **Review**: 'This movie caught me by surprise . For years I have avoided many of Harold Lloyd \\'s sound pictures ( as well as those of Keaton ) because they have a generally well - deserved reputation for being lousy compared to the silent films because the basic formula has been lost . However , when I saw this film I was pleasantly surprised to find I actually liked it, ... \n","\n","  - **Sentiment**: \"pos\""]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"gsESIItjqd5Q"},"outputs":[],"source":["TEXT = torchtext.data.Field(tokenize = 'spacy', include_lengths = True) # helper to tokenize using spacy\n","LABEL = torchtext.data.LabelField(dtype = torch.float)\n","\n","def get_dataloader_bert(tokens_ids, masks, lbls, random=True, batch_size=64):\n","    \"\"\"\"\"\n","    Returns a dataloader to iterate over the data. \n","    Arguments:\n","    - tokens_ids:\n","    - masks: \n","    - lbls: \n","    \"\"\"\"\"\n","    tokens_tensor = torch.tensor(tokens_ids)\n","    y_tensor = torch.tensor(lbls.reshape(-1, 1)).float()\n","    masks_tensor = torch.tensor(masks)\n","\n","    dataset = TensorDataset(tokens_tensor, masks_tensor, y_tensor)\n","    if random:\n","      sampler = RandomSampler(dataset)\n","      dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n","    else:\n","      sampler = SequentialSampler(dataset)\n","      dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","    return dataloader\n","\n","def fetch_imdb_data():\n","    \"\"\"\"\"\n","    Returns the imdb dataset\n","    \"\"\"\"\"\n","    full_train_data, val_data_ = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n","    return full_train_data, val_data_"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"dSNrqCtKeca4"},"outputs":[],"source":["# Download the IMDB data\n","full_train_data, full_test_data = fetch_imdb_data()\n","\n","# We randomly subsample the IMDB dataset (the training would take too long with the full dataset)\n","train_data = [full_train_data[random.randint(1,24000)] for _ in range(1000)]\n","test_data = [full_test_data[random.randint(1,24000)] for _ in range(3000)]"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"qXx7pcXBgj_3"},"outputs":[],"source":["bert_train_texts = list(\" \".join(train_data[i].text) for i in range(len(train_data))) \n","train_labels = list(train_data[i].label for i in range(len(train_data)))\n","\n","bert_test_texts = list(\" \".join(test_data[i].text) for i in range(len(test_data)))\n","test_labels = list(test_data[i].label for i in range(len(test_data)))"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"p6wDt75dgr6e"},"outputs":[],"source":["train_data = torchtext.data.Dataset(train_data, full_train_data.fields)\n","test_data = torchtext.data.Dataset(test_data, full_test_data.fields)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hvBFcDbb7B-q"},"source":["## Tokenization\n","\n","- The input to the bert model are word-pieces ([Original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)). Standard tokens are broken down into word pieces through the use of a WordPiece tokenizer. \n","\n","- A WordPiece tokenizer breaks the unknown words into multiple subwords.\n","For example, if the word \"chore\" does not belong to the vocabulary as a single piece, it might get split into two pieces belonging to the vocabulary: 'cho' and '##re'. \n","\n","- All the subwords start with the \"#\" symbol except for the first subword in the word. Imagine the words \"played\", \"playing\" are rare words and thus would not occur in a normal vocabulary. These words would be considered into the wordpiece tokenizer into this form: [`play`, `##ed`] and [`play`, `##ing`]. \n","\n","- You can have a look at the file `bert-base-uncased-vocab.txt` in your environnment to have an idea of the words present in the vocabular\n","\n","- Wordpiece tokenizers tends to be quite slow, however some efficient implementations exist: tokenizers from the [huggingface Library](https://github.com/huggingface/tokenizers) are much faster than the standard naive implementations (Implemented in Rust with python bindings)."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":98706,"status":"ok","timestamp":1579990490367,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"CaND54ZWguEV","outputId":"a6b5a254-bf60-4d6d-d31c-f6a3f61d8dad"},"outputs":[{"data":{"text/plain":["['super',\n"," '##cal',\n"," '##if',\n"," '##rag',\n"," '##ilis',\n"," '##tic',\n"," '##ex',\n"," '##pia',\n"," '##lid',\n"," '##oc',\n"," '##ious']"]},"execution_count":14,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","# Here is an example of splitting a rare token into wordpieces !\n","tokenizer.tokenize(\"supercalifragilisticexpialidocious\")"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"rfjikrg-gvkV"},"outputs":[],"source":["train_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  bert_train_texts] # tokenize reviews in train\n","test_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  bert_test_texts] # tokenize reviews in test"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wjM4OJE97Fdy"},"source":["## Padding and preparing tensors"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"ED0pVgaNxulH"},"outputs":[],"source":["bert_train_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in train_tokens] # wordpieces to ids\n","bert_test_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in test_tokens] # wordpieces to ids\n","\n","# Pad up to max_seq_length\n","bert_train_tokens_ids = pad_sequences(bert_train_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","bert_test_tokens_ids = pad_sequences(bert_test_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"DOLzB86lg9qU"},"outputs":[],"source":["train_y = np.array(train_labels) == 'pos' # gives a vector of bool [True, False, False, ...]\n","test_y = np.array(test_labels) == 'pos'"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"peapnkUWg_1j"},"outputs":[],"source":["# Attention masking for not attending padded tokens\n","bert_train_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in bert_train_tokens_ids]\n","bert_test_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in bert_test_tokens_ids]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5Doa9zFRexz0"},"source":["- Below is the output of the preprocessing (tokenization, padding etc). You can see that we have a tensor of IDs for the first sentence pointing to our vocabulary.\n","- The first id is always 101 refering to the **[CLS]** token in the vocabulary.\n","- The padding is done by the id=0 corresponding to **[PAD]** token in the vocabulary."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":510},"colab_type":"code","executionInfo":{"elapsed":112251,"status":"ok","timestamp":1579990503975,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"q-xoaVDjeukE","outputId":"e771c70e-85a1-42c6-e3bd-c5c0de40d0f3"},"outputs":[{"data":{"text/plain":["array([  101,  1045,  2323,  2031,  6618,  2008,  2151,  3185,  2007,\n","        1996, 14955,  3334,  3351,  2923,  3203,  1999,  2009,  2003,\n","        1050,  1005,  1056,  2183,  2000,  2022,  2204,  1012,  2009,\n","        2941,  4627,  2041,  3100,  1010,  2021,  2076,  1996,  2034,\n","        4028,  3496,  2017,  2424,  2041,  2008,  1996,  3185,  2017,\n","        1005,  2128,  3666,  2003,  1037,  3185,  2503,  1997,  1037,\n","        3185,  1012,  2045,  1005,  1055,  2111,  3564,  1999,  1037,\n","        3185,  3004,  3666,  2008,  3185,  1012,  2028,  2611,  1999,\n","        1996,  4378,  2003,  2061, 15703,  2008,  1045,  2052,  2031,\n","        2357,  2105,  1998, 21384,  2014,  1012,  1037,  2978,  4326,\n","        1010,  2021,  2521,  2013,  2204,  1012,   102,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0])"]},"execution_count":19,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["bert_train_tokens_ids[0]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SUNdg7WnfxAQ"},"source":["- The labels are presented below"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":112236,"status":"ok","timestamp":1579990503976,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"fkiftRwWfK3L","outputId":"c0b5eb48-27f5-4c9c-d656-69e2b0737465"},"outputs":[{"data":{"text/plain":["array([False,  True,  True,  True,  True])"]},"execution_count":20,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["test_y[0:5]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GkcdTBuG7JzH"},"source":["## Finetuning Bert"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G1vZ6NcD8yTs"},"source":["- Let's define our model `BertFinetune`\n","  - This model will be composed of the uncased version of bert + a dense layer on top of it for binary classification.\n","  \n","- Here are the steps for finetuning:\n","  - We load our pretrained LM transformer model with already trained weights\n","  - Add a new linear layer on top of the trained model\n","  - Finetune the parameters of the newly defined model on the downstream task\n","  - Evaluate\n"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"BxxqyTQPhBUc"},"outputs":[],"source":["class BertFinetune(torch.nn.Module):\n","    def __init__(self):\n","        super(BertFinetune, self).__init__()\n","\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.linear = torch.nn.Linear(768, 1)\n","    \n","    def forward(self, tokens, masks):\n","        # pooled_output will just consider the hidden state of the first token (i.e., the [CLS])\n","        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n","        logits = self.linear(pooled_output)\n","        return logits, pooled_output\n","\n","bert = BertFinetune()\n","bert = bert.cuda() # We push our model on the GPU!"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"1ewHHvRFhdqA"},"outputs":[],"source":["train_dataloader = get_dataloader_bert(bert_train_tokens_ids, bert_train_masks, train_y, batch_size=args.batch_size)\n","test_dataloader = get_dataloader_bert(bert_test_tokens_ids, bert_test_masks, test_y, random=False, batch_size=args.batch_size)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8aORee7EVI6W"},"source":["- To have an idea of the form of pooled output, its shape is [4, 768] which is the [CLS] hidden states for the 4 sentences in the batch"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":121664,"status":"ok","timestamp":1579990513532,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"cxNxLbx5VHGY","outputId":"57cb7938-6b55-427b-e32b-47807eb22e48"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 768])\n"]}],"source":["for data in train_dataloader: # Just doing one forward pass to check the output of our BERT based network\n","  token_ids, masks, labels = tuple(t.to(args.device) for t in data)\n","  pb, cls = bert(token_ids, masks)\n","  print(cls.shape)\n","  break"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oD03Rf8ZXdA3"},"source":["**Finally! Let's fine-tune our model !**"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"colab_type":"code","executionInfo":{"elapsed":313074,"status":"ok","timestamp":1579990704966,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"OzKrhav6him6","outputId":"24a11c37-c4ac-4650-f3e9-3f6b1d19c725"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0  --- loss:  0.6436396466493607\n","Epoch: 1  --- loss:  0.4031721138656139\n","Epoch: 2  --- loss:  0.2642683334052563\n"]}],"source":["optimizer = torch.optim.Adam(bert.parameters(), lr=args.learning_rate)\n","\n","losses = []\n","bert.train()\n","\n","for epoch in range(args.num_train_epochs):\n","    train_loss = 0\n","    for step_num, data in enumerate(train_dataloader):\n","        token_ids, masks, labels = tuple(t.to(args.device) for t in data) # Moving the input tensors to the GPU\n","\n","        logits, cls = bert(token_ids, masks) # Forward pass\n","        loss = args.loss(logits, labels) # compute our classification loss\n","\n","        train_loss += loss.item()  \n","        \n","        bert.zero_grad()\n","        loss.backward() \n","        clip_grad_norm_(parameters=bert.parameters(), max_norm=args.clip_gradient_max_norm)\n","\n","        optimizer.step()\n","        \n","    print(f'Epoch: {epoch}  --- loss:  {train_loss/(step_num + 1)}')"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"colab_type":"code","executionInfo":{"elapsed":314171,"status":"ok","timestamp":1579990706070,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"RamCUV_WoTUb","outputId":"8a3f9b4f-b441-45f9-d747-18f577280d49"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Jan 25 22:18:25 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P0    42W /  70W |   5821MiB / 15079MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":370897,"status":"ok","timestamp":1579990762803,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"hpvlOi7Jhl0n","outputId":"bff37d52-c11e-46be-e88e-f26d27567325"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time (Seconds) ________________ 56.39476752281189\n","___\n","              precision    recall  f1-score   support\n","\n","       False       0.89      0.87      0.88      1466\n","        True       0.88      0.89      0.89      1534\n","\n","    accuracy                           0.88      3000\n","   macro avg       0.88      0.88      0.88      3000\n","weighted avg       0.88      0.88      0.88      3000\n","\n"]}],"source":["def get_test_scores(model, test_dataloader, args, test_y):\n","  s = time.time()\n","  model.eval()\n","  model.to(args.device) # make sure the model is on the right device\n","  bert_predicted = []\n","  with torch.no_grad(): # no need for gradient computation for a simple eval\n","      for batch_data in test_dataloader:\n","          token_ids, masks, labels = tuple(t.to(args.device) for t in batch_data) # sending tensor to the right device\n","          logits, _ = model(token_ids, masks) # Forward pass with our BERT model\n","          bert_predicted += list(torch.sigmoid(logits.cpu().detach()[:, 0]).numpy() > 0.5) # Get predictions\n","  model.train()\n","\n","  print(\"Time (Seconds) ________________\", (time.time() - s))\n","  print(\"___\")\n","  print(classification_report(test_y, bert_predicted))\n","\n","get_test_scores(bert, test_dataloader, args, test_y)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"50OojoeKuxWK"},"source":["# Distillation Hands-on\n","\n","As we saw in the slides, distillation can be done using the logits of the teacher and the student. In our case the teacher is our fine-tuned BERT model. The following function is computing the logits on the training set. We will use those logits to do the distillation."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"L9vq1zpuGdNk"},"outputs":[],"source":["def get_training_logits(bert, bert_train_tokens_ids, bert_train_masks, train_y, args):\n","  \"\"\"\"\"\n","  Function to get the training logits from the already trained bert model\n","  \"\"\"\"\"\n","  bert.eval()\n","  all_logits = []\n","  train_logits_loader = get_dataloader_bert(bert_train_tokens_ids, bert_train_masks, train_y, random=False, batch_size=args.batch_size)\n","  lbls = []\n","  with torch.no_grad():\n","      for batch in train_logits_loader:\n","          token_ids, masks, labels = tuple(t.to(args.device) for t in batch)\n","          log, _ = bert(token_ids, masks)\n","          all_logits.extend(torch.flatten(log).cpu().numpy())\n","          lbls.extend(torch.flatten(labels).cpu().numpy())\n","  bert.train()\n","  return torch.Tensor(tuple(all_logits)), torch.Tensor(tuple(lbls))"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"eD4SwhBWmE5-"},"outputs":[],"source":["all_logits, lbls = get_training_logits(bert, bert_train_tokens_ids, bert_train_masks, train_y, args)\n","logits_loader = DataLoader(all_logits, batch_size=64, shuffle=False) # Dataloader for logits"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":389728,"status":"ok","timestamp":1579990781656,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"Gk4-6VFum2IR","outputId":"046fd879-00f2-48b8-e709-ea0a1ba955b1"},"outputs":[{"data":{"text/plain":["torch.Size([1000])"]},"execution_count":29,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["all_logits.shape"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VoVrtZiQ-YxE"},"source":["## Prepare the data"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"AtPYjAkM5l2J"},"outputs":[],"source":["TEXT.build_vocab(train_data, max_size = 25000)\n","LABEL.build_vocab(train_data,)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"zBi0airszPi1"},"outputs":[],"source":["@dataclass\n","class ArgsClf:\n","    num_train_epochs: int = 5 # epochs\n","    batch_size: int = 64 \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    loss = torch.nn.BCELoss()\n","    clip_gradient_max_norm: float = 5.0\n","    learning_rate=0.01"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"CfQuWkvZrrvh"},"outputs":[],"source":["args_clf = ArgsClf()"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Sqb041jt5mOA"},"outputs":[],"source":["train_iterator= torchtext.data.BucketIterator(\n","    train_data, \n","    sort_key=None,\n","    shuffle=False,\n","    batch_size = args_clf.batch_size,\n","    device = args_clf.device)\n","\n","test_iterator= torchtext.data.BucketIterator(\n","    test_data, \n","    sort_key=None,\n","    shuffle=False,\n","    batch_size = args_clf.batch_size,\n","    device = args_clf.device)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fbE81390-XDO"},"source":["## Model Definition\n","\n","- Very simple model with 1 embedding layer and a Linear layer"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"q-zMNujj5mWW"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BinaryCLF(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","    def forward(self, text):        \n","        embedded = self.embedding(text).permute(1, 0, 2)        \n","        return self.fc(F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1))"]},{"cell_type":"code","execution_count":0,"metadata":{"cellView":"form","colab":{},"colab_type":"code","id":"yU1LqSLKk6x3"},"outputs":[],"source":["#@title\n","##### Uncomment if you want to get all the logits for the 25k datapoints #####\n","\n","def get_full_logits():\n","  \"\"\"\"\"\n","  Returns the logits on the full IMDB Dataset\n","  Can be used for data augmentation and distillation\n","  \"\"\"\"\"\n","  # format data input \n","  train_texts_full = list(\" \".join(full_train_data[i].text) for i in range(len(full_train_data))) # get the sentences\n","  train_tokens_ids_full = list(tokenizer.encode(t) for t in train_texts_full) # use wordpiece tokenizer  train_tokens_ids_full = [tok.ids[:args.max_seq_length-1] for tok in train_tokens_ids_full] # truncate to max_seq_length\n","  train_tokens_ids_full = pad_sequences(train_tokens_ids_full, maxlen=args.max_seq_length, \n","                                        truncating=\"post\", padding=\"post\", dtype=\"int\") # pad sequences\n","  train_masks_full = [[float(i > 0) for i in ii] for ii in train_tokens_ids_full] \n","  train_masks_tensor_full = torch.tensor(train_tokens_ids_full)\n","  train_masks_tensor_full = torch.tensor(train_masks_full)\n","  dataset = TensorDataset(torch.tensor(train_tokens_ids_full), train_masks_tensor_full)\n","  sampler = SequentialSampler(dataset)\n","  dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n","  bert.eval()\n","  bert.cuda()\n","  all_logits = []\n","  with torch.no_grad():\n","      for batch in dataloader:\n","          token_ids, masks = tuple(t.to(args.device) for t in batch)\n","          log, _ = bert(token_ids, masks)\n","          all_logits.extend(torch.flatten(log).cpu().numpy())\n","\n","  return torch.Tensor(tuple(all_logits))\n","\n","#logits = get_full_logits()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bfNc_QzXlC7y"},"source":["## Helper Distillation\n","\n","- We provided for you the bert logits for the training set iterator, `train_iterator` which are in the `logits_loader`\n","- We also provided a function `get_full_logits` that you can use to get the logits on all the training set.\n","\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"colab_type":"code","executionInfo":{"elapsed":391716,"status":"ok","timestamp":1579990783686,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"4mYLKK3G5uQm","outputId":"e2723880-fdb7-4b10-fb0f-25599dc5a6c5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0  --- loss:  0.6508550924413344\n","Epoch: 1  --- loss:  0.6310171169393203\n","Epoch: 2  --- loss:  0.5791842727100148\n","Epoch: 3  --- loss:  0.48900363725774426\n","Epoch: 4  --- loss:  0.38708231554311867\n","Epoch: 5  --- loss:  0.2989107081118752\n","Epoch: 6  --- loss:  0.22979897611281452\n","Epoch: 7  --- loss:  0.17941275501952453\n","Epoch: 8  --- loss:  0.14212224807809382\n","Epoch: 9  --- loss:  0.1142382082693717\n"]}],"source":["net = BinaryCLF(vocab_size=len(TEXT.vocab), embedding_dim=100, output_dim=1, pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n","\n","criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=args_clf.learning_rate)\n","\n","net.cuda()\n","net.train()\n","\n","for epoch in range(10):\n","    train_loss=0\n","    step_num=0\n","    for batch, logits_bert in zip(train_iterator, logits_loader):\n","        net.zero_grad()\n","        logits_bert = torch.tensor(logits_bert).squeeze()\n","        output = net(batch.text[0]).squeeze(1)\n","        ############# IMPLEMENT DISTILLATION HERE ##################\n","        ############################################################\n","        # Use logits_bert\n","        loss = criterion(torch.sigmoid(output), batch.label.float())\n","        ############# IMPLEMENT DISTILLATION HERE ##################\n","        ############################################################\n","        loss.backward()\n","        train_loss += loss.item()\n","        nn.utils.clip_grad_norm_(net.parameters(), args_clf.clip_gradient_max_norm)\n","        optimizer.step()\n","        step_num+=1\n","    print(f'Epoch: {epoch}  --- loss:  {train_loss/(step_num + 1)}')\n"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"colab_type":"code","executionInfo":{"elapsed":392411,"status":"ok","timestamp":1579990784388,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"sxwH8mFd8Foo","outputId":"a4ef5830-82db-4eb7-d410-4d63cde1c847"},"outputs":[{"name":"stdout","output_type":"stream","text":["___\n","Time (in seconds) ________________ 0.48473691940307617\n","___\n","              precision    recall  f1-score   support\n","\n","       False       0.81      0.82      0.81      1466\n","        True       0.82      0.82      0.82      1534\n","\n","    accuracy                           0.82      3000\n","   macro avg       0.82      0.82      0.82      3000\n","weighted avg       0.82      0.82      0.82      3000\n","\n"]}],"source":["def get_test_score_binaryclf(model, test_iterator, args):\n","  s = time.time()\n","  model.eval()\n","  model.to(args.device)\n","  binary_clf_predicted = []\n","  with torch.no_grad():\n","      for batch in test_iterator:\n","          pb = model(batch.text[0].to(\"cuda\")).squeeze(1)\n","          binary_clf_predicted += list(torch.sigmoid(pb.cpu().detach()).numpy() > 0.5)\n","\n","  print(\"___\")\n","  print(\"Time (in seconds) ________________\",time.time() - s)\n","  print(\"___\")\n","  print(classification_report(test_y, binary_clf_predicted))\n","\n","get_test_score_binaryclf(net, test_iterator, args_clf)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YlHQ62VPN_Kx"},"source":["# Quantization"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"deWAkK0L5_KE"},"outputs":[],"source":["import torch.quantization\n","\n","def print_size_of_model(model):\n","    \"\"\"\"\"\n","    Get the size on disk of the model\n","    \"\"\"\"\"\n","    torch.save(model.state_dict(), \"temp.p\")\n","    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n","    os.remove('temp.p')\n","\n","\n","bert.eval()\n","bert.cpu()\n","\n","# As simple as that:\n","quantized_bert = torch.quantization.quantize_dynamic(\n","    bert, {nn.Linear}, dtype=torch.qint8\n",")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5AbzSuMj2fNK"},"source":["- Let's see the size on disk"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":394248,"status":"ok","timestamp":1579990786237,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"t4h1W1Lg3H72","outputId":"17e6b492-b317-452d-c1bc-f4769c90ae1a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size (MB): 181.426038\n"]}],"source":["print_size_of_model(quantized_bert)"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":396486,"status":"ok","timestamp":1579990788482,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"Z3dBvQnPxTLt","outputId":"7cfe7f8a-636b-46cf-a0b4-fcbf9839aab1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size (MB): 437.977457\n"]}],"source":["print_size_of_model(bert)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TnvGJZXLoCP9"},"source":["- Let's make a small dataset to compare the speed and the accuracy of both models"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"VJJBRk1Wf-fC"},"outputs":[],"source":["data_quantization_benchmark = [full_train_data[random.randint(1,24000)] for _ in range(100)]\n","\n","quantize_texts = list(\" \".join(data_quantization_benchmark[i].text) for i in range(len(data_quantization_benchmark))) \n","quantize_labels = list(data_quantization_benchmark[i].label for i in range(len(data_quantization_benchmark)))\n","quantize_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  quantize_texts] # tokenize reviews in quantization dataset\n","\n","quantize_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in quantize_tokens] # wordpieces to ids\n","quantize_tokens_ids = pad_sequences(quantize_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","quantize_labels = list(data_quantization_benchmark[i].label for i in range(len(data_quantization_benchmark)))\n","\n","quantize_y = np.array(quantize_labels) == 'pos'\n","quantize_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in quantize_tokens_ids]\n","\n","quantize_dataloader = get_dataloader_bert(quantize_tokens_ids, quantize_masks, quantize_y, batch_size=args.batch_size)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":450333,"status":"ok","timestamp":1579990842342,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"Ajr3uPTVnzFQ","outputId":"6601c5ba-b415-4d59-a733-be73ab9e3fa8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time (Seconds) ________________ 53.21387577056885\n","___\n","              precision    recall  f1-score   support\n","\n","       False       0.35      0.33      0.34        39\n","        True       0.59      0.61      0.60        61\n","\n","    accuracy                           0.50       100\n","   macro avg       0.47      0.47      0.47       100\n","weighted avg       0.50      0.50      0.50       100\n","\n"]}],"source":["args.device=\"cpu\"\n","get_test_scores(quantized_bert, quantize_dataloader, args, quantize_y)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":520138,"status":"ok","timestamp":1579990912157,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"},"user_tz":-60},"id":"aukYaE0ZoAnp","outputId":"64d721aa-1802-4669-f32e-b49e2fc3e073"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time (Seconds) ________________ 69.69603300094604\n","___\n","              precision    recall  f1-score   support\n","\n","       False       0.29      0.31      0.30        39\n","        True       0.53      0.51      0.52        61\n","\n","    accuracy                           0.43       100\n","   macro avg       0.41      0.41      0.41       100\n","weighted avg       0.44      0.43      0.43       100\n","\n"]}],"source":["get_test_scores(bert, quantize_dataloader, args , quantize_y)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"wnm2d8dqts_F"},"outputs":[],"source":[""]}]}