{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMLD_WORKSHOP_TRANSFER_NLP_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GAnjZSHn8D1_"
      },
      "source": [
        "# Install the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93qbDScKCWti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "592b03b4-654f-479b-a00b-78b0547a9028"
      },
      "source": [
        "!pip install pytorch_pretrained_bert pytorch-nlp\n",
        "!pip install tokenizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 33.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.3MB/s \n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
            "\r\u001b[K     |███▋                            | 10kB 35.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 20kB 40.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 46.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 40kB 52.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 51kB 56.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 61kB 60.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 71kB 63.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 81kB 64.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.10.47)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.3.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.13.47)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert, pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2\n",
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/d8/5af89a486513ef9c1d1f7a8ad6231f3d20e2015fc84533ff7d25005c8717/tokenizers-0.2.1-cp36-cp36m-manylinux1_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymq6bgJVCWtp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "93bf7a82-56b1-443e-9baf-ce8b51147aeb"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sayankotor/BERT_botcamp19/master/bert-base-uncased-vocab.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-26 08:50:23--  https://raw.githubusercontent.com/sayankotor/BERT_botcamp19/master/bert-base-uncased-vocab.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: ‘bert-base-uncased-vocab.txt’\n",
            "\n",
            "\r          bert-base   0%[                    ]       0  --.-KB/s               \rbert-base-uncased-v 100%[===================>] 226.08K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-01-26 08:50:23 (5.51 MB/s) - ‘bert-base-uncased-vocab.txt’ saved [231508/231508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b5chcV-nSXF-"
      },
      "source": [
        "# Check hardware specs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rUqn7CxCWtw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "f6ce4115-ef2b-4937-ebe5-b45244a31c60"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 26 08:50:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mfYZBx9MhqA-",
        "outputId": "2452536d-f8db-4bad-b308-0264f8621ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!lscpu |grep 'Model name'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yhu_Fkgphprq",
        "outputId": "db4c676e-6439-4ca5-eab4-d177ec5076c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!lscpu | grep 'Thread'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread(s) per core:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lh_W-rYlSbjj"
      },
      "source": [
        "# Finetuning Hands-on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vWWepa2JeXOP",
        "outputId": "4174f585-95b2-406c-b7fd-0ae6611170e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8tm6B20MEK3I",
        "colab": {}
      },
      "source": [
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "torch.cuda.manual_seed(10)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y7V8wid9tgh5"
      },
      "source": [
        "## Parameter description\n",
        "\n",
        "Here are some of the main parameters you will want to consider when finetuning BERT:\n",
        "\n",
        "* **Gradient Clipping**: If the norm of the gradient gets above max_grad_norm, We divide the gradient by its L2 norm. This gradient clipping method avoids exploiding gradients.\n",
        "* **Learning rate**: The `learning_rate` parameter is very important as it controls how we update the already trained parameters from the Language Modelling Task. If this parameter is too high, we will notice a forgetting of the previous task. It needs to be carefully tuned.\n",
        "* **Sequence length**: The attention mechanism scales in O(L^2). So you should avoid handling sequences larger than what you really need. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ3qmcecqXpd",
        "colab": {}
      },
      "source": [
        "@dataclass\n",
        "class ArgsBert:\n",
        "    max_seq_length: int = 256 # The maximum total input sequence length after WordPiece tokenization.\n",
        "    learning_rate: float = 3e-6 # Initial Learning rate for Adam\n",
        "    num_train_epochs: int = 3 # epochs\n",
        "    batch_size: int = 4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    loss = torch.nn.BCEWithLogitsLoss().cuda()\n",
        "    clip_gradient_max_norm: float = 1.0 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PaCu4-GTryN5",
        "colab": {}
      },
      "source": [
        "args = ArgsBert()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oJeo9Iqg6-u5"
      },
      "source": [
        "## Preparing the Data\n",
        "\n",
        "The dataset we will be using is the IMDB movie review sentiment analysis dataset. The data looks like this:\n",
        "\n",
        "  - **Review**: 'This movie caught me by surprise . For years I have avoided many of Harold Lloyd \\'s sound pictures ( as well as those of Keaton ) because they have a generally well - deserved reputation for being lousy compared to the silent films because the basic formula has been lost . However , when I saw this film I was pleasantly surprised to find I actually liked it, ... \n",
        "\n",
        "  - **Sentiment**: \"pos\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gsESIItjqd5Q",
        "colab": {}
      },
      "source": [
        "TEXT = torchtext.data.Field(tokenize = 'spacy', include_lengths = True) # helper to tokenize using spacy\n",
        "LABEL = torchtext.data.LabelField(dtype = torch.float)\n",
        "\n",
        "def get_dataloader_bert(tokens_ids, masks, lbls, random=True, batch_size=64):\n",
        "    \"\"\"\"\"\n",
        "    Returns a dataloader to iterate over the data. \n",
        "    Arguments:\n",
        "    - tokens_ids:\n",
        "    - masks: \n",
        "    - lbls: \n",
        "    \"\"\"\"\"\n",
        "    tokens_tensor = torch.tensor(tokens_ids)\n",
        "    y_tensor = torch.tensor(lbls.reshape(-1, 1)).float()\n",
        "    masks_tensor = torch.tensor(masks)\n",
        "\n",
        "    dataset = TensorDataset(tokens_tensor, masks_tensor, y_tensor)\n",
        "    if random:\n",
        "      sampler = RandomSampler(dataset)\n",
        "      dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
        "    else:\n",
        "      sampler = SequentialSampler(dataset)\n",
        "      dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def fetch_imdb_data():\n",
        "    \"\"\"\"\"\n",
        "    Returns the imdb dataset\n",
        "    \"\"\"\"\"\n",
        "    full_train_data, val_data_ = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
        "    return full_train_data, val_data_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dSNrqCtKeca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a4b523f-be18-4811-ee49-bd1ca13edc30"
      },
      "source": [
        "# Download the IMDB data\n",
        "full_train_data, full_test_data = fetch_imdb_data()\n",
        "\n",
        "# We randomly subsample the IMDB dataset (the training would take too long with the full dataset)\n",
        "train_data = [full_train_data[random.randint(1,24000)] for _ in range(1000)]\n",
        "test_data = [full_test_data[random.randint(1,24000)] for _ in range(3000)]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 37.0MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qXx7pcXBgj_3",
        "colab": {}
      },
      "source": [
        "bert_train_texts = list(\" \".join(train_data[i].text) for i in range(len(train_data))) \n",
        "train_labels = list(train_data[i].label for i in range(len(train_data)))\n",
        "\n",
        "bert_test_texts = list(\" \".join(test_data[i].text) for i in range(len(test_data)))\n",
        "test_labels = list(test_data[i].label for i in range(len(test_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6wDt75dgr6e",
        "colab": {}
      },
      "source": [
        "train_data = torchtext.data.Dataset(train_data, full_train_data.fields)\n",
        "test_data = torchtext.data.Dataset(test_data, full_test_data.fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hvBFcDbb7B-q"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "- The input to the bert model are word-pieces ([Original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)). Standard tokens are broken down into word pieces through the use of a WordPiece tokenizer. \n",
        "\n",
        "- A WordPiece tokenizer breaks the unknown words into multiple subwords.\n",
        "For example, if the word \"chore\" does not belong to the vocabulary as a single piece, it might get split into two pieces belonging to the vocabulary: 'cho' and '##re'. \n",
        "\n",
        "- All the subwords start with the \"#\" symbol except for the first subword in the word. Imagine the words \"played\", \"playing\" are rare words and thus would not occur in a normal vocabulary. These words would be considered into the wordpiece tokenizer into this form: [`play`, `##ed`] and [`play`, `##ing`]. \n",
        "\n",
        "- You can have a look at the file `bert-base-uncased-vocab.txt` in your environnment to have an idea of the words present in the vocabular\n",
        "\n",
        "- Wordpiece tokenizers tends to be quite slow, however some efficient implementations exist: tokenizers from the [huggingface Library](https://github.com/huggingface/tokenizers) are much faster than the standard naive implementations (Implemented in Rust with python bindings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CaND54ZWguEV",
        "outputId": "d4803d33-cab7-40e1-8eb5-e9e350f0c071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Here is an example of splitting a rare token into wordpieces !\n",
        "tokenizer.tokenize(\"supercalifragilisticexpialidocious\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2716710.82B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['super',\n",
              " '##cal',\n",
              " '##if',\n",
              " '##rag',\n",
              " '##ilis',\n",
              " '##tic',\n",
              " '##ex',\n",
              " '##pia',\n",
              " '##lid',\n",
              " '##oc',\n",
              " '##ious']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rfjikrg-gvkV",
        "colab": {}
      },
      "source": [
        "train_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  bert_train_texts] # tokenize reviews in train\n",
        "test_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  bert_test_texts] # tokenize reviews in test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wjM4OJE97Fdy"
      },
      "source": [
        "## Padding and preparing tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ED0pVgaNxulH",
        "colab": {}
      },
      "source": [
        "bert_train_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in train_tokens] # wordpieces to ids\n",
        "bert_test_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in test_tokens] # wordpieces to ids\n",
        "\n",
        "# Pad up to max_seq_length\n",
        "bert_train_tokens_ids = pad_sequences(bert_train_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "bert_test_tokens_ids = pad_sequences(bert_test_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DOLzB86lg9qU",
        "colab": {}
      },
      "source": [
        "train_y = np.array(train_labels) == 'pos' # gives a vector of bool [True, False, False, ...]\n",
        "test_y = np.array(test_labels) == 'pos'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "peapnkUWg_1j",
        "colab": {}
      },
      "source": [
        "# Attention masking for not attending padded tokens\n",
        "bert_train_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in bert_train_tokens_ids]\n",
        "bert_test_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in bert_test_tokens_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Doa9zFRexz0"
      },
      "source": [
        "- Below is the output of the preprocessing (tokenization, padding etc). You can see that we have a tensor of IDs for the first sentence pointing to our vocabulary.\n",
        "- The first id is always 101 refering to the **[CLS]** token in the vocabulary.\n",
        "- The padding is done by the id=0 corresponding to **[PAD]** token in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q-xoaVDjeukE",
        "outputId": "93c5f7d9-d017-4788-9a82-e17abbbaad65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "bert_train_tokens_ids[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  101,  1045,  2323,  2031,  6618,  2008,  2151,  3185,  2007,\n",
              "        1996, 14955,  3334,  3351,  2923,  3203,  1999,  2009,  2003,\n",
              "        1050,  1005,  1056,  2183,  2000,  2022,  2204,  1012,  2009,\n",
              "        2941,  4627,  2041,  3100,  1010,  2021,  2076,  1996,  2034,\n",
              "        4028,  3496,  2017,  2424,  2041,  2008,  1996,  3185,  2017,\n",
              "        1005,  2128,  3666,  2003,  1037,  3185,  2503,  1997,  1037,\n",
              "        3185,  1012,  2045,  1005,  1055,  2111,  3564,  1999,  1037,\n",
              "        3185,  3004,  3666,  2008,  3185,  1012,  2028,  2611,  1999,\n",
              "        1996,  4378,  2003,  2061, 15703,  2008,  1045,  2052,  2031,\n",
              "        2357,  2105,  1998, 21384,  2014,  1012,  1037,  2978,  4326,\n",
              "        1010,  2021,  2521,  2013,  2204,  1012,   102,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SUNdg7WnfxAQ"
      },
      "source": [
        "- The labels are presented below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fkiftRwWfK3L",
        "outputId": "724070e8-3502-42b1-b281-d5f6e24f16f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_y[0:5]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False,  True,  True,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GkcdTBuG7JzH"
      },
      "source": [
        "## Finetuning Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G1vZ6NcD8yTs"
      },
      "source": [
        "- Let's define our model `BertFinetune`\n",
        "  - This model will be composed of the uncased version of bert + a dense layer on top of it for binary classification.\n",
        "  \n",
        "- Here are the steps for finetuning:\n",
        "  - We load our pretrained LM transformer model with already trained weights\n",
        "  - Add a new linear layer on top of the trained model\n",
        "  - Finetune the parameters of the newly defined model on the downstream task\n",
        "  - Evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BxxqyTQPhBUc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae13c1e1-d744-4a31-dc33-151dad75f754"
      },
      "source": [
        "class BertFinetune(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertFinetune, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.linear = torch.nn.Linear(768, 1)\n",
        "    \n",
        "    def forward(self, tokens, masks):\n",
        "        # pooled_output will just consider the hidden state of the first token (i.e., the [CLS])\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "        logits = self.linear(pooled_output)\n",
        "        return logits, pooled_output\n",
        "\n",
        "bert = BertFinetune()\n",
        "bert = bert.cuda() # We push our model on the GPU!"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:05<00:00, 72955131.52B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ewHHvRFhdqA",
        "colab": {}
      },
      "source": [
        "train_dataloader = get_dataloader_bert(bert_train_tokens_ids, bert_train_masks, train_y, batch_size=args.batch_size)\n",
        "test_dataloader = get_dataloader_bert(bert_test_tokens_ids, bert_test_masks, test_y, random=False, batch_size=args.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8aORee7EVI6W"
      },
      "source": [
        "- To have an idea of the form of pooled output, its shape is [4, 768] which is the [CLS] hidden states for the 4 sentences in the batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cxNxLbx5VHGY",
        "outputId": "7fa2bb5c-f860-4233-b1f1-baee014982a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for data in train_dataloader: # Just doing one forward pass to check the output of our BERT based network\n",
        "  token_ids, masks, labels = tuple(t.to(args.device) for t in data)\n",
        "  pb, cls = bert(token_ids, masks)\n",
        "  print(cls.shape)\n",
        "  break"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oD03Rf8ZXdA3"
      },
      "source": [
        "**Finally! Let's fine-tune our model !**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OzKrhav6him6",
        "outputId": "18655f02-54b6-4cd0-fba3-f7d7f95a2752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "optimizer = torch.optim.Adam(bert.parameters(), lr=args.learning_rate)\n",
        "\n",
        "losses = []\n",
        "bert.train()\n",
        "\n",
        "for epoch in range(args.num_train_epochs):\n",
        "    train_loss = 0\n",
        "    for step_num, data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(args.device) for t in data) # Moving the input tensors to the GPU\n",
        "\n",
        "        logits, cls = bert(token_ids, masks) # Forward pass\n",
        "        loss = args.loss(logits, labels) # compute our classification loss\n",
        "\n",
        "        train_loss += loss.item()  \n",
        "        \n",
        "        bert.zero_grad()\n",
        "        loss.backward() \n",
        "        clip_grad_norm_(parameters=bert.parameters(), max_norm=args.clip_gradient_max_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(f'Epoch: {epoch}  --- loss:  {train_loss/(step_num + 1)}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0  --- loss:  0.6436396466493607\n",
            "Epoch: 1  --- loss:  0.4031721138656139\n",
            "Epoch: 2  --- loss:  0.2642683334052563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RamCUV_WoTUb",
        "outputId": "30cd9bd9-230b-4fde-aca2-c5d226b763d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 26 09:05:11 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0    43W /  70W |   5821MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hpvlOi7Jhl0n",
        "outputId": "b8a5d794-6db3-4697-cb7b-41dbd73b5d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "def get_test_scores(model, test_dataloader, args, test_y):\n",
        "  s = time.time()\n",
        "  model.eval()\n",
        "  model.to(args.device) # make sure the model is on the right device\n",
        "  bert_predicted = []\n",
        "  with torch.no_grad(): # no need for gradient computation for a simple eval\n",
        "      for batch_data in test_dataloader:\n",
        "          token_ids, masks, labels = tuple(t.to(args.device) for t in batch_data) # sending tensor to the right device\n",
        "          logits, _ = model(token_ids, masks) # Forward pass with our BERT model\n",
        "          bert_predicted += list(torch.sigmoid(logits.cpu().detach()[:, 0]).numpy() > 0.5) # Get predictions\n",
        "  model.train()\n",
        "\n",
        "  print(\"Time (Seconds) ________________\", (time.time() - s))\n",
        "  print(\"___\")\n",
        "  print(classification_report(test_y, bert_predicted))\n",
        "\n",
        "get_test_scores(bert, test_dataloader, args, test_y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (Seconds) ________________ 57.489317178726196\n",
            "___\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.89      0.87      0.88      1466\n",
            "        True       0.88      0.89      0.89      1534\n",
            "\n",
            "    accuracy                           0.88      3000\n",
            "   macro avg       0.88      0.88      0.88      3000\n",
            "weighted avg       0.88      0.88      0.88      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "50OojoeKuxWK"
      },
      "source": [
        "# Distillation Hands-on\n",
        "\n",
        "As we saw in the slides, distillation can be done using the logits of the teacher and the student. In our case the teacher is our fine-tuned BERT model. The following function is computing the logits on the training set. We will use those logits to do the distillation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L9vq1zpuGdNk",
        "colab": {}
      },
      "source": [
        "def get_training_logits(bert, bert_train_tokens_ids, bert_train_masks, train_y, args):\n",
        "  \"\"\"\"\"\n",
        "  Function to get the training logits from the already trained bert model\n",
        "  \"\"\"\"\"\n",
        "  bert.eval()\n",
        "  all_logits = []\n",
        "  train_logits_loader = get_dataloader_bert(bert_train_tokens_ids, bert_train_masks, train_y, random=False, batch_size=args.batch_size)\n",
        "  lbls = []\n",
        "  with torch.no_grad():\n",
        "      for batch in train_logits_loader:\n",
        "          token_ids, masks, labels = tuple(t.to(args.device) for t in batch)\n",
        "          log, _ = bert(token_ids, masks)\n",
        "          all_logits.extend(torch.flatten(log).cpu().numpy())\n",
        "          lbls.extend(torch.flatten(labels).cpu().numpy())\n",
        "  bert.train()\n",
        "  return torch.Tensor(tuple(all_logits)), torch.Tensor(tuple(lbls))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eD4SwhBWmE5-",
        "colab": {}
      },
      "source": [
        "all_logits, lbls = get_training_logits(bert, bert_train_tokens_ids, bert_train_masks, train_y, args)\n",
        "logits_loader = DataLoader(all_logits, batch_size=64, shuffle=False) # Dataloader for logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gk4-6VFum2IR",
        "outputId": "d5f1fb3a-fac4-470f-ec90-7d7f6d50fcc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_logits.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VoVrtZiQ-YxE"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AtPYjAkM5l2J",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_data, max_size = 25000)\n",
        "LABEL.build_vocab(train_data,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zBi0airszPi1",
        "colab": {}
      },
      "source": [
        "@dataclass\n",
        "class ArgsClf:\n",
        "    num_train_epochs: int = 5 # epochs\n",
        "    batch_size: int = 64 \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    loss = torch.nn.BCELoss()\n",
        "    clip_gradient_max_norm: float = 5.0\n",
        "    learning_rate=0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CfQuWkvZrrvh",
        "colab": {}
      },
      "source": [
        "args_clf = ArgsClf()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sqb041jt5mOA",
        "colab": {}
      },
      "source": [
        "train_iterator= torchtext.data.BucketIterator(\n",
        "    train_data, \n",
        "    sort_key=None,\n",
        "    shuffle=False,\n",
        "    batch_size = args_clf.batch_size,\n",
        "    device = args_clf.device)\n",
        "\n",
        "test_iterator= torchtext.data.BucketIterator(\n",
        "    test_data, \n",
        "    sort_key=None,\n",
        "    shuffle=False,\n",
        "    batch_size = args_clf.batch_size,\n",
        "    device = args_clf.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fbE81390-XDO"
      },
      "source": [
        "## Model Definition\n",
        "\n",
        "- Very simple model with 1 embedding layer and a Linear layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q-zMNujj5mWW",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BinaryCLF(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "    def forward(self, text):        \n",
        "        embedded = self.embedding(text).permute(1, 0, 2)        \n",
        "        return self.fc(F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "yU1LqSLKk6x3",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "##### Uncomment if you want to get all the logits for the 25k datapoints #####\n",
        "\n",
        "def get_full_logits():\n",
        "  \"\"\"\"\"\n",
        "  Returns the logits on the full IMDB Dataset\n",
        "  Can be used for data augmentation and distillation\n",
        "  \"\"\"\"\"\n",
        "  # format data input \n",
        "  train_texts_full = list(\" \".join(full_train_data[i].text) for i in range(len(full_train_data))) # get the sentences\n",
        "  train_tokens_ids_full = list(tokenizer.encode(t) for t in train_texts_full) # use wordpiece tokenizer  train_tokens_ids_full = [tok.ids[:args.max_seq_length-1] for tok in train_tokens_ids_full] # truncate to max_seq_length\n",
        "  train_tokens_ids_full = pad_sequences(train_tokens_ids_full, maxlen=args.max_seq_length, \n",
        "                                        truncating=\"post\", padding=\"post\", dtype=\"int\") # pad sequences\n",
        "  train_masks_full = [[float(i > 0) for i in ii] for ii in train_tokens_ids_full] \n",
        "  train_masks_tensor_full = torch.tensor(train_tokens_ids_full)\n",
        "  train_masks_tensor_full = torch.tensor(train_masks_full)\n",
        "  dataset = TensorDataset(torch.tensor(train_tokens_ids_full), train_masks_tensor_full)\n",
        "  sampler = SequentialSampler(dataset)\n",
        "  dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
        "  bert.eval()\n",
        "  bert.cuda()\n",
        "  all_logits = []\n",
        "  with torch.no_grad():\n",
        "      for batch in dataloader:\n",
        "          token_ids, masks = tuple(t.to(args.device) for t in batch)\n",
        "          log, _ = bert(token_ids, masks)\n",
        "          all_logits.extend(torch.flatten(log).cpu().numpy())\n",
        "\n",
        "  return torch.Tensor(tuple(all_logits))\n",
        "\n",
        "#logits = get_full_logits()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bfNc_QzXlC7y"
      },
      "source": [
        "## Helper Distillation\n",
        "\n",
        "- We provided for you the bert logits for the training set iterator, `train_iterator` which are in the `logits_loader`\n",
        "- We also provided a function `get_full_logits` that you can use to get the logits on all the training set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mYLKK3G5uQm",
        "outputId": "32e91c24-488f-415a-9a4d-7b66d16e50ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "net = BinaryCLF(vocab_size=len(TEXT.vocab), embedding_dim=100, output_dim=1, pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "\n",
        "#criterion = torch.nn.BCELoss()\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# note the torch module need extra brackets above\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=args_clf.learning_rate)\n",
        "\n",
        "net.cuda()\n",
        "net.train()\n",
        "\n",
        "alpha = 1 # alpha = 1 might be best.\n",
        "\n",
        "for epoch in range(10):\n",
        "    train_loss=0\n",
        "    step_num=0\n",
        "    for batch, logits_bert in zip(train_iterator, logits_loader):\n",
        "        net.zero_grad()\n",
        "        logits_bert = torch.tensor(logits_bert).squeeze().to(\"cuda\")\n",
        "        output = net(batch.text[0]).squeeze(1).to(\"cuda\")\n",
        "        ############# IMPLEMENT DISTILLATION HERE ##################\n",
        "        ############################################################\n",
        "        # Use logits_bert\n",
        "        #print(output.shape)\n",
        "        #print(logits_bert.shape)\n",
        "        #loss = criterion(output,logits_bert) \n",
        "        # could have also added .cuda() after logits_bert\n",
        "\n",
        "        # full answer would have been:\n",
        "\n",
        "        loss = (1-alpha)*torch.nn.BCELoss()(torch.sigmoid(output), batch.label.float()) + alpha*torch.nn.MSELoss()(output,logits_bert)\n",
        "\n",
        "        \n",
        "        # tensors needed placed on cuda\n",
        "\n",
        "        ############# IMPLEMENT DISTILLATION HERE ##################\n",
        "        ############################################################\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), args_clf.clip_gradient_max_norm)\n",
        "        optimizer.step()\n",
        "        step_num+=1\n",
        "    print(f'Epoch: {epoch}  --- loss:  {train_loss/(step_num + 1)}')\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0  --- loss:  12.582270229563994\n",
            "Epoch: 1  --- loss:  12.204035085790297\n",
            "Epoch: 2  --- loss:  11.330210854025449\n",
            "Epoch: 3  --- loss:  9.701850526473102\n",
            "Epoch: 4  --- loss:  7.702196654151468\n",
            "Epoch: 5  --- loss:  6.04255186810213\n",
            "Epoch: 6  --- loss:  4.796094824286068\n",
            "Epoch: 7  --- loss:  3.8621275354834164\n",
            "Epoch: 8  --- loss:  3.176787166034474\n",
            "Epoch: 9  --- loss:  2.6148606293341694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sxwH8mFd8Foo",
        "outputId": "202b801b-61f6-439f-f08d-ee632c52013e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "def get_test_score_binaryclf(model, test_iterator, args):\n",
        "  s = time.time()\n",
        "  model.eval()\n",
        "  model.to(args.device)\n",
        "  binary_clf_predicted = []\n",
        "  with torch.no_grad():\n",
        "      for batch in test_iterator:\n",
        "          pb = model(batch.text[0].to(\"cuda\")).squeeze(1)\n",
        "          binary_clf_predicted += list(torch.sigmoid(pb.cpu().detach()).numpy() > 0.5)\n",
        "\n",
        "  print(\"___\")\n",
        "  print(\"Time (in seconds) ________________\",time.time() - s)\n",
        "  print(\"___\")\n",
        "  print(classification_report(test_y, binary_clf_predicted))\n",
        "\n",
        "get_test_score_binaryclf(net, test_iterator, args_clf)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "___\n",
            "Time (in seconds) ________________ 0.5225231647491455\n",
            "___\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.88      0.59      0.71      1466\n",
            "        True       0.70      0.92      0.80      1534\n",
            "\n",
            "    accuracy                           0.76      3000\n",
            "   macro avg       0.79      0.76      0.75      3000\n",
            "weighted avg       0.79      0.76      0.75      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YlHQ62VPN_Kx"
      },
      "source": [
        "# Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "deWAkK0L5_KE",
        "colab": {}
      },
      "source": [
        "import torch.quantization\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    \"\"\"\"\"\n",
        "    Get the size on disk of the model\n",
        "    \"\"\"\"\"\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "\n",
        "bert.eval()\n",
        "bert.cpu()\n",
        "\n",
        "# As simple as that:\n",
        "quantized_bert = torch.quantization.quantize_dynamic(\n",
        "    bert, {nn.Linear}, dtype=torch.qint8\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5AbzSuMj2fNK"
      },
      "source": [
        "- Let's see the size on disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t4h1W1Lg3H72",
        "outputId": "ee2cb60d-8377-4ae7-f4c4-50be4a87e848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print_size_of_model(quantized_bert)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size (MB): 181.426086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z3dBvQnPxTLt",
        "outputId": "af4412bf-bfa0-4795-ea2d-849d49d53979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print_size_of_model(bert)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size (MB): 437.977469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TnvGJZXLoCP9"
      },
      "source": [
        "- Let's make a small dataset to compare the speed and the accuracy of both models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VJJBRk1Wf-fC",
        "colab": {}
      },
      "source": [
        "data_quantization_benchmark = [full_train_data[random.randint(1,24000)] for _ in range(100)]\n",
        "\n",
        "quantize_texts = list(\" \".join(data_quantization_benchmark[i].text) for i in range(len(data_quantization_benchmark))) \n",
        "quantize_labels = list(data_quantization_benchmark[i].label for i in range(len(data_quantization_benchmark)))\n",
        "quantize_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  quantize_texts] # tokenize reviews in quantization dataset\n",
        "\n",
        "quantize_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in quantize_tokens] # wordpieces to ids\n",
        "quantize_tokens_ids = pad_sequences(quantize_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "quantize_labels = list(data_quantization_benchmark[i].label for i in range(len(data_quantization_benchmark)))\n",
        "\n",
        "quantize_y = np.array(quantize_labels) == 'pos'\n",
        "quantize_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in quantize_tokens_ids]\n",
        "\n",
        "quantize_dataloader = get_dataloader_bert(quantize_tokens_ids, quantize_masks, quantize_y, batch_size=args.batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ajr3uPTVnzFQ",
        "outputId": "fce632a2-f079-4dc6-908d-c6b732eca8ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "args.device=\"cpu\"\n",
        "get_test_scores(quantized_bert, quantize_dataloader, args, quantize_y)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (Seconds) ________________ 61.065494775772095\n",
            "___\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.45      0.44      0.44        39\n",
            "        True       0.65      0.66      0.65        61\n",
            "\n",
            "    accuracy                           0.57       100\n",
            "   macro avg       0.55      0.55      0.55       100\n",
            "weighted avg       0.57      0.57      0.57       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aukYaE0ZoAnp",
        "outputId": "5922dbdf-63bb-4a64-c3b6-2b62800464bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "get_test_scores(bert, quantize_dataloader, args , quantize_y)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time (Seconds) ________________ 78.53542280197144\n",
            "___\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.33      0.36      0.35        39\n",
            "        True       0.57      0.54      0.55        61\n",
            "\n",
            "    accuracy                           0.47       100\n",
            "   macro avg       0.45      0.45      0.45       100\n",
            "weighted avg       0.48      0.47      0.47       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wnm2d8dqts_F",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}