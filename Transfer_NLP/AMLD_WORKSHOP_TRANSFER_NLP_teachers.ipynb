{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AMLD_WORKSHOP_TRANSFER_NLP.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GAnjZSHn8D1_","colab_type":"text"},"source":["# Install the necessary packages"]},{"cell_type":"code","metadata":{"id":"gTsZLZ-IdsAG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":360},"outputId":"e10c5f4e-7a82-48e8-8997-27df6b3655e9","executionInfo":{"status":"ok","timestamp":1579990398841,"user_tz":-60,"elapsed":7283,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["!pip install pytorch_pretrained_bert pytorch-nlp\n","!pip install tokenizers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n","Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.10.47)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.3.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.13.47)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (2.6.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.47->boto3->pytorch_pretrained_bert) (1.12.0)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.2.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F5nrwFSvxcmW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"58597d17-5a7e-410c-b8f9-48f97fb42192","executionInfo":{"status":"ok","timestamp":1579990400181,"user_tz":-60,"elapsed":8613,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["!wget https://raw.githubusercontent.com/sayankotor/BERT_botcamp19/master/bert-base-uncased-vocab.txt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["--2020-01-25 22:13:18--  https://raw.githubusercontent.com/sayankotor/BERT_botcamp19/master/bert-base-uncased-vocab.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 231508 (226K) [text/plain]\n","Saving to: ‘bert-base-uncased-vocab.txt.2’\n","\n","\r          bert-base   0%[                    ]       0  --.-KB/s               \rbert-base-uncased-v 100%[===================>] 226.08K  --.-KB/s    in 0.04s   \n","\n","2020-01-25 22:13:19 (5.52 MB/s) - ‘bert-base-uncased-vocab.txt.2’ saved [231508/231508]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b5chcV-nSXF-","colab_type":"text"},"source":["# Check hardware specs"]},{"cell_type":"code","metadata":{"id":"MBmsGZKESEtC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"4e4f8d81-4a45-4820-cbcd-5fdaedc78044","executionInfo":{"status":"ok","timestamp":1579990401506,"user_tz":-60,"elapsed":9931,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Sat Jan 25 22:13:20 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   52C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mfYZBx9MhqA-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"064b252f-02b1-450c-ac24-d716ad929469","executionInfo":{"status":"ok","timestamp":1579990402833,"user_tz":-60,"elapsed":11250,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["!lscpu |grep 'Model name'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yhu_Fkgphprq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ec468938-9f4b-418e-d266-ed984e325a86","executionInfo":{"status":"ok","timestamp":1579990404013,"user_tz":-60,"elapsed":12421,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["!lscpu | grep 'Thread'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Thread(s) per core:  2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lh_W-rYlSbjj","colab_type":"text"},"source":["# Finetuning Hands-on"]},{"cell_type":"code","metadata":{"id":"vWWepa2JeXOP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"a21c6ce6-db41-4d21-d83d-0aa6901ae653","executionInfo":{"status":"ok","timestamp":1579990406011,"user_tz":-60,"elapsed":14409,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["import torch\n","import torchtext\n","import random\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","import torch.nn.functional as F\n","from dataclasses import dataclass\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from pytorch_pretrained_bert import BertModel, BertTokenizer\n","import os\n","import time\n","import random\n","from torch.nn.utils import clip_grad_norm_\n","from sklearn.metrics import classification_report"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"8tm6B20MEK3I","colab_type":"code","colab":{}},"source":["random.seed(10)\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed(10)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7V8wid9tgh5","colab_type":"text"},"source":["## Parameter description\n","\n","Here are some of the main parameters you will want to consider when finetuning BERT:\n","\n","* **Gradient Clipping**: If the norm of the gradient gets above max_grad_norm, We divide the gradient by its L2 norm. This gradient clipping method avoids exploiding gradients.\n","* **Learning rate**: The `learning_rate` parameter is very important as it controls how we update the already trained parameters from the Language Modelling Task. If this parameter is too high, we will notice a forgetting of the previous task. It needs to be carefully tuned.\n","* **Sequence length**: The attention mechanism scales in O(L^2). So you should avoid handling sequences larger than what you really need. "]},{"cell_type":"code","metadata":{"id":"yJ3qmcecqXpd","colab_type":"code","colab":{}},"source":["@dataclass\n","class ArgsBert:\n","    max_seq_length: int = 256 # The maximum total input sequence length after WordPiece tokenization.\n","    learning_rate: float = 3e-6 # Initial Learning rate for Adam\n","    num_train_epochs: int = 3 # epochs\n","    batch_size: int = 4\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    loss = torch.nn.BCEWithLogitsLoss().cuda()\n","    clip_gradient_max_norm: float = 1.0 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PaCu4-GTryN5","colab_type":"code","colab":{}},"source":["args = ArgsBert()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJeo9Iqg6-u5","colab_type":"text"},"source":["## Preparing the Data\n","\n","The dataset we will be using is the IMDB movie review sentiment analysis dataset. The data looks like this:\n","\n","  - **Review**: 'This movie caught me by surprise . For years I have avoided many of Harold Lloyd \\'s sound pictures ( as well as those of Keaton ) because they have a generally well - deserved reputation for being lousy compared to the silent films because the basic formula has been lost . However , when I saw this film I was pleasantly surprised to find I actually liked it, ... \n","\n","  - **Sentiment**: \"pos\""]},{"cell_type":"code","metadata":{"id":"gsESIItjqd5Q","colab_type":"code","colab":{}},"source":["TEXT = torchtext.data.Field(tokenize = 'spacy', include_lengths = True) # helper to tokenize using spacy\n","LABEL = torchtext.data.LabelField(dtype = torch.float)\n","\n","def get_dataloader_bert(tokens_ids, masks, lbls, random=True, batch_size=64):\n","    \"\"\"\"\"\n","    Returns a dataloader to iterate over the data. \n","    Arguments:\n","    - tokens_ids:\n","    - masks: \n","    - lbls: \n","    \"\"\"\"\"\n","    tokens_tensor = torch.tensor(tokens_ids)\n","    y_tensor = torch.tensor(lbls.reshape(-1, 1)).float()\n","    masks_tensor = torch.tensor(masks)\n","\n","    dataset = TensorDataset(tokens_tensor, masks_tensor, y_tensor)\n","    if random:\n","      sampler = RandomSampler(dataset)\n","      dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n","    else:\n","      sampler = SequentialSampler(dataset)\n","      dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","    return dataloader\n","\n","def fetch_imdb_data():\n","    \"\"\"\"\"\n","    Returns the imdb dataset\n","    \"\"\"\"\"\n","    full_train_data, val_data_ = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n","    return full_train_data, val_data_"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSNrqCtKeca4","colab_type":"code","colab":{}},"source":["# Download the IMDB data\n","full_train_data, full_test_data = fetch_imdb_data()\n","\n","# We randomly subsample the IMDB dataset (the training would take too long with the full dataset)\n","train_data = [full_train_data[random.randint(1,24000)] for _ in range(1000)]\n","test_data = [full_test_data[random.randint(1,24000)] for _ in range(3000)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qXx7pcXBgj_3","colab_type":"code","colab":{}},"source":["bert_train_texts = list(\" \".join(train_data[i].text) for i in range(len(train_data))) \n","train_labels = list(train_data[i].label for i in range(len(train_data)))\n","\n","bert_test_texts = list(\" \".join(test_data[i].text) for i in range(len(test_data)))\n","test_labels = list(test_data[i].label for i in range(len(test_data)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6wDt75dgr6e","colab_type":"code","colab":{}},"source":["train_data = torchtext.data.Dataset(train_data, full_train_data.fields)\n","test_data = torchtext.data.Dataset(test_data, full_test_data.fields)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvBFcDbb7B-q","colab_type":"text"},"source":["## Tokenization\n","\n","- The input to the bert model are word-pieces ([Original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)). Standard tokens are broken down into word pieces through the use of a WordPiece tokenizer. \n","\n","- A WordPiece tokenizer breaks the unknown words into multiple subwords.\n","For example, if the word \"chore\" does not belong to the vocabulary as a single piece, it might get split into two pieces belonging to the vocabulary: 'cho' and '##re'. \n","\n","- All the subwords start with the \"#\" symbol except for the first subword in the word. Imagine the words \"played\", \"playing\" are rare words and thus would not occur in a normal vocabulary. These words would be considered into the wordpiece tokenizer into this form: [`play`, `##ed`] and [`play`, `##ing`]. \n","\n","- You can have a look at the file `bert-base-uncased-vocab.txt` in your environnment to have an idea of the words present in the vocabular\n","\n","- Wordpiece tokenizers tends to be quite slow, however some efficient implementations exist: tokenizers from the [huggingface Library](https://github.com/huggingface/tokenizers) are much faster than the standard naive implementations (Implemented in Rust with python bindings)."]},{"cell_type":"code","metadata":{"id":"CaND54ZWguEV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"a6b5a254-bf60-4d6d-d31c-f6a3f61d8dad","executionInfo":{"status":"ok","timestamp":1579990490367,"user_tz":-60,"elapsed":98706,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","# Here is an example of splitting a rare token into wordpieces !\n","tokenizer.tokenize(\"supercalifragilisticexpialidocious\")"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['super',\n"," '##cal',\n"," '##if',\n"," '##rag',\n"," '##ilis',\n"," '##tic',\n"," '##ex',\n"," '##pia',\n"," '##lid',\n"," '##oc',\n"," '##ious']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"rfjikrg-gvkV","colab_type":"code","colab":{}},"source":["train_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  bert_train_texts] # tokenize reviews in train\n","test_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  bert_test_texts] # tokenize reviews in test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wjM4OJE97Fdy","colab_type":"text"},"source":["## Padding and preparing tensors"]},{"cell_type":"code","metadata":{"id":"ED0pVgaNxulH","colab_type":"code","colab":{}},"source":["bert_train_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in train_tokens] # wordpieces to ids\n","bert_test_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in test_tokens] # wordpieces to ids\n","\n","# Pad up to max_seq_length\n","bert_train_tokens_ids = pad_sequences(bert_train_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","bert_test_tokens_ids = pad_sequences(bert_test_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOLzB86lg9qU","colab_type":"code","colab":{}},"source":["train_y = np.array(train_labels) == 'pos' # gives a vector of bool [True, False, False, ...]\n","test_y = np.array(test_labels) == 'pos'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"peapnkUWg_1j","colab_type":"code","colab":{}},"source":["# Attention masking for not attending padded tokens\n","bert_train_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in bert_train_tokens_ids]\n","bert_test_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in bert_test_tokens_ids]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Doa9zFRexz0","colab_type":"text"},"source":["- Below is the output of the preprocessing (tokenization, padding etc). You can see that we have a tensor of IDs for the first sentence pointing to our vocabulary.\n","- The first id is always 101 refering to the **[CLS]** token in the vocabulary.\n","- The padding is done by the id=0 corresponding to **[PAD]** token in the vocabulary."]},{"cell_type":"code","metadata":{"id":"q-xoaVDjeukE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":510},"outputId":"e771c70e-85a1-42c6-e3bd-c5c0de40d0f3","executionInfo":{"status":"ok","timestamp":1579990503975,"user_tz":-60,"elapsed":112251,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["bert_train_tokens_ids[0]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  101,  1045,  2323,  2031,  6618,  2008,  2151,  3185,  2007,\n","        1996, 14955,  3334,  3351,  2923,  3203,  1999,  2009,  2003,\n","        1050,  1005,  1056,  2183,  2000,  2022,  2204,  1012,  2009,\n","        2941,  4627,  2041,  3100,  1010,  2021,  2076,  1996,  2034,\n","        4028,  3496,  2017,  2424,  2041,  2008,  1996,  3185,  2017,\n","        1005,  2128,  3666,  2003,  1037,  3185,  2503,  1997,  1037,\n","        3185,  1012,  2045,  1005,  1055,  2111,  3564,  1999,  1037,\n","        3185,  3004,  3666,  2008,  3185,  1012,  2028,  2611,  1999,\n","        1996,  4378,  2003,  2061, 15703,  2008,  1045,  2052,  2031,\n","        2357,  2105,  1998, 21384,  2014,  1012,  1037,  2978,  4326,\n","        1010,  2021,  2521,  2013,  2204,  1012,   102,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0,     0,     0,     0,     0,     0,\n","           0,     0,     0,     0])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"SUNdg7WnfxAQ","colab_type":"text"},"source":["- The labels are presented below"]},{"cell_type":"code","metadata":{"id":"fkiftRwWfK3L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c0b5eb48-27f5-4c9c-d656-69e2b0737465","executionInfo":{"status":"ok","timestamp":1579990503976,"user_tz":-60,"elapsed":112236,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["test_y[0:5]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([False,  True,  True,  True,  True])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"GkcdTBuG7JzH","colab_type":"text"},"source":["## Finetuning Bert"]},{"cell_type":"markdown","metadata":{"id":"G1vZ6NcD8yTs","colab_type":"text"},"source":["- Let's define our model `BertFinetune`\n","  - This model will be composed of the uncased version of bert + a dense layer on top of it for binary classification.\n","  \n","- Here are the steps for finetuning:\n","  - We load our pretrained LM transformer model with already trained weights\n","  - Add a new linear layer on top of the trained model\n","  - Finetune the parameters of the newly defined model on the downstream task\n","  - Evaluate\n"]},{"cell_type":"code","metadata":{"id":"BxxqyTQPhBUc","colab_type":"code","colab":{}},"source":["class BertFinetune(torch.nn.Module):\n","    def __init__(self):\n","        super(BertFinetune, self).__init__()\n","\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.linear = torch.nn.Linear(768, 1)\n","    \n","    def forward(self, tokens, masks):\n","        # pooled_output will just consider the hidden state of the first token (i.e., the [CLS])\n","        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n","        logits = self.linear(pooled_output)\n","        return logits, pooled_output\n","\n","bert = BertFinetune()\n","bert = bert.cuda() # We push our model on the GPU!"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ewHHvRFhdqA","colab_type":"code","colab":{}},"source":["train_dataloader = get_dataloader_bert(bert_train_tokens_ids, bert_train_masks, train_y, batch_size=args.batch_size)\n","test_dataloader = get_dataloader_bert(bert_test_tokens_ids, bert_test_masks, test_y, random=False, batch_size=args.batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8aORee7EVI6W","colab_type":"text"},"source":["- To have an idea of the form of pooled output, its shape is [4, 768] which is the [CLS] hidden states for the 4 sentences in the batch"]},{"cell_type":"code","metadata":{"id":"cxNxLbx5VHGY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"57cb7938-6b55-427b-e32b-47807eb22e48","executionInfo":{"status":"ok","timestamp":1579990513532,"user_tz":-60,"elapsed":121664,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["for data in train_dataloader: # Just doing one forward pass to check the output of our BERT based network\n","  token_ids, masks, labels = tuple(t.to(args.device) for t in data)\n","  pb, cls = bert(token_ids, masks)\n","  print(cls.shape)\n","  break"],"execution_count":23,"outputs":[{"output_type":"stream","text":["torch.Size([4, 768])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oD03Rf8ZXdA3","colab_type":"text"},"source":["**Finally! Let's fine-tune our model !**"]},{"cell_type":"code","metadata":{"id":"OzKrhav6him6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"24a11c37-c4ac-4650-f3e9-3f6b1d19c725","executionInfo":{"status":"ok","timestamp":1579990704966,"user_tz":-60,"elapsed":313074,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["optimizer = torch.optim.Adam(bert.parameters(), lr=args.learning_rate)\n","\n","losses = []\n","bert.train()\n","\n","for epoch in range(args.num_train_epochs):\n","    train_loss = 0\n","    for step_num, data in enumerate(train_dataloader):\n","        token_ids, masks, labels = tuple(t.to(args.device) for t in data) # Moving the input tensors to the GPU\n","\n","        logits, cls = bert(token_ids, masks) # Forward pass\n","        loss = args.loss(logits, labels) # compute our classification loss\n","\n","        train_loss += loss.item()  \n","        \n","        bert.zero_grad()\n","        loss.backward() \n","        clip_grad_norm_(parameters=bert.parameters(), max_norm=args.clip_gradient_max_norm)\n","\n","        optimizer.step()\n","        \n","    print(f'Epoch: {epoch}  --- loss:  {train_loss/(step_num + 1)}')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Epoch: 0  --- loss:  0.6436396466493607\n","Epoch: 1  --- loss:  0.4031721138656139\n","Epoch: 2  --- loss:  0.2642683334052563\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RamCUV_WoTUb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"8a3f9b4f-b441-45f9-d747-18f577280d49","executionInfo":{"status":"ok","timestamp":1579990706070,"user_tz":-60,"elapsed":314171,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["!nvidia-smi"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Sat Jan 25 22:18:25 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P0    42W /  70W |   5821MiB / 15079MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hpvlOi7Jhl0n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"bff37d52-c11e-46be-e88e-f26d27567325","executionInfo":{"status":"ok","timestamp":1579990762803,"user_tz":-60,"elapsed":370897,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["def get_test_scores(model, test_dataloader, args, test_y):\n","  s = time.time()\n","  model.eval()\n","  model.to(args.device) # make sure the model is on the right device\n","  bert_predicted = []\n","  with torch.no_grad(): # no need for gradient computation for a simple eval\n","      for batch_data in test_dataloader:\n","          token_ids, masks, labels = tuple(t.to(args.device) for t in batch_data) # sending tensor to the right device\n","          logits, _ = model(token_ids, masks) # Forward pass with our BERT model\n","          bert_predicted += list(torch.sigmoid(logits.cpu().detach()[:, 0]).numpy() > 0.5) # Get predictions\n","  model.train()\n","\n","  print(\"Time (Seconds) ________________\", (time.time() - s))\n","  print(\"___\")\n","  print(classification_report(test_y, bert_predicted))\n","\n","get_test_scores(bert, test_dataloader, args, test_y)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Time (Seconds) ________________ 56.39476752281189\n","___\n","              precision    recall  f1-score   support\n","\n","       False       0.89      0.87      0.88      1466\n","        True       0.88      0.89      0.89      1534\n","\n","    accuracy                           0.88      3000\n","   macro avg       0.88      0.88      0.88      3000\n","weighted avg       0.88      0.88      0.88      3000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"50OojoeKuxWK","colab_type":"text"},"source":["# Distillation Hands-on\n","\n","As we saw in the slides, distillation can be done using the logits of the teacher and the student. In our case the teacher is our fine-tuned BERT model. The following function is computing the logits on the training set. We will use those logits to do the distillation."]},{"cell_type":"code","metadata":{"id":"L9vq1zpuGdNk","colab_type":"code","colab":{}},"source":["def get_training_logits(bert, bert_train_tokens_ids, bert_train_masks, train_y, args):\n","  \"\"\"\"\"\n","  Function to get the training logits from the already trained bert model\n","  \"\"\"\"\"\n","  bert.eval()\n","  all_logits = []\n","  train_logits_loader = get_dataloader_bert(bert_train_tokens_ids, bert_train_masks, train_y, random=False, batch_size=args.batch_size)\n","  lbls = []\n","  with torch.no_grad():\n","      for batch in train_logits_loader:\n","          token_ids, masks, labels = tuple(t.to(args.device) for t in batch)\n","          log, _ = bert(token_ids, masks)\n","          all_logits.extend(torch.flatten(log).cpu().numpy())\n","          lbls.extend(torch.flatten(labels).cpu().numpy())\n","  bert.train()\n","  return torch.Tensor(tuple(all_logits)), torch.Tensor(tuple(lbls))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eD4SwhBWmE5-","colab_type":"code","colab":{}},"source":["all_logits, lbls = get_training_logits(bert, bert_train_tokens_ids, bert_train_masks, train_y, args)\n","logits_loader = DataLoader(all_logits, batch_size=64, shuffle=False) # Dataloader for logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gk4-6VFum2IR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"046fd879-00f2-48b8-e709-ea0a1ba955b1","executionInfo":{"status":"ok","timestamp":1579990781656,"user_tz":-60,"elapsed":389728,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["all_logits.shape"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1000])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"VoVrtZiQ-YxE","colab_type":"text"},"source":["## Prepare the data"]},{"cell_type":"code","metadata":{"id":"AtPYjAkM5l2J","colab_type":"code","colab":{}},"source":["TEXT.build_vocab(train_data, max_size = 25000)\n","LABEL.build_vocab(train_data,)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBi0airszPi1","colab_type":"code","colab":{}},"source":["@dataclass\n","class ArgsClf:\n","    num_train_epochs: int = 5 # epochs\n","    batch_size: int = 64 \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    loss = torch.nn.BCELoss()\n","    clip_gradient_max_norm: float = 5.0\n","    learning_rate=0.01"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfQuWkvZrrvh","colab_type":"code","colab":{}},"source":["args_clf = ArgsClf()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sqb041jt5mOA","colab_type":"code","colab":{}},"source":["train_iterator= torchtext.data.BucketIterator(\n","    train_data, \n","    sort_key=None,\n","    shuffle=False,\n","    batch_size = args_clf.batch_size,\n","    device = args_clf.device)\n","\n","test_iterator= torchtext.data.BucketIterator(\n","    test_data, \n","    sort_key=None,\n","    shuffle=False,\n","    batch_size = args_clf.batch_size,\n","    device = args_clf.device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbE81390-XDO","colab_type":"text"},"source":["## Model Definition\n","\n","- Very simple model with 1 embedding layer and a Linear layer"]},{"cell_type":"code","metadata":{"id":"q-zMNujj5mWW","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BinaryCLF(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","    def forward(self, text):        \n","        embedded = self.embedding(text).permute(1, 0, 2)        \n","        return self.fc(F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yU1LqSLKk6x3","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","##### Uncomment if you want to get all the logits for the 25k datapoints #####\n","\n","def get_full_logits():\n","  \"\"\"\"\"\n","  Returns the logits on the full IMDB Dataset\n","  Can be used for data augmentation and distillation\n","  \"\"\"\"\"\n","  # format data input \n","  train_texts_full = list(\" \".join(full_train_data[i].text) for i in range(len(full_train_data))) # get the sentences\n","  train_tokens_ids_full = list(tokenizer.encode(t) for t in train_texts_full) # use wordpiece tokenizer  train_tokens_ids_full = [tok.ids[:args.max_seq_length-1] for tok in train_tokens_ids_full] # truncate to max_seq_length\n","  train_tokens_ids_full = pad_sequences(train_tokens_ids_full, maxlen=args.max_seq_length, \n","                                        truncating=\"post\", padding=\"post\", dtype=\"int\") # pad sequences\n","  train_masks_full = [[float(i > 0) for i in ii] for ii in train_tokens_ids_full] \n","  train_masks_tensor_full = torch.tensor(train_tokens_ids_full)\n","  train_masks_tensor_full = torch.tensor(train_masks_full)\n","  dataset = TensorDataset(torch.tensor(train_tokens_ids_full), train_masks_tensor_full)\n","  sampler = SequentialSampler(dataset)\n","  dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n","  bert.eval()\n","  bert.cuda()\n","  all_logits = []\n","  with torch.no_grad():\n","      for batch in dataloader:\n","          token_ids, masks = tuple(t.to(args.device) for t in batch)\n","          log, _ = bert(token_ids, masks)\n","          all_logits.extend(torch.flatten(log).cpu().numpy())\n","\n","  return torch.Tensor(tuple(all_logits))\n","\n","#logits = get_full_logits()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bfNc_QzXlC7y","colab_type":"text"},"source":["## Helper Distillation\n","\n","- We provided for you the bert logits for the training set iterator, `train_iterator` which are in the `logits_loader`\n","- We also provided a function `get_full_logits` that you can use to get the logits on all the training set.\n","\n"]},{"cell_type":"code","metadata":{"id":"4mYLKK3G5uQm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"e2723880-fdb7-4b10-fb0f-25599dc5a6c5","executionInfo":{"status":"ok","timestamp":1579990783686,"user_tz":-60,"elapsed":391716,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["net = BinaryCLF(vocab_size=len(TEXT.vocab), embedding_dim=100, output_dim=1, pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n","\n","criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=args_clf.learning_rate)\n","\n","net.cuda()\n","net.train()\n","\n","for epoch in range(10):\n","    train_loss=0\n","    step_num=0\n","    for batch, logits_bert in zip(train_iterator, logits_loader):\n","        net.zero_grad()\n","        logits_bert = torch.tensor(logits_bert).squeeze()\n","        output = net(batch.text[0]).squeeze(1)\n","        ############# IMPLEMENT DISTILLATION HERE ##################\n","        ############################################################\n","        # Use logits_bert\n","        loss = criterion(torch.sigmoid(output), batch.label.float())\n","        ############# IMPLEMENT DISTILLATION HERE ##################\n","        ############################################################\n","        loss.backward()\n","        train_loss += loss.item()\n","        nn.utils.clip_grad_norm_(net.parameters(), args_clf.clip_gradient_max_norm)\n","        optimizer.step()\n","        step_num+=1\n","    print(f'Epoch: {epoch}  --- loss:  {train_loss/(step_num + 1)}')\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0  --- loss:  0.6508550924413344\n","Epoch: 1  --- loss:  0.6310171169393203\n","Epoch: 2  --- loss:  0.5791842727100148\n","Epoch: 3  --- loss:  0.48900363725774426\n","Epoch: 4  --- loss:  0.38708231554311867\n","Epoch: 5  --- loss:  0.2989107081118752\n","Epoch: 6  --- loss:  0.22979897611281452\n","Epoch: 7  --- loss:  0.17941275501952453\n","Epoch: 8  --- loss:  0.14212224807809382\n","Epoch: 9  --- loss:  0.1142382082693717\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sxwH8mFd8Foo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"a4ef5830-82db-4eb7-d410-4d63cde1c847","executionInfo":{"status":"ok","timestamp":1579990784388,"user_tz":-60,"elapsed":392411,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["def get_test_score_binaryclf(model, test_iterator, args):\n","  s = time.time()\n","  model.eval()\n","  model.to(args.device)\n","  binary_clf_predicted = []\n","  with torch.no_grad():\n","      for batch in test_iterator:\n","          pb = model(batch.text[0].to(\"cuda\")).squeeze(1)\n","          binary_clf_predicted += list(torch.sigmoid(pb.cpu().detach()).numpy() > 0.5)\n","\n","  print(\"___\")\n","  print(\"Time (in seconds) ________________\",time.time() - s)\n","  print(\"___\")\n","  print(classification_report(test_y, binary_clf_predicted))\n","\n","get_test_score_binaryclf(net, test_iterator, args_clf)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["___\n","Time (in seconds) ________________ 0.48473691940307617\n","___\n","              precision    recall  f1-score   support\n","\n","       False       0.81      0.82      0.81      1466\n","        True       0.82      0.82      0.82      1534\n","\n","    accuracy                           0.82      3000\n","   macro avg       0.82      0.82      0.82      3000\n","weighted avg       0.82      0.82      0.82      3000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YlHQ62VPN_Kx","colab_type":"text"},"source":["# Quantization"]},{"cell_type":"code","metadata":{"id":"deWAkK0L5_KE","colab_type":"code","colab":{}},"source":["import torch.quantization\n","\n","def print_size_of_model(model):\n","    \"\"\"\"\"\n","    Get the size on disk of the model\n","    \"\"\"\"\"\n","    torch.save(model.state_dict(), \"temp.p\")\n","    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n","    os.remove('temp.p')\n","\n","\n","bert.eval()\n","bert.cpu()\n","\n","# As simple as that:\n","quantized_bert = torch.quantization.quantize_dynamic(\n","    bert, {nn.Linear}, dtype=torch.qint8\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5AbzSuMj2fNK","colab_type":"text"},"source":["- Let's see the size on disk"]},{"cell_type":"code","metadata":{"id":"t4h1W1Lg3H72","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"17e6b492-b317-452d-c1bc-f4769c90ae1a","executionInfo":{"status":"ok","timestamp":1579990786237,"user_tz":-60,"elapsed":394248,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["print_size_of_model(quantized_bert)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Size (MB): 181.426038\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z3dBvQnPxTLt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7cfe7f8a-636b-46cf-a0b4-fcbf9839aab1","executionInfo":{"status":"ok","timestamp":1579990788482,"user_tz":-60,"elapsed":396486,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["print_size_of_model(bert)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Size (MB): 437.977457\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TnvGJZXLoCP9","colab_type":"text"},"source":["- Let's make a small dataset to compare the speed and the accuracy of both models"]},{"cell_type":"code","metadata":{"id":"VJJBRk1Wf-fC","colab_type":"code","colab":{}},"source":["data_quantization_benchmark = [full_train_data[random.randint(1,24000)] for _ in range(100)]\n","\n","quantize_texts = list(\" \".join(data_quantization_benchmark[i].text) for i in range(len(data_quantization_benchmark))) \n","quantize_labels = list(data_quantization_benchmark[i].label for i in range(len(data_quantization_benchmark)))\n","quantize_tokens = [['[CLS]'] + tokenizer.tokenize(t)[:args.max_seq_length] + ['[SEP]'] for t in  quantize_texts] # tokenize reviews in quantization dataset\n","\n","quantize_tokens_ids = [tokenizer.convert_tokens_to_ids(review) for review in quantize_tokens] # wordpieces to ids\n","quantize_tokens_ids = pad_sequences(quantize_tokens_ids, maxlen=args.max_seq_length, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","quantize_labels = list(data_quantization_benchmark[i].label for i in range(len(data_quantization_benchmark)))\n","\n","quantize_y = np.array(quantize_labels) == 'pos'\n","quantize_masks = [[float(token_id > 0) for token_id in sent_token_ids] for sent_token_ids in quantize_tokens_ids]\n","\n","quantize_dataloader = get_dataloader_bert(quantize_tokens_ids, quantize_masks, quantize_y, batch_size=args.batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ajr3uPTVnzFQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"6601c5ba-b415-4d59-a733-be73ab9e3fa8","executionInfo":{"status":"ok","timestamp":1579990842342,"user_tz":-60,"elapsed":450333,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["args.device=\"cpu\"\n","get_test_scores(quantized_bert, quantize_dataloader, args, quantize_y)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Time (Seconds) ________________ 53.21387577056885\n","___\n","              precision    recall  f1-score   support\n","\n","       False       0.35      0.33      0.34        39\n","        True       0.59      0.61      0.60        61\n","\n","    accuracy                           0.50       100\n","   macro avg       0.47      0.47      0.47       100\n","weighted avg       0.50      0.50      0.50       100\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aukYaE0ZoAnp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"64d721aa-1802-4669-f32e-b49e2fc3e073","executionInfo":{"status":"ok","timestamp":1579990912157,"user_tz":-60,"elapsed":520138,"user":{"displayName":"yassine ben","photoUrl":"","userId":"14627216274841612758"}}},"source":["get_test_scores(bert, quantize_dataloader, args , quantize_y)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Time (Seconds) ________________ 69.69603300094604\n","___\n","              precision    recall  f1-score   support\n","\n","       False       0.29      0.31      0.30        39\n","        True       0.53      0.51      0.52        61\n","\n","    accuracy                           0.43       100\n","   macro avg       0.41      0.41      0.41       100\n","weighted avg       0.44      0.43      0.43       100\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wnm2d8dqts_F","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}